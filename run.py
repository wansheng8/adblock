#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - å®Œæ•´ç‰ˆ
åŒ…å«è¡¥å……çš„è§„åˆ™å¤„ç†æ€è·¯
"""

import os
import re
import json
import time
import hashlib
import concurrent.futures
from datetime import datetime, timedelta
from typing import Set, List, Tuple, Optional, Dict
import requests

# é…ç½®ä¿¡æ¯
CONFIG = {
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    'MAX_WORKERS': 8,
    'TIMEOUT': 25,
    'RETRY': 3,
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # é‡è¦åŸŸåä¿æŠ¤
    'PROTECTED_DOMAINS': {
        'google.com', 'github.com', 'microsoft.com', 'apple.com',
        'baidu.com', 'qq.com', 'taobao.com', 'jd.com', 'weibo.com',
        'zhihu.com', 'bilibili.com', '163.com', '126.com',
        'gitee.com', 'csdn.net', 'oschina.net'
    },
    
    # å…³é”®å¹¿å‘ŠåŸŸåï¼ˆå¿…é¡»åŒ…å«ï¼‰
    'CRITICAL_AD_DOMAINS': {
        'doubleclick.net',
        'google-analytics.com',
        'googlesyndication.com',
        'googleadservices.com',
        'adservice.google.com',
        'ads.google.com',
        'scorecardresearch.com',
        'outbrain.com',
        'taboola.com',
        'criteo.com'
    },
    
    # åº”æ’é™¤çš„ç™½åå•æ¨¡å¼
    'WHITELIST_PATTERNS': [
        '@@||google.com^',
        '@@||github.com^',
        '@@||baidu.com^',
        '@@||microsoft.com^'
    ]
}

class AdvancedAdBlockGenerator:
    def __init__(self):
        # æ ¸å¿ƒæ•°æ®é›†åˆ
        self.all_black_domains = set()
        self.all_white_domains = set()
        self.all_black_rules = []
        self.all_white_rules = []
        self.element_hiding_rules = []
        self.url_pattern_rules = []
        
        # æœ€ç»ˆè¾“å‡º
        self.final_black_domains = set()
        self.final_white_rules = []
        
        # ç»Ÿè®¡
        self.stats = {
            'total_lines': 0,
            'black_domains': 0,
            'white_domains': 0,
            'complex_rules': 0,
            'element_hiding': 0,
            'url_patterns': 0
        }
        
        # åˆ›å»ºç›®å½•
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºé»˜è®¤è§„åˆ™æº
        self.create_default_sources()
    
    def create_default_sources(self):
        """åˆ›å»ºé»˜è®¤è§„åˆ™æºæ–‡ä»¶"""
        # é»‘åå•æº
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# å¹¿å‘Šè¿‡æ»¤è§„åˆ™æº - å®Œæ•´ç‰ˆ\n")
                f.write("# æ¯è¡Œä¸€ä¸ªURL\n\n")
                f.write("# 1. AdGuardåŸºç¡€å¹¿å‘Šè§„åˆ™\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n\n")
                f.write("# 2. EasyListè§„åˆ™\n")
                f.write("https://easylist.to/easylist/easylist.txt\n\n")
                f.write("# 3. EasyPrivacyè§„åˆ™\n")
                f.write("https://easylist.to/easylist/easyprivacy.txt\n\n")
                f.write("# 4. ä¸­æ–‡å¹¿å‘Šè§„åˆ™\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ADGUARD_FILTER.txt\n\n")
                f.write("# 5. å…ƒç´ éšè—è§„åˆ™\n")
                f.write("https://easylist.to/easylist/easylist.txt\n")
                f.write("https://easylist-downloads.adblockplus.org/easyprivacy.txt\n\n")
                f.write("# 6. é˜²è·Ÿè¸ªè§„åˆ™\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/tracking.txt\n")
        
        # ç™½åå•æº
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("# åªåŒ…å«ç™½åå•è§„åˆ™\n\n")
                f.write("# AdGuardç™½åå•\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n\n")
                f.write("# æ‰‹åŠ¨æ·»åŠ é‡è¦ç™½åå•\n")
                f.write("# æ ¼å¼ï¼š@@||domain.com^\n")
                f.write("@@||google.com^\n")
                f.write("@@||github.com^\n")
                f.write("@@||baidu.com^\n")
                f.write("@@||zhihu.com^\n")
                f.write("@@||bilibili.com^\n")
    
    def download_content(self, url: str) -> Optional[str]:
        """ä¸‹è½½è§„åˆ™å†…å®¹"""
        for i in range(CONFIG['RETRY']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain, */*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(2)
                else:
                    print(f"  âŒ ä¸‹è½½å¤±è´¥ {url}: {str(e)[:100]}")
        return None
    
    def is_element_hiding_rule(self, rule: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å…ƒç´ éšè—è§„åˆ™"""
        rule = rule.strip()
        
        # CSSå…ƒç´ éšè—è§„åˆ™
        if rule.startswith('##'):
            return True
        
        # AdGuardå…ƒç´ éšè—è§„åˆ™
        if re.match(r'^[a-zA-Z0-9.-]+##', rule):
            return True
        
        # åŒ…å«CSSé€‰æ‹©å™¨çš„è§„åˆ™
        if re.search(r'#(#|@)?[\.#\[]', rule):
            return True
        
        return False
    
    def extract_url_pattern(self, rule: str) -> Optional[str]:
        """æå–URLæ¨¡å¼è§„åˆ™"""
        rule = rule.strip()
        
        # URLæ¨¡å¼åŒ¹é…
        patterns = [
            r'^\|\|([^\\^\$]+)\^',        # ||example.com/path^
            r'^\|([^\\|]+)\|',            # |http://example.com|
            r'^/.*/$',                     # /ads/.*/
            r'^\*://\*\.([^/]+)/\*',      # *://*.example.com/*
            r'^https?://[^\$]+',          # http://example.com
            r'^//[^\$]+',                  # //example.com
        ]
        
        for pattern in patterns:
            if re.match(pattern, rule):
                return rule
        
        return None
    
    def parse_modifier_rule(self, rule: str) -> Tuple[str, Optional[str]]:
        """è§£æå¸¦ä¿®é¥°ç¬¦çš„è§„åˆ™"""
        rule = rule.strip()
        domain = None
        
        # åˆ†ç¦»è§„åˆ™ä¸»ä½“å’Œä¿®é¥°ç¬¦
        if '$' in rule:
            parts = rule.split('$', 1)
            rule_part = parts[0]
            modifier_part = parts[1]
            
            # æå–åŸŸåä¿®é¥°ç¬¦
            domain_match = re.search(r'domain=([a-zA-Z0-9.-]+)', modifier_part)
            if domain_match:
                domain = domain_match.group(1).lower()
            
            return rule_part, domain
        
        return rule, None
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        local_domains = {
            'localhost', 'local', 'broadcasthost',
            '0.0.0.0', '127.0.0.1', '::1',
            'ip6-localhost', 'ip6-loopback'
        }
        if domain in local_domains:
            return False
        
        # æ’é™¤IPåœ°å€
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', domain):
            return False
        
        # æ£€æŸ¥åŸŸåæ ¼å¼
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        for part in parts:
            if not part or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def normalize_domain(self, domain: str) -> str:
        """æ ‡å‡†åŒ–åŸŸå"""
        if not domain:
            return ""
        
        domain = domain.lower().strip()
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = ['www.', '*.', 'm.', 'mobile.']
        for prefix in prefixes:
            if domain.startswith(prefix):
                domain = domain[len(prefix):]
        
        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ['.', '^', '$', '|', '~']
        for suffix in suffixes:
            if domain.endswith(suffix):
                domain = domain[:-len(suffix)]
        
        # ç§»é™¤ç«¯å£å·
        if ':' in domain:
            domain = domain.split(':')[0]
        
        return domain
    
    def extract_domain_from_rule(self, rule: str) -> Tuple[List[str], bool]:
        """ä»è§„åˆ™ä¸­æå–åŸŸå"""
        rule = rule.strip()
        if not rule:
            return [], False
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯ç™½åå•
        is_whitelist = rule.startswith('@@')
        
        # å¦‚æœæ˜¯ç™½åå•è§„åˆ™ï¼Œç§»é™¤@@å‰ç¼€
        if is_whitelist:
            rule = rule[2:]
        
        # è§£æä¿®é¥°ç¬¦è§„åˆ™
        rule, modifier_domain = self.parse_modifier_rule(rule)
        
        domains = []
        
        # å°è¯•åŒ¹é…å¤šç§åŸŸåæ ¼å¼
        patterns = [
            r'^\|\|([a-zA-Z0-9.-]+)\^',          # ||domain.com^
            r'^\|\|([a-zA-Z0-9.-]+)/',           # ||domain.com/
            r'^([a-zA-Z0-9.-]+)\^$',             # domain.com^
            r'^\*\.([a-zA-Z0-9.-]+)',            # *.domain.com
            r'^([a-zA-Z0-9.-]+)$',               # domain.com
            r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',    # é€šç”¨åŸŸååŒ¹é…
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, rule)
            for match in matches:
                if isinstance(match, tuple):
                    domain = match[0]
                else:
                    domain = match
                
                domain = self.normalize_domain(domain)
                if self.is_valid_domain(domain):
                    domains.append(domain)
        
        # æ·»åŠ ä¿®é¥°ç¬¦ä¸­çš„åŸŸå
        if modifier_domain:
            domain = self.normalize_domain(modifier_domain)
            if self.is_valid_domain(domain):
                domains.append(domain)
        
        # å»é‡
        domains = list(set(domains))
        
        return domains, is_whitelist
    
    def classify_rule(self, rule: str) -> Tuple[str, List[str], str]:
        """åˆ†ç±»è§„åˆ™ç±»å‹"""
        self.stats['total_lines'] += 1
        
        rule = rule.strip()
        if not rule:
            return 'empty', [], rule
        
        # è·³è¿‡æ³¨é‡Š
        if rule.startswith('!') or rule.startswith('#'):
            return 'comment', [], rule
        
        # æ£€æŸ¥å…ƒç´ éšè—è§„åˆ™
        if self.is_element_hiding_rule(rule):
            self.stats['element_hiding'] += 1
            return 'element_hiding', [], rule
        
        # æ£€æŸ¥URLæ¨¡å¼è§„åˆ™
        url_pattern = self.extract_url_pattern(rule)
        if url_pattern:
            self.stats['url_patterns'] += 1
            return 'url_pattern', [], rule
        
        # æå–åŸŸå
        domains, is_whitelist = self.extract_domain_from_rule(rule)
        
        if domains:
            if is_whitelist:
                self.stats['white_domains'] += len(domains)
                return 'white_domain', domains, rule
            else:
                self.stats['black_domains'] += len(domains)
                return 'black_domain', domains, rule
        
        # å¤æ‚è§„åˆ™
        self.stats['complex_rules'] += 1
        if is_whitelist:
            return 'white_rule', [], rule
        else:
            return 'black_rule', [], rule
    
    def process_rule(self, rule: str, source_url: str = ""):
        """å¤„ç†å•æ¡è§„åˆ™"""
        rule_type, domains, original_rule = self.classify_rule(rule)
        
        if not original_rule or rule_type in ['empty', 'comment']:
            return
        
        if rule_type == 'element_hiding':
            self.element_hiding_rules.append(original_rule)
        elif rule_type == 'url_pattern':
            self.url_pattern_rules.append(original_rule)
        elif rule_type == 'white_domain':
            self.all_white_domains.update(domains)
            self.all_white_rules.append(original_rule)
        elif rule_type == 'black_domain':
            self.all_black_domains.update(domains)
        elif rule_type == 'white_rule':
            self.all_white_rules.append(original_rule)
        elif rule_type == 'black_rule':
            self.all_black_rules.append(original_rule)
    
    def process_url(self, url: str):
        """å¤„ç†å•ä¸ªè§„åˆ™æºURL"""
        print(f"  ğŸ“¥ å¤„ç†: {url}")
        content = self.download_content(url)
        if not content:
            return
        
        lines = content.split('\n')
        domains_found = 0
        
        for line in lines:
            self.process_rule(line, url)
        
        print(f"  âœ“ å®Œæˆ: {len(lines)} è¡Œ")
    
    def load_and_process_sources(self):
        """åŠ è½½å¹¶å¤„ç†æ‰€æœ‰è§„åˆ™æº"""
        print("ğŸ” åŠ è½½è§„åˆ™æº...")
        
        # è¯»å–é»‘åå•æº
        blacklist_urls = []
        if os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        blacklist_urls.append(line)
        
        # è¯»å–ç™½åå•æº
        whitelist_urls = []
        local_whitelist = []
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if line.startswith('http'):
                            whitelist_urls.append(line)
                        else:
                            local_whitelist.append(line)
        
        print(f"  é»‘åå•æº: {len(blacklist_urls)} ä¸ª")
        print(f"  ç™½åå•æº: {len(whitelist_urls)} ä¸ª")
        print(f"  æœ¬åœ°ç™½åå•è§„åˆ™: {len(local_whitelist)} æ¡")
        
        # å¤„ç†æœ¬åœ°ç™½åå•è§„åˆ™
        for rule in local_whitelist:
            self.process_rule(rule, "local_whitelist")
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰URL
        all_urls = blacklist_urls + whitelist_urls
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = []
            for url in all_urls:
                future = executor.submit(self.process_url, url)
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            completed = 0
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result(timeout=35)
                    completed += 1
                    print(f"  âœ… [{completed}/{len(all_urls)}] å¤„ç†å®Œæˆ")
                except Exception as e:
                    print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
        
        print(f"âœ… è§£æå®Œæˆ:")
        print(f"   é»‘åå•åŸŸå: {len(self.all_black_domains):,} ä¸ª")
        print(f"   ç™½åå•åŸŸå: {len(self.all_white_domains):,} ä¸ª")
        print(f"   é»‘åå•è§„åˆ™: {len(self.all_black_rules):,} æ¡")
        print(f"   ç™½åå•è§„åˆ™: {len(self.all_white_rules):,} æ¡")
        print(f"   å…ƒç´ éšè—è§„åˆ™: {len(self.element_hiding_rules):,} æ¡")
        print(f"   URLæ¨¡å¼è§„åˆ™: {len(self.url_pattern_rules):,} æ¡")
    
    def enhance_critical_domains(self):
        """å¢å¼ºå…³é”®å¹¿å‘ŠåŸŸå"""
        print("ğŸ›¡ï¸  å¢å¼ºå…³é”®å¹¿å‘ŠåŸŸå...")
        
        added = 0
        for domain in CONFIG['CRITICAL_AD_DOMAINS']:
            if domain not in self.all_white_domains:
                self.all_black_domains.add(domain)
                added += 1
        
        if added > 0:
            print(f"  æ·»åŠ  {added} ä¸ªå…³é”®å¹¿å‘ŠåŸŸå")
    
    def apply_whitelist(self):
        """åº”ç”¨ç™½åå•"""
        print("ğŸ”„ åº”ç”¨ç™½åå•...")
        
        # æœ€ç»ˆé»‘åå• = æ‰€æœ‰é»‘åå• - æ‰€æœ‰ç™½åå•
        self.final_black_domains = self.all_black_domains.copy()
        self.final_white_rules = self.all_white_rules.copy()
        
        original_count = len(self.final_black_domains)
        
        # ç§»é™¤å®Œå…¨åŒ¹é…çš„ç™½åå•åŸŸå
        domains_to_remove = set()
        for domain in self.final_black_domains:
            if domain in self.all_white_domains:
                domains_to_remove.add(domain)
        
        # ä¿æŠ¤é‡è¦åŸŸå
        for protected in CONFIG['PROTECTED_DOMAINS']:
            if protected in domains_to_remove:
                domains_to_remove.remove(protected)
                print(f"  ğŸ›¡ï¸  ä¿æŠ¤é‡è¦åŸŸå: {protected}")
        
        self.final_black_domains -= domains_to_remove
        
        removed = original_count - len(self.final_black_domains)
        print(f"  ç§»é™¤ {removed} ä¸ªç™½åå•åŸŸå")
        print(f"  æœ€ç»ˆé»‘åå•åŸŸå: {len(self.final_black_domains):,} ä¸ª")
    
    def generate_files(self):
        """ç”Ÿæˆæ‰€æœ‰è§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # è·å–åŒ—äº¬æ—¶é—´
        beijing_time = self.get_beijing_time()
        version = beijing_time.strftime('%Y%m%d')
        timestamp = beijing_time.strftime('%Y-%m-%d %H:%M:%S')
        
        # ç”Ÿæˆad.txt
        self.generate_adblock_file(version, timestamp)
        
        # ç”Ÿæˆdns.txt
        self.generate_dns_file(version, timestamp)
        
        # ç”Ÿæˆhosts.txt
        self.generate_hosts_file(version, timestamp)
        
        # ç”Ÿæˆblack.txt
        self.generate_blacklist_file(version, timestamp)
        
        # ç”Ÿæˆwhite.txt
        self.generate_whitelist_file(version, timestamp)
        
        # ç”Ÿæˆinfo.json
        self.generate_info_file(version, timestamp)
        
        # ç”Ÿæˆè¡¥å……æ–‡ä»¶
        self.generate_supplementary_files(version, timestamp)
    
    def generate_adblock_file(self, version: str, timestamp: str):
        """ç”ŸæˆAdBlockæ ¼å¼è§„åˆ™æ–‡ä»¶"""
        with open('rules/outputs/ad.txt', 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - å®Œæ•´ç‰ˆ v{version}\n")
            f.write(f"! æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.final_black_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•è§„åˆ™: {len(set(self.final_white_rules)):,} æ¡\n")
            f.write(f"! å…ƒç´ éšè—è§„åˆ™: {len(self.element_hiding_rules):,} æ¡\n")
            f.write(f"! URLæ¨¡å¼è§„åˆ™: {len(self.url_pattern_rules):,} æ¡\n")
            f.write(f"! é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n")
            f.write("!\n\n")
            
            # ç™½åå•è§„åˆ™
            if self.final_white_rules:
                f.write("! ====== ç™½åå•è§„åˆ™ ======\n")
                unique_white_rules = sorted(set(self.final_white_rules))
                for rule in unique_white_rules:
                    f.write(f"{rule}\n")
                f.write("\n")
            
            # é»‘åå•åŸŸåè§„åˆ™
            f.write("! ====== åŸŸåé»‘åå• ======\n")
            for domain in sorted(self.final_black_domains):
                f.write(f"||{domain}^\n")
            
            # å¤æ‚é»‘åå•è§„åˆ™
            if self.all_black_rules:
                f.write("\n! ====== å¤æ‚è§„åˆ™ ======\n")
                unique_black_rules = sorted(set(self.all_black_rules))
                for rule in unique_black_rules:
                    f.write(f"{rule}\n")
            
            # URLæ¨¡å¼è§„åˆ™
            if self.url_pattern_rules:
                f.write("\n! ====== URLæ¨¡å¼è§„åˆ™ ======\n")
                unique_url_rules = sorted(set(self.url_pattern_rules))
                for rule in unique_url_rules:
                    f.write(f"{rule}\n")
    
    def generate_dns_file(self, version: str, timestamp: str):
        """ç”ŸæˆDNSæ ¼å¼è§„åˆ™æ–‡ä»¶"""
        with open('rules/outputs/dns.txt', 'w', encoding='utf-8') as f:
            f.write(f"# DNSå¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_black_domains):,} ä¸ª\n")
            f.write(f"# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n")
            f.write("#\n\n")
            
            # åˆ†ç»„å†™å…¥
            domains = sorted(self.final_black_domains)
            batch_size = 1000
            
            for i in range(0, len(domains), batch_size):
                batch = domains[i:i+batch_size]
                if i > 0:
                    f.write("\n")
                for domain in batch:
                    f.write(f"{domain}\n")
    
    def generate_hosts_file(self, version: str, timestamp: str):
        """ç”ŸæˆHostsæ ¼å¼è§„åˆ™æ–‡ä»¶"""
        with open('rules/outputs/hosts.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Hostså¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_black_domains):,} ä¸ª\n")
            f.write(f"# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n")
            f.write("#\n\n")
            f.write("# æœ¬åœ°åŸŸå\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n")
            f.write("#\n")
            f.write("# å¹¿å‘ŠåŸŸå\n\n")
            
            # åˆ†æ‰¹å†™å…¥
            batch_size = 500
            domains = sorted(self.final_black_domains)
            
            for i in range(0, len(domains), batch_size):
                batch = domains[i:i+batch_size]
                f.write(f"# ç¬¬ {i//batch_size + 1} ç»„ ({len(batch)}ä¸ªåŸŸå)\n")
                for domain in batch:
                    f.write(f"0.0.0.0 {domain}\n")
                f.write("\n")
    
    def generate_blacklist_file(self, version: str, timestamp: str):
        """ç”Ÿæˆçº¯é»‘åå•æ–‡ä»¶"""
        with open('rules/outputs/black.txt', 'w', encoding='utf-8') as f:
            f.write(f"# é»‘åå•è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_black_domains):,} ä¸ª\n")
            f.write("#\n\n")
            
            for domain in sorted(self.final_black_domains):
                f.write(f"||{domain}^\n")
    
    def generate_whitelist_file(self, version: str, timestamp: str):
        """ç”Ÿæˆçº¯ç™½åå•æ–‡ä»¶"""
        unique_white_rules = sorted(set(self.final_white_rules))
        
        with open('rules/outputs/white.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ç™½åå•è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp} (åŒ—äº¬æ—¶é—´)\n")
            f.write(f"# è§„åˆ™æ•°é‡: {len(unique_white_rules):,} æ¡\n")
            f.write("#\n\n")
            
            # åŸŸåç™½åå•
            domain_whitelist = [r for r in unique_white_rules if r.startswith('@@||') and r.endswith('^')]
            if domain_whitelist:
                f.write("# åŸŸåç™½åå•\n")
                for rule in domain_whitelist:
                    f.write(f"{rule}\n")
                f.write("\n")
            
            # å…¶ä»–ç™½åå•è§„åˆ™
            other_whitelist = [r for r in unique_white_rules if r not in domain_whitelist]
            if other_whitelist:
                f.write("# å…¶ä»–ç™½åå•è§„åˆ™\n")
                for rule in other_whitelist:
                    f.write(f"{rule}\n")
    
    def generate_info_file(self, version: str, timestamp: str):
        """ç”Ÿæˆä¿¡æ¯æ–‡ä»¶"""
        info = {
            'version': version,
            'updated_at': timestamp,
            'timezone': 'Asia/Shanghai (UTC+8)',
            'statistics': {
                'total_lines_processed': self.stats['total_lines'],
                'blacklist_domains': self.stats['black_domains'],
                'whitelist_domains': self.stats['white_domains'],
                'complex_rules': self.stats['complex_rules'],
                'element_hiding_rules': self.stats['element_hiding'],
                'url_pattern_rules': self.stats['url_patterns'],
                'final_blacklist_domains': len(self.final_black_domains),
                'final_whitelist_rules': len(set(self.final_white_rules))
            },
            'files': {
                'ad.txt': 'AdBlockå®Œæ•´è§„åˆ™',
                'dns.txt': 'DNSè¿‡æ»¤è§„åˆ™',
                'hosts.txt': 'Hostsæ ¼å¼è§„åˆ™',
                'black.txt': 'çº¯é»‘åå•',
                'white.txt': 'çº¯ç™½åå•'
            }
        }
        
        with open('rules/outputs/info.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
    
    def generate_supplementary_files(self, version: str, timestamp: str):
        """ç”Ÿæˆè¡¥å……æ–‡ä»¶"""
        # 1. ç”Ÿæˆå…ƒç´ éšè—è§„åˆ™æ–‡ä»¶
        if self.element_hiding_rules:
            with open('rules/outputs/element_hiding.txt', 'w', encoding='utf-8') as f:
                f.write(f"# å…ƒç´ éšè—è§„åˆ™ v{version}\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write(f"# è§„åˆ™æ•°é‡: {len(self.element_hiding_rules):,} æ¡\n")
                f.write("#\n\n")
                
                unique_rules = sorted(set(self.element_hiding_rules))
                for rule in unique_rules:
                    f.write(f"{rule}\n")
        
        # 2. ç”Ÿæˆå…³é”®åŸŸååˆ—è¡¨
        with open('rules/outputs/critical_domains.txt', 'w', encoding='utf-8') as f:
            f.write(f"# å…³é”®å¹¿å‘ŠåŸŸå v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write("#\n\n")
            
            for domain in sorted(CONFIG['CRITICAL_AD_DOMAINS']):
                f.write(f"{domain}\n")
        
        # 3. ç”Ÿæˆç®€åŒ–ç‰ˆDNSè§„åˆ™ï¼ˆç”¨äºå†…å­˜æœ‰é™çš„è®¾å¤‡ï¼‰
        if len(self.final_black_domains) > 5000:
            top_domains = sorted(self.final_black_domains)[:5000]
            with open('rules/outputs/dns_light.txt', 'w', encoding='utf-8') as f:
                f.write(f"# è½»é‡DNSè§„åˆ™ v{version}\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write(f"# åŸŸåæ•°é‡: {len(top_domains):,} ä¸ª\n")
                f.write("#\n\n")
                
                for domain in top_domains:
                    f.write(f"{domain}\n")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.mdæ–‡ä»¶"""
        print("ğŸ“– ç”ŸæˆREADME.md...")
        
        # è¯»å–è§„åˆ™ä¿¡æ¯
        with open('rules/outputs/info.json', 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # ç”Ÿæˆè®¢é˜…é“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        # ç”ŸæˆREADMEå†…å®¹
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºAdGuardã€uBlock Originã€AdBlock Plusã€AdGuard Homeã€Pi-holeç­‰ã€‚

---

## è®¢é˜…åœ°å€

| è§„åˆ™ç±»å‹ | è§„åˆ™è¯´æ˜ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|:---------|:---------|:---------|:---------|
| **AdBlockè§„åˆ™** | é€‚ç”¨äºæµè§ˆå™¨å¹¿å‘Šæ’ä»¶ | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| **DNSè¿‡æ»¤è§„åˆ™** | é€‚ç”¨äºDNSè¿‡æ»¤è½¯ä»¶ | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| **Hostsè§„åˆ™** | é€‚ç”¨äºç³»ç»Ÿhostsæ–‡ä»¶ | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| **é»‘åå•è§„åˆ™** | çº¯é»‘åå•åŸŸå | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| **ç™½åå•è§„åˆ™** | æ’é™¤è¯¯æ‹¦åŸŸå | `{base_url}/white.txt` | `{cdn_url}/white.txt` |
| **è½»é‡DNSè§„åˆ™** | é€‚ç”¨äºå†…å­˜æœ‰é™çš„è®¾å¤‡ | `{base_url}/dns_light.txt` | `{cdn_url}/dns_light.txt` |

**ç‰ˆæœ¬ {info['version']} ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['statistics']['final_blacklist_domains']:,} ä¸ª
- ç™½åå•è§„åˆ™ï¼š{info['statistics']['final_whitelist_rules']:,} æ¡
- å…ƒç´ éšè—è§„åˆ™ï¼š{info['statistics']['element_hiding_rules']:,} æ¡

---

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at']}** (åŒ—äº¬æ—¶é—´)

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
    
    def get_beijing_time(self) -> datetime:
        """è·å–åŒ—äº¬æ—¶é—´"""
        try:
            from datetime import timezone
            utc_now = datetime.now(timezone.utc)
            beijing_time = utc_now + timedelta(hours=8)
            return beijing_time
        except:
            return datetime.now()
    
    def run_quality_check(self):
        """è¿è¡Œè´¨é‡æ£€æŸ¥"""
        print("ğŸ” è¿è¡Œè´¨é‡æ£€æŸ¥...")
        
        # æ£€æŸ¥å…³é”®å¹¿å‘ŠåŸŸåæ˜¯å¦åŒ…å«
        missing_critical = []
        for domain in CONFIG['CRITICAL_AD_DOMAINS']:
            if domain not in self.final_black_domains:
                missing_critical.append(domain)
        
        if missing_critical:
            print(f"âš ï¸  è­¦å‘Š: ç¼ºå¤± {len(missing_critical)} ä¸ªå…³é”®å¹¿å‘ŠåŸŸå")
            for domain in missing_critical[:5]:
                print(f"   - {domain}")
        
        # æ£€æŸ¥ç™½åå•æ˜¯å¦è¿‡åº¦
        if len(self.final_white_rules) > 1000:
            print(f"âš ï¸  è­¦å‘Š: ç™½åå•è§„åˆ™è¿‡å¤š ({len(self.final_white_rules)} æ¡)")
        
        # æ£€æŸ¥åŸŸåé‡å¤
        if len(self.final_black_domains) < self.stats['black_domains'] * 0.5:
            print("âš ï¸  è­¦å‘Š: å¯èƒ½è¿‡å¤šåŸŸåè¢«ç™½åå•ç§»é™¤")
        
        print("âœ… è´¨é‡æ£€æŸ¥å®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ é«˜çº§å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½å¹¶å¤„ç†è§„åˆ™æº
            self.load_and_process_sources()
            
            # 2. å¢å¼ºå…³é”®å¹¿å‘ŠåŸŸå
            self.enhance_critical_domains()
            
            # 3. åº”ç”¨ç™½åå•
            self.apply_whitelist()
            
            # 4. è¿è¡Œè´¨é‡æ£€æŸ¥
            self.run_quality_check()
            
            # 5. ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶
            self.generate_files()
            
            # 6. ç”ŸæˆREADME
            self.generate_readme()
            
            # ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            elapsed = end_time - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ è§„åˆ™ç”Ÿæˆå®Œæˆï¼")
            print(f"â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"ğŸ“Š æœ€ç»ˆé»‘åå•åŸŸå: {len(self.final_black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•è§„åˆ™: {len(set(self.final_white_rules)):,}æ¡")
            print("ğŸ“ ç”Ÿæˆçš„è§„åˆ™æ–‡ä»¶:")
            print("  - rules/outputs/ad.txt")
            print("  - rules/outputs/dns.txt")
            print("  - rules/outputs/hosts.txt")
            print("  - rules/outputs/black.txt")
            print("  - rules/outputs/white.txt")
            print("  - rules/outputs/element_hiding.txt (å¯é€‰)")
            print("  - rules/outputs/dns_light.txt (å¯é€‰)")
            print("ğŸ“– ä½¿ç”¨è¯´æ˜: README.md")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # è¿è¡Œç”Ÿæˆå™¨
    generator = AdvancedAdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nâœ¨ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ”— æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸ”¬ å»ºè®®è¿è¡Œæµ‹è¯•è„šæœ¬æ£€æŸ¥è§„åˆ™è´¨é‡")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
