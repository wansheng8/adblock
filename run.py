#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - ä¿®å¤ç‰ˆ
"""

import os
import re
import json
import time
import concurrent.futures
from datetime import datetime
from typing import Set, List, Optional, Dict
import requests

# é…ç½®ä¿¡æ¯
CONFIG = {
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    'MAX_WORKERS': 5,
    'TIMEOUT': 30,
    'RETRY': 3,
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # çœŸæ­£çš„ç™½åå•åŸŸåï¼ˆåªæ”¾è¡Œè¿™äº›ï¼‰
    'TRUE_WHITELIST_DOMAINS': {
        'google.com',
        'github.com',
        'microsoft.com',
        'apple.com',
        'baidu.com',
        'qq.com',
        'zhihu.com',
        'bilibili.com',
        'weibo.com',
        'taobao.com'
    },
    
    # é¢å¤–æ”¾è¡Œçš„åŸŸåï¼ˆé¿å…è¯¯æ€ï¼‰
    'EXTRA_SAFE_DOMAINS': {
        'windowsupdate.com',
        'apple-dns.net',
        'msftncsi.com',
        'mzstatic.com',
        'icloud.com'
    }
}

class AdBlockGenerator:
    def __init__(self):
        self.black_domains = set()      # é»‘åå•åŸŸå
        self.final_blacklist = set()    # æœ€ç»ˆé»‘åå•
        self.whitelist_domains = set()  # ç™½åå•åŸŸå
        
        # ç»Ÿè®¡
        self.stats = {
            'lines_processed': 0,
            'domains_found': 0,
            'whitelist_ignored': 0,
            'urls_processed': 0
        }
        
        # åˆ›å»ºç›®å½•
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºé»˜è®¤è§„åˆ™æº
        self.create_default_sources()
    
    def create_default_sources(self):
        """åˆ›å»ºé»˜è®¤è§„åˆ™æºæ–‡ä»¶"""
        # é»‘åå•æº - ä½¿ç”¨æ›´å¤šå¯é çš„æº
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æº - ä¿®å¤ç‰ˆ\n")
                f.write("# ä½¿ç”¨å¯é çš„å¹¿å‘Šè§„åˆ™æº\n\n")
                
                f.write("# 1. AdGuard Base Filter\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n\n")
                
                f.write("# 2. EasyList\n")
                f.write("https://easylist.to/easylist/easylist.txt\n\n")
                
                f.write("# 3. AdGuard Tracking Protection\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/tracking_servers.txt\n\n")
                
                f.write("# 4. AdGuard Mobile Ads\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/MobileFilter/sections/adservers.txt\n\n")
                
                f.write("# 5. Peter Lowe's Ad and tracking server list\n")
                f.write("https://pgl.yoyo.org/adservers/serverlist.php?hostformat=adblockplus&showintro=0&mimetype=plaintext\n\n")
        
        # ç™½åå•æº - åªæ”¾è¡ŒçœŸæ­£éœ€è¦çš„
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("# åªæ”¾è¡ŒçœŸæ­£éœ€è¦çš„åŸŸå\n\n")
                f.write("# é‡è¦ç½‘ç«™ä¸»åŸŸå\n")
                for domain in CONFIG['TRUE_WHITELIST_DOMAINS']:
                    f.write(f"{domain}\n")
                f.write("\n# é¢å¤–å®‰å…¨åŸŸå\n")
                for domain in CONFIG['EXTRA_SAFE_DOMAINS']:
                    f.write(f"{domain}\n")
    
    def download_content(self, url: str) -> Optional[str]:
        """ä¸‹è½½è§„åˆ™å†…å®¹"""
        for i in range(CONFIG['RETRY']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/plain, */*',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Cache-Control': 'no-cache'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                self.stats['urls_processed'] += 1
                return response.text
            except requests.exceptions.Timeout:
                print(f"    â±ï¸  è¶…æ—¶ ({i+1}/{CONFIG['RETRY']})")
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(3)
            except Exception as e:
                print(f"    âŒ ä¸‹è½½å¤±è´¥: {e}")
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(2)
        return None
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        local_domains = {'localhost', 'local', 'broadcasthost', '0.0.0.0', '127.0.0.1', '::1'}
        if domain in local_domains:
            return False
        
        # æ’é™¤IPåœ°å€
        ip_pattern = r'^\d+\.\d+\.\d+\.\d+$'
        if re.match(ip_pattern, domain):
            return False
        
        # æ’é™¤å¤ªçŸ­çš„åŸŸå
        if len(domain) < 3:
            return False
        
        # åŸºæœ¬åŸŸåæ ¼å¼æ£€æŸ¥
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        # æ£€æŸ¥æ¯éƒ¨åˆ†
        for part in parts:
            if not part or len(part) > 63:
                return False
            # å…è®¸å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def extract_domain_from_line(self, line: str) -> Optional[str]:
        """ä»è¡Œä¸­æå–åŸŸå"""
        line = line.strip()
        if not line:
            return None
        
        # è·³è¿‡æ³¨é‡Š
        if line.startswith('!') or line.startswith('#') or line.startswith('['):
            return None
        
        # å¦‚æœæ˜¯ç™½åå•è§„åˆ™ï¼ˆä»¥@@å¼€å¤´ï¼‰ï¼Œè·³è¿‡ä½†ç»Ÿè®¡
        if line.startswith('@@'):
            self.stats['whitelist_ignored'] += 1
            return None
        
        # å¤„ç†ä¸åŒçš„è§„åˆ™æ ¼å¼
        domain = None
        
        # 1. AdBlockæ ¼å¼: ||domain.com^
        if line.startswith('||') and '^' in line:
            match = re.match(r'^\|\|([a-zA-Z0-9.-]+)\^', line)
            if match:
                domain = match.group(1).lower()
        
        # 2. ç®€å•åŸŸåæ ¼å¼: domain.com
        elif re.match(r'^[a-zA-Z0-9.-]+$', line):
            domain = line.lower()
        
        # 3. Hostsæ ¼å¼: 0.0.0.0 domain.com
        elif re.match(r'^\d+\.\d+\.\d+\.\d+\s+', line):
            parts = line.split()
            if len(parts) >= 2:
                domain = parts[1].lower()
        
        # 4. é€šé…ç¬¦æ ¼å¼: *.domain.com
        elif line.startswith('*.'):
            domain = line[2:].lower()
        
        # 5. å…¶ä»–å¸¸è§æ ¼å¼
        else:
            # å°è¯•æå–domain.com^æ ¼å¼
            match = re.match(r'^([a-zA-Z0-9.-]+)\^', line)
            if match:
                domain = match.group(1).lower()
        
        if not domain:
            return None
        
        # æ¸…ç†åŸŸå
        domain = domain.strip()
        
        # ç§»é™¤wwwå‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]
        
        # ç§»é™¤æœ«å°¾çš„ç‰¹æ®Šå­—ç¬¦
        domain = re.sub(r'[\^\$]$', '', domain)
        
        # éªŒè¯åŸŸå
        if self.is_valid_domain(domain):
            self.stats['domains_found'] += 1
            return domain
        
        return None
    
    def process_blacklist_url(self, url: str):
        """å¤„ç†é»‘åå•URL"""
        print(f"  ğŸ“¥ ä¸‹è½½: {url}")
        content = self.download_content(url)
        if not content:
            print(f"    âš ï¸  è·³è¿‡ (ä¸‹è½½å¤±è´¥)")
            return
        
        domains_found = 0
        lines = content.split('\n')
        
        for line in lines:
            self.stats['lines_processed'] += 1
            
            domain = self.extract_domain_from_line(line)
            if domain:
                # åªåœ¨æ·»åŠ å‰æ£€æŸ¥ç™½åå•
                if not self.is_whitelisted_domain(domain):
                    self.black_domains.add(domain)
                    domains_found += 1
        
        print(f"    âœ“ æ‰¾åˆ° {domains_found} ä¸ªå¹¿å‘ŠåŸŸå")
    
    def is_whitelisted_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦åœ¨ç™½åå•ä¸­"""
        # ç›´æ¥åŒ¹é…
        if domain in self.whitelist_domains:
            return True
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç™½åå•åŸŸåçš„å­åŸŸå
        for white_domain in self.whitelist_domains:
            # æ³¨æ„ï¼šè¿™é‡Œåªæ”¾è¡Œç¡®åˆ‡çš„åŸŸåï¼Œä¸è¿‡åº¦æ”¾è¡Œå­åŸŸå
            # ä¾‹å¦‚ï¼šwhite_domain = "google.com"ï¼Œåªæ”¾è¡Œ"google.com"ï¼Œä¸æ”¾è¡Œ"ads.google.com"
            if domain == white_domain:
                return True
        
        return False
    
    def load_whitelist(self) -> Set[str]:
        """åŠ è½½ç™½åå•"""
        print("âœ… åŠ è½½ç™½åå•...")
        
        # ä»é…ç½®ä¸­è·å–åŸºç¡€ç™½åå•
        whitelist_domains = set(CONFIG['TRUE_WHITELIST_DOMAINS'])
        whitelist_domains.update(CONFIG['EXTRA_SAFE_DOMAINS'])
        
        # ä»æ–‡ä»¶è¯»å–é™„åŠ çš„ç™½åå•
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    # å¤„ç†çº¯åŸŸå
                    domain = self.extract_domain_from_line(line)
                    if domain:
                        whitelist_domains.add(domain)
                    elif self.is_valid_domain(line):
                        whitelist_domains.add(line.lower())
        
        self.whitelist_domains = whitelist_domains
        
        print(f"  ç™½åå•åŸŸå: {len(whitelist_domains)} ä¸ª")
        print("  ç™½åå•ç¤ºä¾‹:", list(sorted(whitelist_domains))[:15])
        
        return whitelist_domains
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        version = datetime.now().strftime('%Y%m%d')
        
        # æ’åºåŸŸå
        sorted_blacklist = sorted(self.final_blacklist)
        sorted_whitelist = sorted(self.whitelist_domains)
        
        # 1. AdBlockè§„åˆ™ (ad.txt)
        print("  ç”Ÿæˆ ad.txt...")
        with open('rules/outputs/ad.txt', 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"! æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.final_blacklist):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(self.whitelist_domains)} ä¸ª\n")
            f.write("!\n\n")
            
            # ç™½åå•è§„åˆ™ï¼ˆæ”¾åœ¨å‰é¢ï¼‰
            f.write("! ====== ç™½åå• ======\n")
            for domain in sorted_whitelist:
                f.write(f"@@||{domain}^$important\n")
            
            f.write("\n! ====== é»‘åå• ======\n")
            # åˆ†æ‰¹å†™å…¥ï¼Œé¿å…å†…å­˜é—®é¢˜
            for i, domain in enumerate(sorted_blacklist):
                f.write(f"||{domain}^\n")
                if (i + 1) % 10000 == 0:
                    print(f"    å·²å†™å…¥ {i+1} æ¡è§„åˆ™")
        
        # 2. DNSè§„åˆ™ (dns.txt)
        print("  ç”Ÿæˆ dns.txt...")
        with open('rules/outputs/dns.txt', 'w', encoding='utf-8') as f:
            f.write(f"# DNSå¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_blacklist):,} ä¸ª\n")
            f.write("#\n\n")
            
            for i, domain in enumerate(sorted_blacklist):
                f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        print("  ç”Ÿæˆ hosts.txt...")
        with open('rules/outputs/hosts.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Hostså¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_blacklist):,} ä¸ª\n")
            f.write("#\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n")
            f.write("#\n")
            f.write("# å¹¿å‘ŠåŸŸå\n\n")
            
            for i in range(0, len(sorted_blacklist), 1000):
                batch = sorted_blacklist[i:i+1000]
                for domain in batch:
                    f.write(f"0.0.0.0 {domain}\n")
                f.write("\n")
        
        # 4. çº¯é»‘åå• (black.txt)
        print("  ç”Ÿæˆ black.txt...")
        with open('rules/outputs/black.txt', 'w', encoding='utf-8') as f:
            for domain in sorted_blacklist:
                f.write(f"{domain}\n")
        
        # 5. çº¯ç™½åå• (white.txt)
        print("  ç”Ÿæˆ white.txt...")
        with open('rules/outputs/white.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ç™½åå•è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.whitelist_domains)} ä¸ª\n")
            f.write("#\n\n")
            
            for domain in sorted_whitelist:
                f.write(f"{domain}\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': version,
            'updated_at': timestamp,
            'statistics': {
                'urls_processed': self.stats['urls_processed'],
                'lines_processed': self.stats['lines_processed'],
                'domains_found': self.stats['domains_found'],
                'final_blacklist_domains': len(self.final_blacklist),
                'whitelist_domains': len(self.whitelist_domains),
                'whitelist_ignored': self.stats['whitelist_ignored']
            }
        }
        
        with open('rules/outputs/info.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
        print(f"   åŸå§‹åŸŸå: {self.stats['domains_found']:,} ä¸ª")
        print(f"   æœ€ç»ˆé»‘åå•: {len(self.final_blacklist):,} ä¸ª")
        print(f"   ç™½åå•åŸŸå: {len(self.whitelist_domains)} ä¸ª")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - ä¿®å¤ç‰ˆ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½ç™½åå•
            self.load_whitelist()
            
            # 2. å¤„ç†é»‘åå•æº
            print("\nğŸ” å¤„ç†é»‘åå•æº...")
            
            blacklist_urls = []
            if os.path.exists(CONFIG['BLACK_SOURCE']):
                with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            blacklist_urls.append(line)
            
            if not blacklist_urls:
                print("  âš ï¸ æœªæ‰¾åˆ°é»‘åå•æº")
                return False
            
            print(f"  æ‰¾åˆ° {len(blacklist_urls)} ä¸ªé»‘åå•æº")
            
            # é¡ºåºå¤„ç†ï¼Œé¿å…å¹¶å‘é—®é¢˜
            for i, url in enumerate(blacklist_urls):
                print(f"\n[{i+1}/{len(blacklist_urls)}] ", end='')
                self.process_blacklist_url(url)
            
            print(f"\nğŸ“Š åŸå§‹åŸŸåæ”¶é›†å®Œæˆ:")
            print(f"   æ€»è¡Œæ•°: {self.stats['lines_processed']:,}")
            print(f"   æ‰¾åˆ°åŸŸå: {self.stats['domains_found']:,}")
            print(f"   å”¯ä¸€åŸŸå: {len(self.black_domains):,}")
            
            # 3. åº”ç”¨ç™½åå•ï¼ˆæ›´ä¿å®ˆçš„æ–¹å¼ï¼‰
            print("\nğŸ”„ åº”ç”¨ç™½åå•è¿‡æ»¤...")
            original_count = len(self.black_domains)
            
            # åªç§»é™¤å®Œå…¨åŒ¹é…çš„ç™½åå•åŸŸåï¼Œä¸è¿‡åº¦è¿‡æ»¤å­åŸŸå
            self.final_blacklist = set()
            for domain in self.black_domains:
                if not self.is_whitelisted_domain(domain):
                    self.final_blacklist.add(domain)
            
            removed = original_count - len(self.final_blacklist)
            print(f"  ç§»é™¤ {removed} ä¸ªç™½åå•åŸŸå")
            print(f"  æœ€ç»ˆé»‘åå•: {len(self.final_blacklist):,} ä¸ªåŸŸå")
            
            # 4. æ£€æŸ¥é»‘åå•çŠ¶æ€
            self.check_blacklist_status()
            
            # 5. ç”Ÿæˆæ–‡ä»¶
            self.generate_files()
            
            # 6. ç”ŸæˆREADME
            self.generate_readme()
            
            # 7. è¿è¡ŒéªŒè¯
            self.run_validation()
            
            elapsed = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ è§„åˆ™ç”Ÿæˆå®Œæˆï¼")
            print(f"â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"ğŸ“Š å¤„ç†URL: {self.stats['urls_processed']}ä¸ª")
            print(f"ğŸ“Š åŸå§‹åŸŸå: {self.stats['domains_found']:,}ä¸ª")
            print(f"ğŸ“Š æœ€ç»ˆé»‘åå•: {len(self.final_blacklist):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.whitelist_domains)}ä¸ª")
            print("ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def check_blacklist_status(self):
        """æ£€æŸ¥é»‘åå•çŠ¶æ€"""
        print("\nğŸ” æ£€æŸ¥é»‘åå•çŠ¶æ€...")
        
        if not self.final_blacklist:
            print("âš ï¸ è­¦å‘Š: é»‘åå•ä¸ºç©º!")
            print("å¯èƒ½çš„åŸå› :")
            print("  1. ç½‘ç»œé—®é¢˜ï¼Œè§„åˆ™æºæ— æ³•ä¸‹è½½")
            print("  2. ç™½åå•è¿‡æ»¤è¿‡ä¸¥")
            print("  3. åŸŸåæå–é€»è¾‘æœ‰é—®é¢˜")
            return
        
        print(f"  é»‘åå•åŸŸåæ•°é‡: {len(self.final_blacklist):,}")
        
        # æ£€æŸ¥å…¸å‹å¹¿å‘ŠåŸŸå
        test_domains = [
            'doubleclick.net',
            'google-analytics.com',
            'googlesyndication.com',
            'googleadservices.com',
            'adsystem.com',
            'adnxs.com',
            'scorecardresearch.com',
            'amazon-adsystem.com',
            'facebook.com/tr',  # Facebookè¿½è¸ª
            'ads.youtube.com'
        ]
        
        found = 0
        for domain in test_domains:
            # æ£€æŸ¥ä¸»åŸŸå
            main_domain = domain.split('/')[0]
            if main_domain in self.final_blacklist:
                found += 1
                print(f"  âœ… {domain} åœ¨é»‘åå•ä¸­")
            else:
                print(f"  âŒ {domain} ä¸åœ¨é»‘åå•ä¸­")
        
        print(f"  æµ‹è¯•åŸŸåè¦†ç›–ç‡: {found}/{len(test_domains)}")
        
        # æ˜¾ç¤ºéƒ¨åˆ†é»‘åå•åŸŸå
        print("\n  éƒ¨åˆ†é»‘åå•åŸŸåç¤ºä¾‹:")
        sample = list(self.final_blacklist)[:20]
        for i, domain in enumerate(sample):
            print(f"    {i+1:2d}. {domain}")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.md"""
        print("\nğŸ“– ç”ŸæˆREADME.md...")
        
        try:
            with open('rules/outputs/info.json', 'r', encoding='utf-8') as f:
                info = json.load(f)
        except:
            info = {'version': 'unknown', 'statistics': {}}
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ç®€æ´é«˜æ•ˆçš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™ï¼Œä¸“æ³¨äºæ‹¦æˆªå¹¿å‘ŠåŸŸåã€‚

---

## è®¢é˜…åœ°å€

| è§„åˆ™ç±»å‹ | è§„åˆ™è¯´æ˜ | è®¢é˜…é“¾æ¥ |
|:---------|:---------|:---------|
| **AdBlockè§„åˆ™** | é€‚ç”¨äºæµè§ˆå™¨å¹¿å‘Šæ’ä»¶ | `{base_url}/ad.txt` |
| **DNSè¿‡æ»¤è§„åˆ™** | é€‚ç”¨äºDNSè¿‡æ»¤è½¯ä»¶ | `{base_url}/dns.txt` |
| **Hostsè§„åˆ™** | é€‚ç”¨äºç³»ç»Ÿhostsæ–‡ä»¶ | `{base_url}/hosts.txt` |
| **é»‘åå•è§„åˆ™** | çº¯é»‘åå•åŸŸå | `{base_url}/black.txt` |
| **ç™½åå•è§„åˆ™** | æ’é™¤è¯¯æ‹¦åŸŸå | `{base_url}/white.txt` |

**ç‰ˆæœ¬ {info.get('version', 'unknown')} ç»Ÿè®¡ï¼š**
- å¤„ç†è§„åˆ™æºï¼š{info['statistics'].get('urls_processed', 0)} ä¸ª
- åŸå§‹åŸŸåï¼š{info['statistics'].get('domains_found', 0):,} ä¸ª
- æœ€ç»ˆé»‘åå•ï¼š{info['statistics'].get('final_blacklist_domains', 0):,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['statistics'].get('whitelist_domains', 0)} ä¸ª

---

## ä½¿ç”¨è¯´æ˜

### 1. æµè§ˆå™¨æ’ä»¶ï¼ˆå¦‚uBlock Originï¼‰
1. æ‰“å¼€uBlock Originè®¾ç½®
2. ç‚¹å‡»"è§„åˆ™åˆ—è¡¨"
3. ç‚¹å‡»"å¯¼å…¥..."
4. ç²˜è´´è®¢é˜…åœ°å€ï¼š`{base_url}/ad.txt`
5. ç‚¹å‡»"åº”ç”¨æ›´æ”¹"

### 2. DNSè¿‡æ»¤ï¼ˆå¦‚AdGuard Homeï¼‰
1. æ‰“å¼€AdGuard Homeæ§åˆ¶å°
2. è¿›å…¥"è¿‡æ»¤å™¨" â†’ "DNSå°é”åˆ—è¡¨"
3. ç‚¹å‡»"æ·»åŠ å°é”åˆ—è¡¨"
4. åç§°ï¼šå¹¿å‘Šè¿‡æ»¤è§„åˆ™
5. URLï¼š`{base_url}/dns.txt`
6. ç‚¹å‡»"ä¿å­˜"

### 3. ç³»ç»ŸHostsæ–‡ä»¶
1. ä¸‹è½½ï¼š`{base_url}/hosts.txt`
2. å¤‡ä»½åŸæœ‰hostsæ–‡ä»¶
3. å°†ä¸‹è½½çš„å†…å®¹è¿½åŠ åˆ°hostsæ–‡ä»¶æœ«å°¾
4. åˆ·æ–°DNSç¼“å­˜

---

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info.get('updated_at', 'æœªçŸ¥')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°*

## æ³¨æ„äº‹é¡¹

1. æœ¬è§„åˆ™åŒ…å«çº¦ {info['statistics'].get('final_blacklist_domains', 0):,} ä¸ªå¹¿å‘ŠåŸŸå
2. ç™½åå•åªåŒ…å« {info['statistics'].get('whitelist_domains', 0)} ä¸ªå…³é”®åŸŸå
3. å¦‚æœå‘ç°è¯¯æ‹¦ï¼Œè¯·æ·»åŠ åˆ°ç™½åå•
4. è§„åˆ™æ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ

---
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
    
    def run_validation(self):
        """è¿è¡ŒéªŒè¯"""
        print("\nğŸ” è¿è¡ŒéªŒè¯...")
        
        if not self.final_blacklist:
            print("âš ï¸ è­¦å‘Š: é»‘åå•ä¸ºç©ºï¼ŒéªŒè¯å¤±è´¥")
            return
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = [
            'rules/outputs/ad.txt',
            'rules/outputs/dns.txt',
            'rules/outputs/hosts.txt',
            'rules/outputs/black.txt',
            'rules/outputs/white.txt',
            'rules/outputs/info.json'
        ]
        
        all_exist = True
        for file in required_files:
            if os.path.exists(file):
                print(f"  âœ… {os.path.basename(file)} å­˜åœ¨")
            else:
                print(f"  âŒ {os.path.basename(file)} ç¼ºå¤±")
                all_exist = False
        
        if all_exist:
            print("âœ… æ‰€æœ‰æ–‡ä»¶ç”ŸæˆæˆåŠŸ")
        else:
            print("âš ï¸  éƒ¨åˆ†æ–‡ä»¶ç¼ºå¤±")

def main():
    """ä¸»å‡½æ•°"""
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    print("æ£€æŸ¥ç½‘ç»œè¿æ¥...")
    try:
        response = requests.get('https://raw.githubusercontent.com/', timeout=10)
        if response.status_code == 200:
            print("âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
        else:
            print(f"âš ï¸  ç½‘ç»œè¿æ¥å¼‚å¸¸: HTTP {response.status_code}")
    except Exception as e:
        print(f"âš ï¸  ç½‘ç»œè¿æ¥å¼‚å¸¸: {e}")
    
    generator = AdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nâœ¨ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ”— æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        print("\nğŸ“¦ ç”Ÿæˆçš„æ–‡ä»¶å¤§å°:")
        for file in os.listdir('rules/outputs'):
            filepath = os.path.join('rules/outputs', file)
            if os.path.isfile(filepath):
                size = os.path.getsize(filepath)
                if size > 1024*1024:
                    size_str = f"{size/(1024*1024):.1f} MB"
                elif size > 1024:
                    size_str = f"{size/1024:.1f} KB"
                else:
                    size_str = f"{size} B"
                print(f"  {file}: {size_str}")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
