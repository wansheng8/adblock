#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - å¢å¼ºç‰ˆ
ä¼˜åŒ–è§„åˆ™å¤„ç†ï¼Œæé«˜æ‹¦æˆªå‘½ä¸­ç‡
"""

import os
import re
import json
import time
import hashlib
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, Dict, List, Optional, Tuple
import requests
import urllib.parse
from collections import defaultdict

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 10,  # å¢åŠ å¹¶å‘æ•°
    'TIMEOUT': 45,      # å¢åŠ è¶…æ—¶
    'RETRY_TIMES': 5,   # å¢åŠ é‡è¯•æ¬¡æ•°
    
    # è§„åˆ™æºæ–‡ä»¶ - å¢å¼ºç‰ˆ
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    'CHINA_SOURCE': 'rules/sources/china.txt',  # æ–°å¢ä¸­æ–‡å¹¿å‘Šè§„åˆ™æº
    'ENHANCED_SOURCE': 'rules/sources/enhanced.txt',  # æ–°å¢å¢å¼ºè§„åˆ™æº
    
    # è¾“å‡ºæ–‡ä»¶
    'OUTPUT_FILES': {
        'ad': 'rules/outputs/ad.txt',
        'dns': 'rules/outputs/dns.txt',
        'hosts': 'rules/outputs/hosts.txt',
        'black': 'rules/outputs/black.txt',
        'white': 'rules/outputs/white.txt',
        'info': 'rules/outputs/info.json',
        'smart_ad': 'rules/outputs/smart_ad.txt',  # æ–°å¢æ™ºèƒ½è§„åˆ™
        'mobile_ad': 'rules/outputs/mobile_ad.txt',  # æ–°å¢ç§»åŠ¨ç«¯è§„åˆ™
    },
    
    # æ’é™¤çš„åŸŸå
    'EXCLUDE_DOMAINS': [
        'localhost', 'local', 'broadcasthost',
        '127.0.0.1', '0.0.0.0', '::1',
        'ip6-localhost', 'ip6-loopback'
    ],
    
    # é€šç”¨é¡¶çº§åŸŸåï¼ˆé¿å…è¯¯æ€ï¼‰
    'TLD_WHITELIST': [
        'com', 'org', 'net', 'edu', 'gov', 'mil', 'int',
        'cn', 'uk', 'de', 'fr', 'jp', 'ru', 'br', 'in',
        'it', 'ca', 'au', 'es', 'mx', 'kr', 'nl', 'ch',
        'se', 'no', 'fi', 'dk', 'pl', 'be', 'at', 'gr',
        'pt', 'il', 'ie', 'sg', 'hk', 'tw', 'my', 'th',
        'id', 'vn', 'ph', 'tr', 'sa', 'ae', 'eg'
    ],
    
    # å‘½ä¸­ç‡ä¼˜åŒ–é…ç½®
    'HIT_OPTIMIZATION': {
        'enable_smart_rules': True,  # å¯ç”¨æ™ºèƒ½è§„åˆ™
        'enable_china_focus': True,  # å¯ç”¨ä¸­æ–‡ç½‘ç«™ä¸“æ³¨æ¨¡å¼
        'enable_mobile_optimization': True,  # ç§»åŠ¨ç«¯ä¼˜åŒ–
        'min_hit_score': 0.3,  # æœ€å°å‘½ä¸­åˆ†æ•°é˜ˆå€¼
        'max_domains_per_source': 50000,  # æ¯ä¸ªæºçš„æœ€å¤§åŸŸåæ•°
        'enable_wildcard_expansion': True,  # é€šé…ç¬¦æ‰©å±•
        'enable_subdomain_generation': True,  # å­åŸŸåç”Ÿæˆ
    },
    
    # ä¸­æ–‡å¹¿å‘Šå…³é”®è¯ï¼ˆç”¨äºå¢å¼ºåŒ¹é…ï¼‰
    'CHINESE_AD_KEYWORDS': [
        'å¹¿å‘Š', 'æ¨å¹¿', 'è¥é”€', 'æŠ•æ”¾', 'è”ç›Ÿ', 'æµé‡', 'å˜ç°',
        'å¼¹çª—', 'æ‚¬æµ®', 'æ¨ªå¹…', 'æ’å±', 'å¼€å±', 'è´´ç‰‡', 'å‰è´´',
        'ä¸­æ’', 'åè´´', 'è§’æ ‡', 'ä¿¡æ¯æµ', 'åŸç”Ÿ', 'æ¿€åŠ±è§†é¢‘',
        'admob', 'mopub', 'facebook', 'twitter', 'instagram',
        'googlead', 'doubleclick', 'adsystem', 'adservice',
        'tracking', 'analytics', 'statistics', 'monitor',
        'beacon', 'pixel', 'tag', 'cookie', 'fingerprint'
    ],
    
    # ç§»åŠ¨ç«¯å¹¿å‘Šå…³é”®è¯
    'MOBILE_AD_KEYWORDS': [
        'mobile', 'mob', 'android', 'ios', 'iphone', 'ipad',
        'app', 'sdk', 'inapp', 'interstitial', 'reward',
        'banner', 'native', 'video', 'fullscreen', 'push',
        'notification', 'advert', 'promo', 'offer', 'install'
    ]
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DomainOptimizer:
    """åŸŸåä¼˜åŒ–å™¨"""
    
    @staticmethod
    def expand_wildcard_domain(domain: str) -> List[str]:
        """æ‰©å±•é€šé…ç¬¦åŸŸå"""
        if '*' not in domain:
            return [domain]
        
        expansions = []
        # ç®€å•çš„é€šé…ç¬¦æ‰©å±•
        if domain.startswith('*.') and domain.count('*') == 1:
            base = domain[2:]  # ç§»é™¤*.å‰ç¼€
            expansions.append(base)
            # æ·»åŠ å¸¸è§å­åŸŸå
            common_subs = ['www', 'm', 'mobile', 'app', 'api', 'static', 'cdn', 'img', 'image']
            for sub in common_subs:
                expansions.append(f"{sub}.{base}")
        
        return expansions
    
    @staticmethod
    def generate_subdomains(domain: str) -> List[str]:
        """ç”Ÿæˆå¸¸è§å­åŸŸå"""
        subdomains = []
        common_subs = [
            'ad', 'ads', 'adserver', 'advert', 'advertising',
            'track', 'tracking', 'analytics', 'stats', 'stat',
            'click', 'clk', 'affiliate', 'aff', 'promo',
            'banner', 'popup', 'float', 'sponsor', 'sponsored',
            'media', 'video', 'img', 'image', 'static', 'cdn',
            'js', 'script', 'pixel', 'beacon', 'tag'
        ]
        
        for sub in common_subs:
            subdomains.append(f"{sub}.{domain}")
        
        return subdomains
    
    @staticmethod
    def is_ad_domain(domain: str) -> bool:
        """åˆ¤æ–­åŸŸåæ˜¯å¦å¯èƒ½æ˜¯å¹¿å‘ŠåŸŸå"""
        ad_patterns = [
            r'ad[0-9]*[\._-]', r'ads[0-9]*[\._-]', r'advert',
            r'track', r'tracking', r'analytics', r'stats',
            r'doubleclick', r'googlead', r'googlesyndication',
            r'facebook\.com/(plugins|widgets)',
            r'amazon-adsystem', r'moatads', r'scorecardresearch',
            r'quantserve', r'outbrain', r'taboola',
            r'adsystem', r'adservice', r'adserver'
        ]
        
        domain_lower = domain.lower()
        for pattern in ad_patterns:
            if re.search(pattern, domain_lower):
                return True
        
        # æ£€æŸ¥å¸¸è§å¹¿å‘ŠåŸŸååç¼€
        ad_suffixes = ['.ad.', '.ads.', '.adv.', '.advert.', '.advertising.']
        for suffix in ad_suffixes:
            if suffix in domain_lower:
                return True
        
        return False

class HitRateOptimizer:
    """å‘½ä¸­ç‡ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.domain_hits = defaultdict(int)
        self.pattern_hits = defaultdict(int)
        self.keyword_hits = defaultdict(int)
        
    def analyze_url(self, url: str) -> Dict:
        """åˆ†æURLç‰¹å¾"""
        parsed = urllib.parse.urlparse(url)
        domain = parsed.netloc.lower()
        path = parsed.path.lower()
        query = parsed.query.lower()
        
        features = {
            'domain': domain,
            'has_ad_keyword': False,
            'has_tracking': False,
            'has_analytics': False,
            'query_params': len(parsed.query) > 0,
            'path_length': len(path),
            'subdomain_count': domain.count('.')
        }
        
        # æ£€æŸ¥å¹¿å‘Šå…³é”®è¯
        ad_patterns = ['ad', 'ads', 'adv', 'advert', 'track', 'analytic', 'pixel', 'beacon']
        for pattern in ad_patterns:
            if pattern in domain or pattern in path or pattern in query:
                features['has_ad_keyword'] = True
                break
        
        # æ£€æŸ¥è¿½è¸ªå‚æ•°
        track_params = ['utm_', 'ref=', 'source=', 'campaign=', 'cid=', 'gclid=']
        for param in track_params:
            if param in query:
                features['has_tracking'] = True
                break
        
        return features
    
    def score_domain(self, domain: str) -> float:
        """ç»™åŸŸåè¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¯èƒ½æ˜¯å¹¿å‘Šï¼‰"""
        score = 0.0
        
        # åŸºæœ¬ç‰¹å¾è¯„åˆ†
        if DomainOptimizer.is_ad_domain(domain):
            score += 0.5
        
        # å…³é”®è¯åŒ¹é…
        for keyword in CONFIG['CHINESE_AD_KEYWORDS']:
            if keyword.lower() in domain.lower():
                score += 0.3
                break
        
        for keyword in CONFIG['MOBILE_AD_KEYWORDS']:
            if keyword.lower() in domain.lower():
                score += 0.2
                break
        
        # åŸŸåç»“æ„è¯„åˆ†
        parts = domain.split('.')
        if len(parts) >= 4:  # å¤šå±‚å­åŸŸåæ›´å¯èƒ½æ˜¯å¹¿å‘Š
            score += 0.2
        
        # æ£€æŸ¥æ•°å­—ç¼–å·ï¼ˆå¸¸è§äºå¹¿å‘ŠæœåŠ¡å™¨ï¼‰
        if re.search(r'\d{2,}', domain):
            score += 0.1
        
        return min(score, 1.0)
    
    def filter_low_score_domains(self, domains: Set[str], min_score: float = None) -> Set[str]:
        """è¿‡æ»¤ä½åˆ†åŸŸå"""
        if min_score is None:
            min_score = CONFIG['HIT_OPTIMIZATION']['min_hit_score']
        
        filtered = set()
        for domain in domains:
            score = self.score_domain(domain)
            if score >= min_score:
                filtered.add(domain)
        
        logger.info(f"åŸŸåè¿‡æ»¤: {len(domains)} -> {len(filtered)} (åˆ†æ•°é˜ˆå€¼: {min_score})")
        return filtered

class EnhancedAdBlockGenerator(AdBlockGenerator):
    """å¢å¼ºç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        super().__init__()
        self.optimizer = HitRateOptimizer()
        self.china_domains = set()
        self.enhanced_domains = set()
        
        # åˆ›å»ºé¢å¤–æºæ–‡ä»¶
        self.setup_enhanced_sources()
    
    def setup_enhanced_sources(self):
        """åˆ›å»ºå¢å¼ºæºæ–‡ä»¶"""
        # ä¸­æ–‡å¹¿å‘Šè§„åˆ™æº
        if not os.path.exists(CONFIG['CHINA_SOURCE']):
            with open(CONFIG['CHINA_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ä¸­æ–‡å¹¿å‘Šè¿‡æ»¤è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/hoshsadiq/adblock-nocoin-list/master/nocoin.txt\n")
                f.write("https://easylist-downloads.adblockplus.org/easylistchina.txt\n")
                f.write("https://gitee.com/xinggsf/Adblock-Rule/raw/master/rule.txt\n")
                f.write("https://raw.githubusercontent.com/cjx82630/cjxlist/master/cjx-annoyance.txt\n")
                f.write("https://anti-ad.net/easylist.txt\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ChineseFilter.txt\n")
        
        # å¢å¼ºè§„åˆ™æº
        if not os.path.exists(CONFIG['ENHANCED_SOURCE']):
            with open(CONFIG['ENHANCED_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# å¢å¼ºå¹¿å‘Šè¿‡æ»¤è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/filters.txt\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/badware.txt\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/privacy.txt\n")
                f.write("https://raw.githubusercontent.com/Spam404/lists/master/main-blacklist.txt\n")
                f.write("https://raw.githubusercontent.com/DandelionSprout/adfilt/master/Alternate%20versions%20Anti-Malware%20List/AntiMalwareAdBlocked.txt\n")
                f.write("https://raw.githubusercontent.com/Perflyst/PiHoleBlocklist/master/SmartTV.txt\n")
                f.write("https://raw.githubusercontent.com/Perflyst/PiHoleBlocklist/master/android-tracking.txt\n")
    
    def load_sources(self):
        """åŠ è½½æ‰€æœ‰è§„åˆ™æº"""
        super().load_sources()
        
        # åŠ è½½ä¸­æ–‡è§„åˆ™æº
        try:
            with open(CONFIG['CHINA_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.black_urls.append(line)
        except FileNotFoundError:
            logger.warning(f"ä¸­æ–‡è§„åˆ™æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['CHINA_SOURCE']}")
        
        # åŠ è½½å¢å¼ºè§„åˆ™æº
        try:
            with open(CONFIG['ENHANCED_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.black_urls.append(line)
        except FileNotFoundError:
            logger.warning(f"å¢å¼ºè§„åˆ™æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['ENHANCED_SOURCE']}")
        
        logger.info(f"æ€»å…±åŠ è½½ {len(self.black_urls)} ä¸ªè§„åˆ™æº")
    
    def enhanced_extract_domain(self, line: str) -> Optional[str]:
        """å¢å¼ºç‰ˆåŸŸåæå–"""
        if not line:
            return None
        
        line = line.strip()
        
        # ç§»é™¤æ³¨é‡Šå’Œå¤šä½™ç©ºæ ¼
        if '!' in line:
            line = line.split('!')[0].strip()
        if '#' in line:
            line = line.split('#')[0].strip()
        
        # è·³è¿‡ç©ºè¡Œå’Œç‰¹æ®Šè§„åˆ™
        if not line or line.startswith('!') or line.startswith('##'):
            return None
        
        # å¤„ç†Adblockè§„åˆ™
        patterns = [
            # æ ‡å‡†åŸŸåè§„åˆ™: ||domain.com^
            (r'^\|\|([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})\^', 1),
            # å¸¦å­åŸŸ: ||sub.domain.com^
            (r'^\|\|([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+){2,})\^', 1),
            # ç™½åå•è§„åˆ™: @@||domain.com^
            (r'^@@\|\|([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})\^', 1),
            # Hostsæ ¼å¼: 0.0.0.0 domain.com
            (r'^(?:0\.0\.0\.0|127\.0\.0\.1)\s+([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})', 1),
            # ç®€å•åŸŸå
            (r'^([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})$', 1),
            # é€šé…ç¬¦åŸŸå: *.domain.com
            (r'^(?:\*\.)?([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})', 1),
            # åŒ…å«ä¸‹åˆ’çº¿çš„åŸŸåï¼ˆè™½ç„¶ä¸è§„èŒƒä½†å®é™…å­˜åœ¨ï¼‰
            (r'([a-zA-Z0-9_-]+(\.[a-zA-Z0-9_-]+)+\.[a-zA-Z]{2,})', 1),
        ]
        
        for pattern, group in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(group).lower()
                # æ¸…ç†åŸŸå
                domain = re.sub(r'^www\d*\.', '', domain)
                domain = re.sub(r'^www\.', '', domain)
                domain = re.sub(r'^m\.', '', domain)
                domain = re.sub(r'^static\.', '', domain)
                domain = re.sub(r'^cdn\.', '', domain)
                
                # éªŒè¯åŸŸå
                if self.is_valid_enhanced_domain(domain):
                    return domain
        
        return None
    
    def is_valid_enhanced_domain(self, domain: str) -> bool:
        """å¢å¼ºç‰ˆåŸŸåéªŒè¯"""
        if not domain:
            return False
        
        # æ’é™¤é…ç½®ä¸­çš„åŸŸå
        if domain in CONFIG['EXCLUDE_DOMAINS']:
            return False
        
        # æ£€æŸ¥é•¿åº¦
        if len(domain) < 4 or len(domain) > 253:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‚¹å·
        if '.' not in domain:
            return False
        
        # æ£€æŸ¥é¡¶çº§åŸŸå
        parts = domain.split('.')
        tld = parts[-1]
        
        # è·³è¿‡å¤ªçŸ­çš„TLDï¼ˆå¯èƒ½æ˜¯è¯¯åŒ¹é…ï¼‰
        if len(tld) < 2:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå…¬å…±åç¼€
        if tld not in CONFIG['TLD_WHITELIST']:
            # å¦‚æœæ˜¯æ•°å­—TLDï¼ˆå¯èƒ½æ˜¯IPï¼‰ï¼Œè·³è¿‡
            if tld.isdigit():
                return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        for part in parts:
            if len(part) < 1 or len(part) > 63:
                return False
            # å…è®¸å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ï¼Œä½†ä¸èƒ½ä»¥è¿å­—ç¬¦å¼€å¤´æˆ–ç»“å°¾
            if not re.match(r'^[a-z0-9]([a-z0-9-]*[a-z0-9])?$', part):
                return False
        
        # é¢å¤–çš„å¹¿å‘ŠåŸŸåæ£€æŸ¥
        if not DomainOptimizer.is_ad_domain(domain):
            # å¦‚æœä¸æ˜¯æ˜æ˜¾çš„å¹¿å‘ŠåŸŸåï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå
            # è·³è¿‡çœ‹èµ·æ¥åƒè·¯å¾„çš„å­—ç¬¦ä¸²
            if '/' in domain or '?' in domain or '&' in domain:
                return False
        
        return True
    
    def parse_enhanced_content(self, content: str, source_type: str = 'black') -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
        """å¢å¼ºç‰ˆå†…å®¹è§£æ"""
        black_domains = set()
        black_rules = set()
        white_domains = set()
        white_rules = set()
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
            if line.startswith('!') or line.startswith('#') or line.startswith('//'):
                continue
            
            # ç™½åå•è§„åˆ™
            if line.startswith('@@'):
                domain = self.enhanced_extract_domain(line)
                if domain:
                    white_domains.add(domain)
                    white_rules.add(f"@@||{domain}^")
                else:
                    # ä¿ç•™éåŸŸåç™½åå•è§„åˆ™
                    if not re.match(r'^@@\|\|.*\^$', line):
                        white_rules.add(line)
            
            # é»‘åå•è§„åˆ™
            else:
                # å°è¯•æå–åŸŸå
                domain = self.enhanced_extract_domain(line)
                if domain:
                    black_domains.add(domain)
                    
                    # é€šé…ç¬¦æ‰©å±•
                    if CONFIG['HIT_OPTIMIZATION']['enable_wildcard_expansion'] and '*' in line:
                        expansions = DomainOptimizer.expand_wildcard_domain(domain)
                        for expanded in expansions:
                            if expanded != domain and self.is_valid_enhanced_domain(expanded):
                                black_domains.add(expanded)
                    
                    # å­åŸŸåç”Ÿæˆï¼ˆé’ˆå¯¹å¹¿å‘ŠåŸŸåï¼‰
                    if (CONFIG['HIT_OPTIMIZATION']['enable_subdomain_generation'] and 
                        DomainOptimizer.is_ad_domain(domain)):
                        subdomains = DomainOptimizer.generate_subdomains(domain)
                        for subdomain in subdomains:
                            if self.is_valid_enhanced_domain(subdomain):
                                black_domains.add(subdomain)
                
                # ä¿ç•™å…¶ä»–è§„åˆ™
                else:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆè§„åˆ™
                    if self.is_valid_rule(line):
                        black_rules.add(line)
        
        return black_domains, black_rules, white_domains, white_rules
    
    def is_valid_rule(self, rule: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆè§„åˆ™"""
        if not rule or len(rule) < 3:
            return False
        
        # è·³è¿‡æ˜æ˜¾æ— æ•ˆçš„è§„åˆ™
        invalid_patterns = [
            r'^\s*$',
            r'^##',
            r'^#\$#',
            r'^!\s+',
            r'^\[Adblock',
            r'^\/\*',
            r'^\*\/$'
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, rule):
                return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è‡³å°‘ä¸€ä¸ªå­—æ¯æˆ–æ•°å­—
        if not re.search(r'[a-zA-Z0-9]', rule):
            return False
        
        return True
    
    def download_and_parse_all(self):
        """å¢å¼ºç‰ˆä¸‹è½½å’Œè§£æ"""
        logger.info("å¼€å§‹ä¸‹è½½å’Œè§£æè§„åˆ™ï¼ˆå¢å¼ºç‰ˆï¼‰...")
        
        # å‡†å¤‡URLåˆ—è¡¨
        all_urls = []
        for url in self.black_urls:
            all_urls.append((url, 'black', 'normal'))
        
        # æ·»åŠ ä¸­æ–‡æº
        try:
            with open(CONFIG['CHINA_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        all_urls.append((line, 'black', 'china'))
        except:
            pass
        
        # å¹¶å‘ä¸‹è½½
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = {}
            for url, type_, source_category in all_urls:
                future = executor.submit(self.download_url, url)
                futures[future] = (url, type_, source_category)
            
            # å¤„ç†ç»“æœ
            processed = 0
            for future in concurrent.futures.as_completed(futures):
                processed += 1
                url, type_, source_category = futures[future]
                
                try:
                    content = future.result()
                    if content:
                        # é™åˆ¶å†…å®¹å¤§å°
                        if len(content) > 10 * 1024 * 1024:  # 10MB
                            logger.warning(f"å†…å®¹è¿‡å¤§ï¼Œæˆªå–å‰10MB: {url}")
                            content = content[:10 * 1024 * 1024]
                        
                        # è§£æå†…å®¹
                        black_domains, black_rules, white_domains, white_rules = self.parse_enhanced_content(content)
                        
                        # æ ¹æ®æ¥æºåˆ†ç±»å­˜å‚¨
                        if source_category == 'china':
                            self.china_domains.update(black_domains)
                        else:
                            self.black_domains.update(black_domains)
                            self.black_rules.update(black_rules)
                        
                        # ç™½åå•å¤„ç†
                        self.white_domains.update(white_domains)
                        self.white_rules.update(white_rules)
                        
                        logger.debug(f"å¤„ç†å®Œæˆ: {url} ({len(black_domains)} ä¸ªåŸŸå)")
                        
                        # è¿›åº¦æ˜¾ç¤º
                        if processed % 10 == 0:
                            logger.info(f"å¤„ç†è¿›åº¦: {processed}/{len(all_urls)}")
                
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
        
        logger.info(f"è§£æå®Œæˆ: é»‘åå•åŸŸå {len(self.black_domains):,} ä¸ª")
        logger.info(f"ä¸­æ–‡åŸŸå {len(self.china_domains):,} ä¸ª")
        logger.info(f"ç™½åå•åŸŸå {len(self.white_domains):,} ä¸ª")
    
    def apply_enhanced_whitelist(self):
        """å¢å¼ºç‰ˆç™½åå•åº”ç”¨"""
        if not self.white_domains:
            logger.warning("æ²¡æœ‰ç™½åå•åŸŸå")
            return
        
        original_black = len(self.black_domains)
        original_china = len(self.china_domains)
        
        # 1. ç›´æ¥åŒ¹é…ç§»é™¤
        self.black_domains -= self.white_domains
        self.china_domains -= self.white_domains
        
        # 2. å­åŸŸååŒ¹é…ï¼ˆæ›´ç²¾ç¡®ï¼‰
        if len(self.white_domains) < 10000:  # é¿å…æ€§èƒ½é—®é¢˜
            to_remove_black = set()
            to_remove_china = set()
            
            for white_domain in self.white_domains:
                # æ„å»ºæ­£åˆ™æ¨¡å¼ï¼ŒåŒ¹é…ä»¥ .whitedomain ç»“å°¾çš„åŸŸå
                pattern = re.compile(rf'.*\.{re.escape(white_domain)}$')
                
                # æ£€æŸ¥é»‘åå•
                for black_domain in self.black_domains:
                    if pattern.match(black_domain):
                        to_remove_black.add(black_domain)
                
                # æ£€æŸ¥ä¸­æ–‡åŸŸå
                for china_domain in self.china_domains:
                    if pattern.match(china_domain):
                        to_remove_china.add(china_domain)
            
            self.black_domains -= to_remove_black
            self.china_domains -= to_remove_china
            
            total_removed = (original_black + original_china) - (len(self.black_domains) + len(self.china_domains))
            logger.info(f"ç™½åå•åº”ç”¨å®Œæˆ: ç§»é™¤ {total_removed} ä¸ªåŸŸå")
    
    def optimize_for_hit_rate(self):
        """å‘½ä¸­ç‡ä¼˜åŒ–"""
        logger.info("å¼€å§‹å‘½ä¸­ç‡ä¼˜åŒ–...")
        
        # åˆå¹¶æ‰€æœ‰åŸŸå
        all_domains = self.black_domains.union(self.china_domains)
        logger.info(f"åˆå¹¶ååŸŸåæ€»æ•°: {len(all_domains):,}")
        
        # åº”ç”¨å‘½ä¸­ç‡ä¼˜åŒ–
        if CONFIG['HIT_OPTIMIZATION']['enable_smart_rules']:
            optimized_domains = self.optimizer.filter_low_score_domains(all_domains)
        else:
            optimized_domains = all_domains
        
        # é™åˆ¶æœ€å¤§åŸŸåæ•°
        max_domains = CONFIG['HIT_OPTIMIZATION']['max_domains_per_source']
        if len(optimized_domains) > max_domains:
            # ä¼˜å…ˆä¿ç•™é«˜åˆ†åŸŸå
            scored_domains = []
            for domain in optimized_domains:
                score = self.optimizer.score_domain(domain)
                scored_domains.append((domain, score))
            
            # æŒ‰åˆ†æ•°é™åºæ’åº
            scored_domains.sort(key=lambda x: x[1], reverse=True)
            
            # å–å‰Nä¸ª
            optimized_domains = set([d[0] for d in scored_domains[:max_domains]])
            logger.info(f"é™åˆ¶åŸŸåæ•°é‡: {max_domains:,}")
        
        self.black_domains = optimized_domains
        logger.info(f"ä¼˜åŒ–ååŸŸåæ€»æ•°: {len(self.black_domains):,}")
    
    def generate_smart_rules(self):
        """ç”Ÿæˆæ™ºèƒ½è§„åˆ™"""
        logger.info("ç”Ÿæˆæ™ºèƒ½è§„åˆ™...")
        
        # æ™ºèƒ½Adblockè§„åˆ™
        smart_rules = []
        
        # æ·»åŠ åŸºç¡€è§„åˆ™
        smart_rules.extend([
            "! æ™ºèƒ½å¹¿å‘Šè¿‡æ»¤è§„åˆ™",
            "! ç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "! åŸŸåæ•°é‡: " + str(len(self.black_domains)),
            ""
        ])
        
        # æ·»åŠ ç™½åå•è§„åˆ™
        if self.white_rules:
            smart_rules.append("! ç™½åå•è§„åˆ™")
            for rule in sorted(self.white_rules):
                smart_rules.append(rule)
            smart_rules.append("")
        
        # æŒ‰åŸŸåç±»å‹åˆ†ç»„
        domain_groups = defaultdict(list)
        for domain in self.black_domains:
            score = self.optimizer.score_domain(domain)
            
            if score >= 0.7:
                domain_groups['high'].append(domain)
            elif score >= 0.4:
                domain_groups['medium'].append(domain)
            else:
                domain_groups['low'].append(domain)
        
        # æŒ‰åˆ†ç»„æ·»åŠ è§„åˆ™
        for group_name in ['high', 'medium', 'low']:
            if domain_groups[group_name]:
                smart_rules.append(f"! {group_name.capitalize()} ä¼˜å…ˆçº§åŸŸå")
                for domain in sorted(domain_groups[group_name]):
                    smart_rules.append(f"||{domain}^")
                smart_rules.append("")
        
        # ä¿å­˜æ™ºèƒ½è§„åˆ™
        with open(CONFIG['OUTPUT_FILES']['smart_ad'], 'w', encoding='utf-8') as f:
            f.write('\n'.join(smart_rules))
        
        logger.info(f"æ™ºèƒ½è§„åˆ™ç”Ÿæˆå®Œæˆ: {len(smart_rules)} è¡Œ")
    
    def generate_mobile_rules(self):
        """ç”Ÿæˆç§»åŠ¨ç«¯ä¼˜åŒ–è§„åˆ™"""
        logger.info("ç”Ÿæˆç§»åŠ¨ç«¯ä¼˜åŒ–è§„åˆ™...")
        
        mobile_rules = [
            "! ç§»åŠ¨ç«¯å¹¿å‘Šè¿‡æ»¤è§„åˆ™",
            "! ç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "! ä¸“ä¸ºç§»åŠ¨è®¾å¤‡ä¼˜åŒ–",
            ""
        ]
        
        # ç­›é€‰ç§»åŠ¨ç«¯ç›¸å…³åŸŸå
        mobile_domains = set()
        for domain in self.black_domains:
            # æ£€æŸ¥æ˜¯å¦ä¸ºç§»åŠ¨å¹¿å‘Šç›¸å…³
            is_mobile = False
            
            # æ£€æŸ¥å…³é”®è¯
            for keyword in CONFIG['MOBILE_AD_KEYWORDS']:
                if keyword.lower() in domain.lower():
                    is_mobile = True
                    break
            
            # æ£€æŸ¥å¸¸è§ç§»åŠ¨å¹¿å‘Šæ¨¡å¼
            mobile_patterns = [
                r'^m\.', r'\.m\.', r'mobile', r'android', r'ios',
                r'app', r'sdk', r'inapp', r'interstitial'
            ]
            
            for pattern in mobile_patterns:
                if re.search(pattern, domain, re.IGNORECASE):
                    is_mobile = True
                    break
            
            if is_mobile:
                mobile_domains.add(domain)
        
        # æ·»åŠ ç§»åŠ¨ç«¯ä¸“ç”¨è§„åˆ™
        mobile_rules.append(f"! ç§»åŠ¨å¹¿å‘ŠåŸŸå: {len(mobile_domains)} ä¸ª")
        for domain in sorted(mobile_domains):
            mobile_rules.append(f"||{domain}^")
        
        # æ·»åŠ ç§»åŠ¨ç«¯ç‰¹å®šè§„åˆ™
        mobile_rules.extend([
            "",
            "! ç§»åŠ¨ç«¯ç‰¹å®šè§„åˆ™",
            "||inmobi.com^",
            "||ironsrc.com^",
            "||applovin.com^",
            "||unity3d.com^$app=com.android.browser",
            "||vungle.com^",
            "||chartboost.com^",
            "||adjust.com^",
            "||appsflyer.com^",
            "||branch.io^",
            "||facebook.com/plugins/^$subdocument",
            "||google.com/ads/^$subdocument",
            ""
        ])
        
        # ä¿å­˜ç§»åŠ¨ç«¯è§„åˆ™
        with open(CONFIG['OUTPUT_FILES']['mobile_ad'], 'w', encoding='utf-8') as f:
            f.write('\n'.join(mobile_rules))
        
        logger.info(f"ç§»åŠ¨ç«¯è§„åˆ™ç”Ÿæˆå®Œæˆ: {len(mobile_domains)} ä¸ªåŸŸå")
    
    def generate_files(self):
        """å¢å¼ºç‰ˆæ–‡ä»¶ç”Ÿæˆ"""
        logger.info("ç”Ÿæˆè§„åˆ™æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰...")
        
        # å…ˆè¿›è¡Œå‘½ä¸­ç‡ä¼˜åŒ–
        self.optimize_for_hit_rate()
        
        # è°ƒç”¨çˆ¶ç±»ç”ŸæˆåŸºç¡€æ–‡ä»¶
        super().generate_files()
        
        # ç”Ÿæˆæ™ºèƒ½è§„åˆ™
        self.generate_smart_rules()
        
        # ç”Ÿæˆç§»åŠ¨ç«¯è§„åˆ™
        if CONFIG['HIT_OPTIMIZATION']['enable_mobile_optimization']:
            self.generate_mobile_rules()
        
        # æ›´æ–°info.json
        self.update_info_file()
    
    def update_info_file(self):
        """æ›´æ–°ä¿¡æ¯æ–‡ä»¶"""
        info_file = CONFIG['OUTPUT_FILES']['info']
        with open(info_file, 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # æ·»åŠ å¢å¼ºä¿¡æ¯
        info['enhanced'] = {
            'smart_rules_generated': os.path.exists(CONFIG['OUTPUT_FILES']['smart_ad']),
            'mobile_rules_generated': os.path.exists(CONFIG['OUTPUT_FILES']['mobile_ad']),
            'china_domains_count': len(self.china_domains),
            'optimization_applied': True,
            'hit_optimization_config': CONFIG['HIT_OPTIMIZATION']
        }
        
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
    
    def generate_readme(self):
        """å¢å¼ºç‰ˆREADMEç”Ÿæˆ"""
        logger.info("ç”ŸæˆREADME.mdï¼ˆå¢å¼ºç‰ˆï¼‰...")
        
        # è·å–è§„åˆ™ä¿¡æ¯
        with open(CONFIG['OUTPUT_FILES']['info'], 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # ç”Ÿæˆé“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        version = info['version']
        
        readme_content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - å¢å¼ºç‰ˆ

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œç»è¿‡ä¼˜åŒ–æé«˜æ‹¦æˆªå‘½ä¸­ç‡ã€‚

## ğŸ“Š è§„åˆ™ç»Ÿè®¡

**ç‰ˆæœ¬ {version} è§„åˆ™ç»Ÿè®¡ï¼š**
- æ€»é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- ä¸­æ–‡å¹¿å‘ŠåŸŸåï¼š{info['enhanced']['china_domains_count']:,} ä¸ª
- å…¶ä»–è§„åˆ™ï¼šé»‘åå• {info['rules']['blacklist_rules']:,} æ¡ï¼Œç™½åå• {info['rules']['whitelist_rules']:,} æ¡

## ğŸš€ è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ | è¯´æ˜ |
|----------|----------|----------|----------|------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` | é€šç”¨è§„åˆ™ï¼Œé€‚åˆæ‰€æœ‰ç”¨æˆ· |
| æ™ºèƒ½å¹¿å‘Šè§„åˆ™ | Adblock | `{base_url}/smart_ad.txt` | `{cdn_url}/smart_ad.txt` | æ™ºèƒ½ä¼˜åŒ–ï¼Œé«˜å‘½ä¸­ç‡ |
| ç§»åŠ¨ç«¯è§„åˆ™ | Adblock | `{base_url}/mobile_ad.txt` | `{cdn_url}/mobile_ad.txt` | ç§»åŠ¨è®¾å¤‡ä¸“ç”¨ |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` | Pi-holeç­‰DNSè¿‡æ»¤å™¨ |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` | ç³»ç»ŸHostsæ–‡ä»¶ |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` | çº¯é»‘åå•åŸŸå |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` | æ’é™¤è¯¯æ€ |

## ğŸ¯ ä¼˜åŒ–ç‰¹æ€§

### 1. å‘½ä¸­ç‡ä¼˜åŒ–
- **æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿ**ï¼šæ¯ä¸ªåŸŸåæ ¹æ®å¹¿å‘Šç‰¹å¾è¯„åˆ†
- **ä¼˜å…ˆçº§è¿‡æ»¤**ï¼šä¼˜å…ˆä¿ç•™é«˜å¹¿å‘Šå¯èƒ½æ€§åŸŸå
- **ä¸­æ–‡ä¼˜åŒ–**ï¼šä¸“é—¨é’ˆå¯¹ä¸­æ–‡ç½‘ç«™å¹¿å‘Šä¼˜åŒ–
- **ç§»åŠ¨ç«¯ä¼˜åŒ–**ï¼šä¼˜åŒ–ç§»åŠ¨è®¾å¤‡å¹¿å‘Šæ‹¦æˆª

### 2. è§„åˆ™è´¨é‡
- **è‡ªåŠ¨å»é‡**ï¼šç§»é™¤é‡å¤åŸŸå
- **æœ‰æ•ˆæ€§éªŒè¯**ï¼šéªŒè¯åŸŸåæ ¼å¼å’Œæœ‰æ•ˆæ€§
- **ç™½åå•ä¿æŠ¤**ï¼šé¿å…è¯¯æ€æ­£å¸¸ç½‘ç«™
- **å®šæœŸæ›´æ–°**ï¼šæ¯å¤©è‡ªåŠ¨æ›´æ–°è§„åˆ™

### 3. æ€§èƒ½ä¼˜åŒ–
- **å¤šçº¿ç¨‹ä¸‹è½½**ï¼šå¹¶è¡Œä¸‹è½½è§„åˆ™æº
- **æ™ºèƒ½ç¼“å­˜**ï¼šå‡å°‘é‡å¤ä¸‹è½½
- **å¢é‡æ›´æ–°**ï¼šåªæ›´æ–°å˜åŒ–éƒ¨åˆ†

## ğŸ“… æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*

## ğŸ”§ ä½¿ç”¨å»ºè®®

1. **AdGuard/uBlock Origin**ï¼šä½¿ç”¨ `smart_ad.txt` è·å¾—æœ€ä½³å¹³è¡¡
2. **Pi-hole/AdGuard Home**ï¼šä½¿ç”¨ `dns.txt` è¿›è¡ŒDNSå±‚é¢æ‹¦æˆª
3. **ç§»åŠ¨è®¾å¤‡**ï¼šä½¿ç”¨ `mobile_ad.txt` ä¸“é—¨é’ˆå¯¹ç§»åŠ¨å¹¿å‘Š
4. **å¦‚æœé‡åˆ°è¯¯æ€**ï¼šæ£€æŸ¥ `white.txt` æˆ–æäº¤Issue

## ğŸ“ˆ å‘½ä¸­ç‡æå‡æŠ€å·§

1. å®šæœŸæ›´æ–°è§„åˆ™ï¼ˆè‡³å°‘æ¯å‘¨ä¸€æ¬¡ï¼‰
2. ç»“åˆä½¿ç”¨æ™ºèƒ½è§„åˆ™å’ŒåŸºç¡€è§„åˆ™
3. é’ˆå¯¹ç‰¹å®šç½‘ç«™æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
4. å…³æ³¨æ›´æ–°æ—¥å¿—ä¸­çš„ä¼˜åŒ–å†…å®¹

---

**æç¤º**ï¼šå¦‚æœå‘ç°è¯¯æ‹¦æˆªæˆ–æ¼æ‹¦æˆªï¼Œè¯·é€šè¿‡Issueåé¦ˆã€‚
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œå¢å¼ºç‰ˆæµç¨‹"""
        print("=" * 60)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - å¢å¼ºç‰ˆ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½è§„åˆ™æº
            self.load_sources()
            
            # 2. ä¸‹è½½å’Œè§£æè§„åˆ™ï¼ˆå¢å¼ºç‰ˆï¼‰
            self.download_and_parse_all()
            
            # 3. åº”ç”¨ç™½åå•ï¼ˆå¢å¼ºç‰ˆï¼‰
            self.apply_enhanced_whitelist()
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰
            self.generate_files()
            
            # 5. ç”ŸæˆREADME.mdï¼ˆå¢å¼ºç‰ˆï¼‰
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            print("\n" + "=" * 60)
            print("âœ… å¢å¼ºç‰ˆå¤„ç†å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ‡¨ğŸ‡³ ä¸­æ–‡åŸŸå: {len(self.china_domains):,}ä¸ª")
            print(f"âœ… ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ¯ æ™ºèƒ½è§„åˆ™: rules/outputs/smart_ad.txt")
            print(f"ğŸ“± ç§»åŠ¨ç«¯è§„åˆ™: rules/outputs/mobile_ad.txt")
            print(f"ğŸ“ æ‰€æœ‰è§„åˆ™: rules/outputs/")
            print(f"ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # æ˜¾ç¤ºæ¨¡å¼é€‰æ‹©
    print("è¯·é€‰æ‹©ç”Ÿæˆæ¨¡å¼ï¼š")
    print("1. åŸºç¡€æ¨¡å¼ï¼ˆå¿«é€Ÿï¼Œæ ‡å‡†è§„åˆ™ï¼‰")
    print("2. å¢å¼ºæ¨¡å¼ï¼ˆæ¨èï¼Œé«˜å‘½ä¸­ç‡ï¼‰")
    print("3. å®Œæ•´æ¨¡å¼ï¼ˆæœ€å…¨ï¼Œä½†è¾ƒæ…¢ï¼‰")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3, é»˜è®¤2): ").strip() or "2"
    
    if choice == "1":
        print("\nğŸš€ ä½¿ç”¨åŸºç¡€æ¨¡å¼...")
        generator = AdBlockGenerator()
    elif choice == "3":
        print("\nğŸ”¥ ä½¿ç”¨å®Œæ•´æ¨¡å¼...")
        # å®Œæ•´æ¨¡å¼é…ç½®
        CONFIG['HIT_OPTIMIZATION']['enable_smart_rules'] = True
        CONFIG['HIT_OPTIMIZATION']['enable_china_focus'] = True
        CONFIG['HIT_OPTIMIZATION']['enable_mobile_optimization'] = True
        CONFIG['HIT_OPTIMIZATION']['min_hit_score'] = 0.2  # æ›´ä½é˜ˆå€¼
        CONFIG['MAX_WORKERS'] = 15  # æ›´å¤šå¹¶å‘
        generator = EnhancedAdBlockGenerator()
    else:
        print("\nâš¡ ä½¿ç”¨å¢å¼ºæ¨¡å¼...")
        generator = EnhancedAdBlockGenerator()
    
    # è¿è¡Œç”Ÿæˆå™¨
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸ“Š å‘½ä¸­ç‡å»ºè®®ï¼šä½¿ç”¨ smart_ad.txt è·å¾—æœ€ä½³æ•ˆæœ")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
