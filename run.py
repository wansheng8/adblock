#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
AdBlock è§„åˆ™é›†åˆå™¨ - ä¼˜åŒ–ç‰ˆ
è‡ªåŠ¨ä»å¤šä¸ªæºæ”¶é›†å¹¿å‘Šè¿‡æ»¤è§„åˆ™ï¼Œåˆå¹¶å»é‡åç”Ÿæˆç»Ÿä¸€çš„è¿‡æ»¤è§„åˆ™æ–‡ä»¶
"""

import os
import re
import time
import requests
import threading
import queue
from datetime import datetime, timedelta, timezone
from typing import List, Set, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
import hashlib
import gzip
import json

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AdBlockRuleCollector:
    def __init__(self):
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.sources_dir = os.path.join(self.base_dir, "rules", "sources")
        self.outputs_dir = os.path.join(self.base_dir, "rules", "outputs")
        self.white_sources_file = os.path.join(self.sources_dir, "white.txt")
        self.black_sources_file = os.path.join(self.sources_dir, "black.txt")
        self.output_file = os.path.join(self.outputs_dir, "adblock.txt")
        self.stats_file = os.path.join(self.outputs_dir, "stats.json")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.sources_dir, exist_ok=True)
        os.makedirs(self.outputs_dir, exist_ok=True)
        
        # ç”¨æˆ·ä»£ç†
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # è§„åˆ™ç»Ÿè®¡
        self.stats = {
            'white_rules': 0,
            'black_rules': 0,
            'total_rules': 0,
            'sources_processed': 0,
            'sources_failed': 0,
            'duplicate_removed': 0,
            'white_sources': [],
            'black_sources': []
        }
        
        # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨é›†åˆå­˜å‚¨è§„åˆ™å“ˆå¸Œï¼Œè€Œä¸æ˜¯å®Œæ•´è§„åˆ™
        self.white_rules_hashes = set()
        self.black_rules_hashes = set()
        self.lock = threading.Lock()
        
        # ä¸´æ—¶æ–‡ä»¶å­˜å‚¨
        self.temp_dir = os.path.join(self.base_dir, "temp")
        os.makedirs(self.temp_dir, exist_ok=True)
        
    def load_sources(self, source_type: str) -> List[Tuple[str, str]]:
        """åŠ è½½è§„åˆ™æºURLåˆ—è¡¨ï¼Œè¿”å›(åç§°, URL)å…ƒç»„åˆ—è¡¨"""
        source_file = self.white_sources_file if source_type == 'white' else self.black_sources_file
        
        if not os.path.exists(source_file):
            # åˆ›å»ºé»˜è®¤æºæ–‡ä»¶
            default_sources = self._get_default_sources(source_type)
            with open(source_file, 'w', encoding='utf-8') as f:
                for name, url in default_sources:
                    f.write(f"{name} {url}\n")
            return default_sources
        
        sources = []
        with open(source_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split(maxsplit=1)
                    if len(parts) == 2:
                        name, url = parts
                        sources.append((name.strip(), url.strip()))
                    else:
                        # å¦‚æœæ²¡æœ‰åç§°ï¼Œä½¿ç”¨URLçš„ä¸€éƒ¨åˆ†ä½œä¸ºåç§°
                        url = line
                        name = self._extract_name_from_url(url)
                        sources.append((name, url))
        return sources
    
    def _extract_name_from_url(self, url: str) -> str:
        """ä»URLæå–åç§°"""
        # ç§»é™¤åè®®
        if '://' in url:
            url = url.split('://')[1]
        
        # ç§»é™¤è·¯å¾„ä¸­çš„é€šç”¨éƒ¨åˆ†
        name = url.replace('raw.githubusercontent.com/', '') \
                 .replace('github.com/', '') \
                 .replace('easylist-downloads.adblockplus.org/', '') \
                 .replace('easylist.to/', '') \
                 .replace('secure.fanboy.co.nz/', '')
        
        # é™åˆ¶é•¿åº¦
        if len(name) > 50:
            name = name[:50] + "..."
        
        return name
    
    def _get_default_sources(self, source_type: str) -> List[Tuple[str, str]]:
        """è·å–é»˜è®¤è§„åˆ™æº"""
        if source_type == 'white':
            return [
                ("Annoyances", "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_14_Annoyances/filter.txt"),
                ("EasyList China", "https://easylist-downloads.adblockplus.org/easylistchina.txt"),
                ("CJX Annoyance", "https://raw.githubusercontent.com/cjx82630/cjxlist/master/cjx-annoyance.txt")
            ]
        else:  # black
            return [
                ("AdGuard Base", "https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt"),
                ("EasyList", "https://easylist.to/easylist/easylist.txt"),
                ("Spyware Filter", "https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/SpywareFilter/sections/tracking_servers.txt"),
                ("Fanboy Annoyance", "https://secure.fanboy.co.nz/fanboy-annoyance.txt"),
                ("Anti-AD", "https://raw.githubusercontent.com/privacy-protection-tools/anti-AD/master/anti-ad-easylist.txt"),
                ("REIJI AD Collection", "https://raw.githubusercontent.com/REIJI007/Adblock-Rule-Collection/main/ADBLOCK_RULE_COLLECTION_DNS.txt")
            ]
    
    def fetch_rules(self, source_name: str, url: str, source_type: str) -> Dict:
        """ä»URLè·å–è§„åˆ™"""
        temp_file = os.path.join(self.temp_dir, f"{hashlib.md5(url.encode()).hexdigest()}.txt")
        
        try:
            # å…ˆå°è¯•ä»ç¼“å­˜è¯»å–ï¼ˆå¦‚æœæ–‡ä»¶å­˜åœ¨ä¸”å°äº1å°æ—¶ï¼‰
            if os.path.exists(temp_file):
                file_age = time.time() - os.path.getmtime(temp_file)
                if file_age < 3600:  # 1å°æ—¶ç¼“å­˜
                    with open(temp_file, 'r', encoding='utf-8') as f:
                        rules = [line.strip() for line in f if line.strip()]
                    
                    with self.lock:
                        if source_type == 'white':
                            self.stats['white_rules'] += len(rules)
                        else:
                            self.stats['black_rules'] += len(rules)
                        self.stats['sources_processed'] += 1
                    
                    print(f"âœ“ ä»ç¼“å­˜è¯»å–: {source_name} ({len(rules)} æ¡è§„åˆ™)")
                    return {'name': source_name, 'url': url, 'count': len(rules), 'rules': rules}
            
            # ä»ç½‘ç»œè·å–
            print(f"æ­£åœ¨è·å–: {source_name}")
            response = requests.get(url, headers=self.headers, timeout=60, verify=False)
            response.raise_for_status()
            
            rules = []
            seen_hashes = set()
            
            for line in response.text.splitlines():
                line = line.strip()
                if self._is_valid_rule(line):
                    # è®¡ç®—å“ˆå¸Œç”¨äºå»é‡
                    rule_hash = hashlib.md5(line.encode()).hexdigest()
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    with self.lock:
                        if source_type == 'white':
                            if rule_hash in self.white_rules_hashes:
                                self.stats['duplicate_removed'] += 1
                                continue
                            self.white_rules_hashes.add(rule_hash)
                        else:
                            if rule_hash in self.black_rules_hashes:
                                self.stats['duplicate_removed'] += 1
                                continue
                            self.black_rules_hashes.add(rule_hash)
                    
                    rules.append(line)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            with open(temp_file, 'w', encoding='utf-8') as f:
                for rule in rules:
                    f.write(rule + '\n')
            
            with self.lock:
                if source_type == 'white':
                    self.stats['white_rules'] += len(rules)
                else:
                    self.stats['black_rules'] += len(rules)
                self.stats['sources_processed'] += 1
            
            print(f"âœ“ æˆåŠŸè·å–: {source_name} ({len(rules)} æ¡è§„åˆ™)")
            return {'name': source_name, 'url': url, 'count': len(rules), 'rules': rules}
            
        except requests.exceptions.Timeout:
            error_msg = "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            error_msg = "è¿æ¥é”™è¯¯"
        except Exception as e:
            error_msg = str(e)
        
        with self.lock:
            self.stats['sources_failed'] += 1
        print(f"âœ— è·å–å¤±è´¥: {source_name} - {error_msg}")
        return {'name': source_name, 'url': url, 'count': 0, 'rules': [], 'error': error_msg}
    
    def _is_valid_rule(self, rule: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™"""
        if not rule or len(rule) > 1000:  # é˜²æ­¢è¿‡é•¿çš„è§„åˆ™
            return False
        
        # è·³è¿‡æ³¨é‡Šå’Œå…ƒæ•°æ®
        if rule.startswith('!'):
            return False
        if rule.startswith('[') or rule.startswith('#'):
            return False
        if '[' in rule and ']' in rule:  # å¯èƒ½åŒ…å«æ— æ•ˆå­—ç¬¦
            return False
        
        # æ£€æŸ¥è§„åˆ™ç±»å‹
        rule_lower = rule.lower()
        
        # å…ƒç´ éšè—è§„åˆ™
        if '##' in rule:
            return True
        
        # åŸŸåè§„åˆ™
        if rule.startswith('||') or rule.startswith('@@'):
            return True
        
        # URLè¿‡æ»¤è§„åˆ™
        if '^' in rule or '$' in rule:
            return True
        
        # åŒ…å«ç‰¹å®šå¹¿å‘Šå…³é”®è¯
        ad_keywords = ['ad', 'ads', 'advert', 'banner', 'popup', 'track', 'analytics', 
                      'cookie', 'sponsor', 'promo', 'doubleclick', 'googlead']
        for keyword in ad_keywords:
            if keyword in rule_lower:
                return True
        
        return False
    
    def _is_domain_rule(self, rule: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºåŸŸåè§„åˆ™"""
        return rule.startswith('||') or rule.startswith('@@')
    
    def _is_element_hiding_rule(self, rule: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå…ƒç´ éšè—è§„åˆ™"""
        return '##' in rule
    
    def _is_url_filter_rule(self, rule: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºURLè¿‡æ»¤è§„åˆ™"""
        return '^' in rule or '$' in rule
    
    def process_and_write_rules(self, all_rules_data: List[Dict]):
        """å¤„ç†å’Œå†™å…¥è§„åˆ™æ–‡ä»¶"""
        print("\nâš™ï¸ å¤„ç†å’Œåˆå¹¶è§„åˆ™...")
        
        # åˆ†é˜¶æ®µå¤„ç†è§„åˆ™
        white_rules = []
        black_rules = []
        
        for source_data in all_rules_data:
            if 'rules' in source_data:
                for rule in source_data['rules']:
                    if rule.startswith('@@'):  # ç™½åå•è§„åˆ™
                        white_rules.append(rule)
                    else:  # é»‘åå•è§„åˆ™
                        black_rules.append(rule)
        
        # å»é‡
        white_rules = list(dict.fromkeys(white_rules))  # ä¿æŒé¡ºåºçš„å»é‡
        black_rules = list(dict.fromkeys(black_rules))
        
        # åˆ†ç»„æ’åºï¼šåŸŸåè§„åˆ™ -> URLè§„åˆ™ -> å…ƒç´ éšè—è§„åˆ™
        white_domain_rules = [r for r in white_rules if self._is_domain_rule(r)]
        white_url_rules = [r for r in white_rules if self._is_url_filter_rule(r) and not self._is_domain_rule(r)]
        white_element_rules = [r for r in white_rules if self._is_element_hiding_rule(r)]
        
        black_domain_rules = [r for r in black_rules if self._is_domain_rule(r)]
        black_url_rules = [r for r in black_rules if self._is_url_filter_rule(r) and not self._is_domain_rule(r)]
        black_element_rules = [r for r in black_rules if self._is_element_hiding_rule(r)]
        
        # åˆå¹¶è§„åˆ™
        final_rules = []
        final_rules.extend(white_domain_rules)
        final_rules.extend(white_url_rules)
        final_rules.extend(white_element_rules)
        final_rules.extend(black_domain_rules)
        final_rules.extend(black_url_rules)
        final_rules.extend(black_element_rules)
        
        self.stats['total_rules'] = len(final_rules)
        
        print(f"ç™½åå•è§„åˆ™: {len(white_rules)} æ¡")
        print(f"é»‘åå•è§„åˆ™: {len(black_rules)} æ¡")
        print(f"æ€»è§„åˆ™æ•°: {len(final_rules)} æ¡")
        
        # ç”Ÿæˆè§„åˆ™æ–‡ä»¶å¤´
        shanghai_tz = timezone(timedelta(hours=8))
        update_time = datetime.now(shanghai_tz).strftime('%Y-%m-%d %H:%M:%S')
        
        file_header = f"""! Title: AdBlock ç»¼åˆè¿‡æ»¤è§„åˆ™
! Description: ç»¼åˆå¤šä¸ªä¼˜è´¨è§„åˆ™æºï¼ŒåŒ…å«å…ƒç´ éšè—ã€é”™è¯¯æ‹¦æˆªã€æ¨ªå¹…å¹¿å‘Šæ‹¦æˆªã€åˆ†æå·¥å…·æ‹¦æˆªã€å¼¹çª—å¹¿å‘Šæ‹¦æˆªç­‰
! Version: {datetime.now().strftime('%Y%m%d')}
! TimeUpdated: {update_time} (ä¸Šæµ·æ—¶é—´)
! Homepage: https://github.com/wansheng8/adblock
! Expires: 1 days
! Total rules: {len(final_rules)}
! Memory optimized: yes
!
! è§„åˆ™æ¥æº:
"""
        
        # æ·»åŠ è§„åˆ™æºä¿¡æ¯
        for source in all_rules_data:
            if 'rules' in source:
                file_header += f"! - {source['name']}: {source['count']} æ¡è§„åˆ™\n"
        
        file_header += "\n! ç™½åå•è§„åˆ™ (æ”¾è¡Œè§„åˆ™)\n"
        
        # å†™å…¥æ··åˆè§„åˆ™æ–‡ä»¶
        print(f"\nğŸ’¾ å†™å…¥è§„åˆ™æ–‡ä»¶: {self.output_file}")
        
        # åˆ†æ‰¹å†™å…¥ï¼Œé¿å…å†…å­˜é—®é¢˜
        batch_size = 50000
        with open(self.output_file, 'w', encoding='utf-8') as f:
            f.write(file_header)
            
            # åˆ†æ‰¹å†™å…¥è§„åˆ™
            for i in range(0, len(final_rules), batch_size):
                batch = final_rules[i:i + batch_size]
                for rule in batch:
                    f.write(rule + '\n')
                
                if i + batch_size < len(final_rules):
                    print(f"  å·²å†™å…¥ {i + batch_size}/{len(final_rules)} æ¡è§„åˆ™...")
        
        # å†™å…¥å‹ç¼©ç‰ˆæœ¬
        self._create_compressed_version()
        
        # å†™å…¥å•ç‹¬çš„è§„åˆ™æ–‡ä»¶
        print("\nğŸ“ ç”Ÿæˆå…¶ä»–æ ¼å¼è§„åˆ™æ–‡ä»¶...")
        
        with open(os.path.join(self.outputs_dir, "white_only.txt"), 'w', encoding='utf-8') as f:
            f.write("! ä»…ç™½åå•è§„åˆ™\n")
            for rule in white_rules:
                f.write(rule + '\n')
        
        with open(os.path.join(self.outputs_dir, "black_only.txt"), 'w', encoding='utf-8') as f:
            f.write("! ä»…é»‘åå•è§„åˆ™\n")
            for rule in black_rules:
                f.write(rule + '\n')
        
        # å†™å…¥ç»Ÿè®¡æ–‡ä»¶
        with open(self.stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
    
    def _create_compressed_version(self):
        """åˆ›å»ºå‹ç¼©ç‰ˆæœ¬"""
        try:
            with open(self.output_file, 'rb') as f_in:
                with gzip.open(self.output_file + '.gz', 'wb') as f_out:
                    f_out.writelines(f_in)
            print(f"âœ“ å·²åˆ›å»ºå‹ç¼©ç‰ˆæœ¬: {self.output_file}.gz")
        except Exception as e:
            print(f"âœ— åˆ›å»ºå‹ç¼©ç‰ˆæœ¬å¤±è´¥: {e}")
    
    def generate_readme(self, all_rules_data: List[Dict]) -> str:
        """ç”ŸæˆREADME.mdæ–‡ä»¶"""
        # è·å–ä¸Šæµ·æ—¶é—´
        shanghai_tz = timezone(timedelta(hours=8))
        update_time = datetime.now(shanghai_tz).strftime('%Y-%m-%d %H:%M:%S')
        
        # ç”Ÿæˆè¡¨æ ¼
        table_lines = []
        table_lines.append("| ç±»å‹ | æºåç§° | è§„åˆ™æ•°é‡ | çŠ¶æ€ | é“¾æ¥ |")
        table_lines.append("|------|--------|----------|------|------|")
        
        # ç»Ÿè®¡æˆåŠŸçš„æº
        success_count = 0
        fail_count = 0
        
        for source in all_rules_data:
            if 'error' in source:
                status = "âŒ å¤±è´¥"
                count_str = "0"
                fail_count += 1
            else:
                status = "âœ… æˆåŠŸ"
                count_str = str(source['count'])
                success_count += 1
            
            # ç¼©çŸ­URLæ˜¾ç¤º
            display_url = source['url']
            if len(display_url) > 50:
                display_url = display_url[:50] + "..."
            
            source_type = "ç™½åå•" if any(x in source['name'].lower() for x in ['annoy', 'whitelist']) else "é»‘åå•"
            
            table_lines.append(f"| {source_type} | {source['name']} | {count_str} | {status} | [{display_url}]({source['url']}) |")
        
        table_content = "\n".join(table_lines)
        
        # ç”ŸæˆREADMEå†…å®¹
        readme_content = f"""# ğŸ›¡ï¸ AdBlock è§„åˆ™é›†åˆå™¨

ä¸€ä¸ªç²¾å‡†ã€é«˜æ•ˆçš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆå™¨ï¼Œè‡ªåŠ¨ä»å¤šä¸ªä¼˜è´¨è§„åˆ™æºæ”¶é›†å’Œåˆå¹¶å¹¿å‘Šè¿‡æ»¤è§„åˆ™ã€‚

## ğŸ“Š è§„åˆ™è®¢é˜…

{table_content}

## ğŸ“… æœ€æ–°æ›´æ–°æ—¶é—´

**{update_time}** (ä¸Šæµ·æ—¶é—´)

## ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯

- âœ… æˆåŠŸæº: **{success_count}** ä¸ª
- âŒ å¤±è´¥æº: **{fail_count}** ä¸ª
- ğŸ“ æ€»è§„åˆ™æ•°: **{self.stats['total_rules']:,}** æ¡
- ğŸ¯ ç™½åå•è§„åˆ™: {self.stats['white_rules']:,} æ¡
- ğŸ›¡ï¸ é»‘åå•è§„åˆ™: {self.stats['black_rules']:,} æ¡
- ğŸ”„ é‡å¤ç§»é™¤: {self.stats['duplicate_removed']:,} æ¡

## ğŸ”— è®¢é˜…é“¾æ¥

### ä¸»è¦è®¢é˜…
- **æ··åˆè§„åˆ™ (æ¨è)**: [adblock.txt](https://raw.githubusercontent.com/wansheng8/adblock/main/rules/outputs/adblock.txt)
- **å‹ç¼©ç‰ˆæœ¬**: [adblock.txt.gz](https://raw.githubusercontent.com/wansheng8/adblock/main/rules/outputs/adblock.txt.gz)

### ä¸“ç”¨è®¢é˜…
- **ä»…é»‘åå•**: [black_only.txt](https://raw.githubusercontent.com/wansheng8/adblock/main/rules/outputs/black_only.txt)
- **ä»…ç™½åå•**: [white_only.txt](https://raw.githubusercontent.com/wansheng8/adblock/main/rules/outputs/white_only.txt)

### ğŸ“Š ç»Ÿè®¡æ–‡ä»¶
- [stats.json](https://raw.githubusercontent.com/wansheng8/adblock/main/rules/outputs/stats.json)

## âš¡ ä½¿ç”¨è¯´æ˜

1. å®‰è£…å¹¿å‘Šè¿‡æ»¤æ‰©å±•ï¼ˆå¦‚ uBlock Originã€AdGuardï¼‰
2. æ·»åŠ è®¢é˜…é“¾æ¥åˆ°è¿‡æ»¤å™¨
3. å»ºè®®ä½¿ç”¨å‹ç¼©ç‰ˆæœ¬ä»¥å‡å°‘æµé‡
4. äº«å—æ¸…çˆ½çš„ä¸Šç½‘ä½“éªŒ

## ğŸ”„ è‡ªåŠ¨æ›´æ–°

è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œç¡®ä¿æœ€æ–°çš„å¹¿å‘Šè¿‡æ»¤æ•ˆæœã€‚

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

- ä½¿ç”¨å¤šçº¿ç¨‹ä¸‹è½½
- æ™ºèƒ½è§„åˆ™å»é‡
- å†…å­˜ä¼˜åŒ–å¤„ç†
- æ”¯æŒè¶…å¤§è§„åˆ™é›†
- æä¾›å‹ç¼©ç‰ˆæœ¬

---

*æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ï¼Œè¯·åˆç†ä½¿ç”¨å¹¿å‘Šè¿‡æ»¤åŠŸèƒ½ã€‚*
"""
        
        return readme_content
    
    def run(self):
        """ä¸»è¿è¡Œå‡½æ•°"""
        print("=" * 70)
        print("ğŸ›¡ï¸  AdBlock è§„åˆ™é›†åˆå™¨ v2.0 - ä¼˜åŒ–ç‰ˆ")
        print("=" * 70)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆè¶…è¿‡1å¤©çš„ï¼‰
        self._cleanup_temp_files()
        
        # åŠ è½½æº
        print("\nğŸ“ åŠ è½½è§„åˆ™æº...")
        white_sources = self.load_sources('white')
        black_sources = self.load_sources('black')
        
        print(f"ç™½åå•æº: {len(white_sources)} ä¸ª")
        print(f"é»‘åå•æº: {len(black_sources)} ä¸ª")
        
        # å¤šçº¿ç¨‹è·å–è§„åˆ™
        print("\nğŸŒ å¼€å§‹è·å–è§„åˆ™...")
        all_rules_data = []
        
        with ThreadPoolExecutor(max_workers=5) as executor:  # å‡å°‘çº¿ç¨‹æ•°ä»¥é¿å…é™åˆ¶
            futures = []
            
            # æäº¤ç™½åå•ä»»åŠ¡
            for name, url in white_sources:
                futures.append(executor.submit(self.fetch_rules, name, url, 'white'))
            
            # æäº¤é»‘åå•ä»»åŠ¡
            for name, url in black_sources:
                futures.append(executor.submit(self.fetch_rules, name, url, 'black'))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œæ˜¾ç¤ºè¿›åº¦
            completed = 0
            total = len(futures)
            
            for future in as_completed(futures):
                try:
                    result = future.result()
                    all_rules_data.append(result)
                    completed += 1
                    print(f"è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%)")
                except Exception as e:
                    print(f"ä»»åŠ¡æ‰§è¡Œé”™è¯¯: {e}")
                    completed += 1
        
        # å¤„ç†å¹¶å†™å…¥è§„åˆ™
        self.process_and_write_rules(all_rules_data)
        
        # ç”ŸæˆREADME
        print("\nğŸ“„ ç”ŸæˆREADME.md...")
        readme_content = self.generate_readme(all_rules_data)
        
        try:
            with open(os.path.join(self.base_dir, "README.md"), 'w', encoding='utf-8') as f:
                f.write(readme_content)
            print("âœ… README.md ç”ŸæˆæˆåŠŸ")
        except Exception as e:
            print(f"âŒ ç”ŸæˆREADME.mdå¤±è´¥: {e}")
            # å°è¯•åˆ›å»ºç®€åŒ–çš„README
            with open(os.path.join(self.base_dir, "README.md"), 'w', encoding='utf-8') as f:
                f.write(f"# AdBlock Rules\n\næ›´æ–°äº: {datetime.now().strftime('%Y-%m-%d')}\n")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 70)
        print("ğŸ‰ æ‰§è¡Œå®Œæˆï¼")
        print("=" * 70)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»è§„åˆ™æ•°: {self.stats['total_rules']:,}")
        print(f"  æˆåŠŸæº: {self.stats['sources_processed']}")
        print(f"  å¤±è´¥æº: {self.stats['sources_failed']}")
        print(f"  é‡å¤ç§»é™¤: {self.stats['duplicate_removed']:,}")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print(f"  {self.output_file}")
        print(f"  {self.output_file}.gz")
        print("=" * 70)
    
    def _cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            for filename in os.listdir(self.temp_dir):
                filepath = os.path.join(self.temp_dir, filename)
                if os.path.isfile(filepath):
                    file_age = time.time() - os.path.getmtime(filepath)
                    if file_age > 86400:  # 24å°æ—¶
                        os.remove(filepath)
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        collector = AdBlockRuleCollector()
        collector.run()
        return 0
    except MemoryError:
        print("âŒ å†…å­˜ä¸è¶³ï¼Œè¯·å‡å°‘è§„åˆ™æºæˆ–å¢åŠ å†…å­˜")
        return 1
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
