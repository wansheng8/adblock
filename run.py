#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - å®Œæ•´è§£å†³æ–¹æ¡ˆ
"""

import os
import re
import json
import time
import concurrent.futures
from datetime import datetime, timedelta
from typing import Set, List, Tuple, Optional, Dict
import requests

# é…ç½®ä¿¡æ¯
CONFIG = {
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    'MAX_WORKERS': 8,
    'TIMEOUT': 20,
    'RETRY': 3,
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    'PROTECTED_DOMAINS': {
        'google.com', 'github.com', 'microsoft.com', 'apple.com',
        'baidu.com', 'qq.com', 'taobao.com', 'jd.com', 'weibo.com'
    }
}

class AdBlockGenerator:
    def __init__(self):
        # ä¸»è§„åˆ™é›†åˆ
        self.black_domains = set()           # æœ€ç»ˆé»‘åå•åŸŸå
        self.all_black_domains = set()       # æ‰€æœ‰æ”¶é›†åˆ°çš„é»‘åå•åŸŸåï¼ˆåŒ…å«å¯èƒ½è¢«ç§»é™¤çš„ï¼‰
        self.all_white_domains = set()       # æ‰€æœ‰æ”¶é›†åˆ°çš„ç™½åå•åŸŸå
        self.black_rules = set()             # å¤æ‚é»‘åå•è§„åˆ™
        self.white_rules = set()             # å¤æ‚ç™½åå•è§„åˆ™
        
        # æ¥æºè¿½è¸ª
        self.blacklist_sources = []          # é»‘åå•æºURL
        self.whitelist_sources = []          # ç™½åå•æºURL
        self.processed_urls = []             # å·²å¤„ç†çš„URL
        
        # åˆ›å»ºç›®å½•
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºé»˜è®¤è§„åˆ™æº
        self.create_default_sources()
    
    def get_beijing_time(self) -> datetime:
        """è·å–åŒ—äº¬æ—¶é—´"""
        try:
            from datetime import timezone
            utc_now = datetime.now(timezone.utc)
            beijing_time = utc_now + timedelta(hours=8)
            return beijing_time
        except:
            return datetime.now()
    
    def create_default_sources(self):
        """åˆ›å»ºé»˜è®¤è§„åˆ™æºæ–‡ä»¶"""
        # é»‘åå•æº
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# å¹¿å‘Šè¿‡æ»¤è§„åˆ™æº\n")
                f.write("# æ¯è¡Œä¸€ä¸ªURL\n\n")
                f.write("# 1. AdGuardåŸºç¡€å¹¿å‘Šè§„åˆ™ï¼ˆåŒ…å«ç™½åå•ï¼‰\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n\n")
                f.write("# 2. EasyListè§„åˆ™ï¼ˆåŒ…å«ç™½åå•ï¼‰\n")
                f.write("https://easylist.to/easylist/easylist.txt\n\n")
                f.write("# 3. ä¸­æ–‡è§„åˆ™\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ADGUARD_FILTER.txt\n")
        
        # ç™½åå•æº
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("# åªåŒ…å«ä»¥@@å¼€å¤´çš„è§„åˆ™\n\n")
                f.write("# AdGuardç™½åå•\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n\n")
                f.write("# æ‰‹åŠ¨æ·»åŠ é‡è¦ç™½åå•\n")
                f.write("# @@||google.com^\n")
                f.write("# @@||github.com^\n")
    
    def download_content(self, url: str) -> Optional[str]:
        """ä¸‹è½½è§„åˆ™å†…å®¹"""
        for i in range(CONFIG['RETRY']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain, */*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Connection': 'keep-alive'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(1)
                else:
                    print(f"  âŒ ä¸‹è½½å¤±è´¥ {url}: {str(e)[:100]}")
        return None
    
    def normalize_domain(self, domain: str) -> str:
        """æ ‡å‡†åŒ–åŸŸå"""
        if not domain:
            return ""
        
        domain = domain.lower().strip()
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]
        if domain.startswith('*.'):
            domain = domain[2:]
        
        # ç§»é™¤å¸¸è§åç¼€
        if domain.endswith('.'):
            domain = domain[:-1]
        
        return domain
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        domain = self.normalize_domain(domain)
        
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        local_domains = {
            'localhost', 'local', 'broadcasthost',
            '0.0.0.0', '127.0.0.1', '::1',
            'ip6-localhost', 'ip6-loopback'
        }
        if domain in local_domains:
            return False
        
        # æ’é™¤IPåœ°å€
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', domain):
            return False
        
        # æ£€æŸ¥åŸŸåæ ¼å¼
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        for part in parts:
            if not part or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def extract_domain_from_rule(self, rule: str) -> Tuple[Optional[str], bool]:
        """ä»è§„åˆ™ä¸­æå–åŸŸåå’Œåˆ¤æ–­æ˜¯å¦æ˜¯ç™½åå•"""
        rule = rule.strip()
        if not rule:
            return None, False
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯ç™½åå•
        is_whitelist = rule.startswith('@@')
        original_rule = rule
        
        # å¦‚æœæ˜¯ç™½åå•è§„åˆ™ï¼Œç§»é™¤@@å‰ç¼€
        if is_whitelist:
            rule = rule[2:]
        
        # å°è¯•åŒ¹é…å¸¸è§æ ¼å¼
        patterns = [
            r'^\|\|([a-zA-Z0-9.-]+)\^',          # ||domain.com^
            r'^\|\|([a-zA-Z0-9.-]+)\/',          # ||domain.com/
            r'^([a-zA-Z0-9.-]+)\^',              # domain.com^
            r'^([a-zA-Z0-9.-]+)$',               # domain.com
            r'^\d+\.\d+\.\d+\.\d+\s+([a-zA-Z0-9.-]+)',  # 0.0.0.0 domain.com
            r'^\*\.([a-zA-Z0-9.-]+)',            # *.domain.com
        ]
        
        for pattern in patterns:
            match = re.match(pattern, rule)
            if match:
                domain = self.normalize_domain(match.group(1))
                if self.is_valid_domain(domain):
                    return domain, is_whitelist
        
        return None, is_whitelist
    
    def classify_rule(self, line: str) -> Tuple[Optional[str], Optional[str], str]:
        """åˆ†ç±»è§„åˆ™ç±»å‹"""
        line = line.strip()
        if not line:
            return None, None, ""
        
        # è·³è¿‡æ³¨é‡Š
        if line.startswith('!') or line.startswith('#'):
            return None, None, ""
        
        # æå–åŸŸåå¹¶åˆ¤æ–­ç±»å‹
        domain, is_whitelist = self.extract_domain_from_rule(line)
        
        if domain:
            if is_whitelist:
                return 'white_domain', domain, f"@@||{domain}^"
            else:
                return 'black_domain', domain, f"||{domain}^"
        else:
            # å¤æ‚è§„åˆ™
            if is_whitelist:
                return 'white_rule', None, line
            else:
                if len(line) > 3 and re.search(r'[a-zA-Z0-9]', line):
                    return 'black_rule', None, line
        
        return None, None, ""
    
    def process_url(self, url: str, source_type: str) -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
        """å¤„ç†å•ä¸ªè§„åˆ™æºURL"""
        print(f"  ğŸ“¥ ä¸‹è½½: {url}")
        content = self.download_content(url)
        if not content:
            return set(), set(), set(), set()
        
        black_domains = set()
        white_domains = set()
        black_rules = set()
        white_rules = set()
        
        lines_processed = 0
        for line in content.split('\n'):
            lines_processed += 1
            rule_type, domain, rule = self.classify_rule(line)
            
            if rule_type == 'black_domain':
                black_domains.add(domain)
            elif rule_type == 'white_domain':
                white_domains.add(domain)
                if rule:
                    white_rules.add(rule)
            elif rule_type == 'black_rule':
                black_rules.add(rule)
            elif rule_type == 'white_rule':
                white_rules.add(rule)
        
        print(f"  âœ“ å¤„ç†å®Œæˆ: {lines_processed} è¡Œ")
        print(f"    é»‘åå•åŸŸå: {len(black_domains)}, ç™½åå•åŸŸå: {len(white_domains)}")
        
        return black_domains, white_domains, black_rules, white_rules
    
    def load_sources(self):
        """åŠ è½½è§„åˆ™æºURL"""
        print("ğŸ” åŠ è½½è§„åˆ™æº...")
        
        # è¯»å–é»‘åå•æº
        if os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.blacklist_sources.append(line)
        
        # è¯»å–ç™½åå•æº
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.whitelist_sources.append(line)
        
        print(f"  é»‘åå•æº: {len(self.blacklist_sources)} ä¸ª")
        print(f"  ç™½åå•æº: {len(self.whitelist_sources)} ä¸ª")
        
        if not self.blacklist_sources and not self.whitelist_sources:
            print("  âš ï¸ æœªæ‰¾åˆ°è§„åˆ™æºURL")
            return False
        
        return True
    
    def process_all_sources(self):
        """å¤„ç†æ‰€æœ‰è§„åˆ™æº"""
        all_urls = []
        
        # é»‘åå•æº
        for url in self.blacklist_sources:
            all_urls.append(('black', url))
        
        # ç™½åå•æº
        for url in self.whitelist_sources:
            all_urls.append(('white', url))
        
        if not all_urls:
            return
        
        print(f"ğŸ”„ å¼€å§‹å¤„ç† {len(all_urls)} ä¸ªè§„åˆ™æº...")
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰è§„åˆ™æº
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_url = {}
            for source_type, url in all_urls:
                future = executor.submit(self.process_url, url, source_type)
                future_to_url[future] = (source_type, url)
                self.processed_urls.append(url)
            
            # æ”¶é›†ç»“æœ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_url):
                source_type, url = future_to_url[future]
                try:
                    bd, wd, br, wr = future.result(timeout=30)
                    results.append((bd, wd, br, wr))
                    completed += 1
                    print(f"  âœ“ [{completed}/{len(all_urls)}] å®Œæˆ: {url}")
                except Exception as e:
                    print(f"  âŒ å¤„ç†å¤±è´¥ {url}: {e}")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        total_black = 0
        total_white = 0
        for bd, wd, br, wr in results:
            self.all_black_domains.update(bd)
            self.all_white_domains.update(wd)
            self.black_rules.update(br)
            self.white_rules.update(wr)
            total_black += len(bd)
            total_white += len(wd)
        
        print(f"âœ… è§£æå®Œæˆ:")
        print(f"   æ”¶é›†åˆ°çš„é»‘åå•åŸŸå: {len(self.all_black_domains):,} ä¸ª")
        print(f"   æ”¶é›†åˆ°çš„ç™½åå•åŸŸå: {len(self.all_white_domains):,} ä¸ª")
        print(f"   å¤æ‚è§„åˆ™: é»‘åå• {len(self.black_rules):,} æ¡, ç™½åå• {len(self.white_rules):,} æ¡")
    
    def apply_whitelist_logic(self):
        """
        åº”ç”¨ç™½åå•é€»è¾‘ - æ ¸å¿ƒç®—æ³•
        å¤„ç†é»‘åå•æºä¸­çš„ç™½åå•è§„åˆ™
        """
        print("ğŸ¤” åº”ç”¨ç™½åå•é€»è¾‘...")
        
        # å¤‡ä»½åŸå§‹æ•°æ®
        self.black_domains = self.all_black_domains.copy()
        original_count = len(self.black_domains)
        
        # ç­–ç•¥1: ä¿æŠ¤é‡è¦åŸŸåï¼ˆç»å¯¹ä¸åˆ é™¤ï¼‰
        protected_domains = set()
        for domain in self.black_domains:
            for protected in CONFIG['PROTECTED_DOMAINS']:
                if domain == protected or domain.endswith(f".{protected}"):
                    protected_domains.add(domain)
                    break
        
        print(f"  ğŸ›¡ï¸  ä¿æŠ¤ {len(protected_domains)} ä¸ªé‡è¦åŸŸå")
        
        # ç­–ç•¥2: åº”ç”¨ç™½åå•è§„åˆ™ï¼ˆåˆ†ä¼˜å…ˆçº§ï¼‰
        
        # 2.1 é¦–å…ˆåº”ç”¨ç™½åå•æºä¸­çš„ç™½åå•ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        whitelist_source_removals = set()
        for white_domain in self.all_white_domains:
            if white_domain in self.black_domains and white_domain not in protected_domains:
                whitelist_source_removals.add(white_domain)
        
        # 2.2 ç„¶ååº”ç”¨é»‘åå•æºä¸­çš„ç™½åå•ï¼ˆè¾ƒä½ä¼˜å…ˆçº§ï¼‰
        # æ³¨æ„ï¼šé»‘åå•æºä¸­çš„ç™½åå•é€šå¸¸æ˜¯ä¸ºäº†ä¿®å¤ç‰¹å®šé—®é¢˜
        # æˆ‘ä»¬åº”ç”¨è¿™äº›è§„åˆ™ï¼Œä½†å¯ä»¥è®°å½•æ—¥å¿—
        
        # æˆ‘ä»¬å·²ç»åœ¨ process_url ä¸­æ”¶é›†äº†ç™½åå•åŸŸå
        # ç›´æ¥åº”ç”¨
        blacklist_source_removals = set()
        # ï¼ˆæ³¨æ„ï¼šself.all_white_domains å·²ç»åŒ…å«äº†æ‰€æœ‰æ¥æºçš„ç™½åå•ï¼‰
        
        # ç§»é™¤æ“ä½œï¼ˆåªç§»é™¤å®Œå…¨åŒ¹é…ï¼Œä¸ç§»é™¤å­åŸŸåï¼‰
        domains_to_remove = whitelist_source_removals  # ç°åœ¨åªç§»é™¤ç™½åå•æºä¸­çš„
        
        # ä¸ç§»é™¤å­åŸŸåï¼Œåªç§»é™¤å®Œå…¨åŒ¹é…çš„ï¼ˆé˜²æ­¢è¯¯åˆ ï¼‰
        self.black_domains -= domains_to_remove
        
        removed = original_count - len(self.black_domains)
        
        print(f"  ğŸ”„ ç™½åå•å¤„ç†ç»Ÿè®¡:")
        print(f"    ä»é»‘åå•ä¸­ç§»é™¤: {removed} ä¸ªåŸŸå")
        print(f"    å—ä¿æŠ¤çš„åŸŸå: {len(protected_domains)} ä¸ª")
        print(f"    å‰©ä½™é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª")
        
        # è¯¦ç»†æ—¥å¿—
        if removed > 0:
            print(f"  ğŸ“‹ ç§»é™¤çš„åŸŸåç¤ºä¾‹ï¼ˆæœ€å¤šæ˜¾ç¤º5ä¸ªï¼‰:")
            sample = list(domains_to_remove)[:5]
            for domain in sample:
                print(f"    - {domain}")
        
        return removed
    
    def generate_files(self):
        """ç”Ÿæˆå„ç§æ ¼å¼çš„è§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # ä½¿ç”¨åŒ—äº¬æ—¶é—´
        beijing_time = self.get_beijing_time()
        version = beijing_time.strftime('%Y%m%d')
        timestamp = beijing_time.strftime('%Y-%m-%d %H:%M:%S')
        
        # å¯¹åŸŸåæ’åº
        sorted_black_domains = sorted(self.black_domains)
        
        # 1. AdBlockæ ¼å¼è§„åˆ™ (ad.txt)
        with open('rules/outputs/ad.txt', 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç‰ˆæœ¬ {version}\n")
            f.write(f"! æ›´æ–°æ—¶é—´ (åŒ—äº¬æ—¶é—´): {timestamp}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(self.all_white_domains):,} ä¸ª\n")
            f.write(f"! è§„åˆ™æº: {len(self.processed_urls)} ä¸ª\n")
            f.write(f"! é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n")
            f.write("!\n\n")
            
            # ç™½åå•è§„åˆ™
            if self.white_rules:
                f.write("! ====== ç™½åå•è§„åˆ™ ======\n")
                for rule in sorted(self.white_rules):
                    if rule.startswith('@@'):
                        f.write(f"{rule}\n")
                f.write("\n")
            
            # é»‘åå•åŸŸåè§„åˆ™
            f.write("! ====== åŸŸåé»‘åå• ======\n")
            for domain in sorted_black_domains:
                f.write(f"||{domain}^\n")
        
        # 2. DNSè¿‡æ»¤è§„åˆ™ (dns.txt)
        with open('rules/outputs/dns.txt', 'w', encoding='utf-8') as f:
            f.write(f"# DNSå¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç‰ˆæœ¬ {version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´ (åŒ—äº¬æ—¶é—´): {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"# é€‚ç”¨äº: AdGuard Home, Pi-hole, SmartDNS\n")
            f.write(f"# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n")
            f.write("#\n\n")
            
            for domain in sorted_black_domains:
                f.write(f"{domain}\n")
        
        # 3. Hostsæ ¼å¼è§„åˆ™ (hosts.txt)
        with open('rules/outputs/hosts.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Hostså¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç‰ˆæœ¬ {version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´ (åŒ—äº¬æ—¶é—´): {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"# é€‚ç”¨äº: ç³»ç»Ÿhostsæ–‡ä»¶\n")
            f.write(f"# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n")
            f.write("#\n\n")
            f.write("# æœ¬åœ°åŸŸå\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n")
            f.write("#\n")
            f.write("# å¹¿å‘ŠåŸŸå\n")
            
            # åˆ†æ‰¹å†™å…¥
            batch_size = 1000
            for i in range(0, len(sorted_black_domains), batch_size):
                batch = sorted_black_domains[i:i+batch_size]
                f.write(f"\n# åŸŸå {i+1}-{i+len(batch)}\n")
                for domain in batch:
                    f.write(f"0.0.0.0 {domain}\n")
        
        # 4. çº¯é»‘åå•è§„åˆ™ (black.txt)
        with open('rules/outputs/black.txt', 'w', encoding='utf-8') as f:
            f.write(f"# é»‘åå•è§„åˆ™ - ç‰ˆæœ¬ {version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´ (åŒ—äº¬æ—¶é—´): {timestamp}\n")
            f.write("#\n\n")
            for domain in sorted_black_domains:
                f.write(f"||{domain}^\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        with open('rules/outputs/white.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ç™½åå•è§„åˆ™ - ç‰ˆæœ¬ {version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´ (åŒ—äº¬æ—¶é—´): {timestamp}\n")
            f.write(f"# è§„åˆ™æ•°é‡: {len([r for r in self.white_rules if r.startswith('@@')]):,} æ¡\n")
            f.write("#\n\n")
            
            white_list = sorted([r for r in self.white_rules if r.startswith('@@')])
            for rule in white_list:
                f.write(f"{rule}\n")
        
        # 6. è§„åˆ™ä¿¡æ¯æ–‡ä»¶ (info.json)
        info = {
            'version': version,
            'updated_at': timestamp,
            'timezone': 'Asia/Shanghai (UTC+8)',
            'statistics': {
                'final_blacklist_domains': len(self.black_domains),
                'original_blacklist_domains': len(self.all_black_domains),
                'whitelist_domains': len(self.all_white_domains),
                'whitelist_rules': len([r for r in self.white_rules if r.startswith('@@')]),
                'sources_processed': len(self.processed_urls),
                'domains_removed_by_whitelist': len(self.all_black_domains) - len(self.black_domains)
            },
            'sources': {
                'blacklist': len(self.blacklist_sources),
                'whitelist': len(self.whitelist_sources)
            }
        }
        
        with open('rules/outputs/info.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print("ğŸ“„ è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
        print(f"   ad.txt - AdBlockæ ¼å¼ ({len(self.black_domains):,}ä¸ªåŸŸå)")
        print(f"   dns.txt - DNSæ ¼å¼ ({len(self.black_domains):,}ä¸ªåŸŸå)")
        print(f"   hosts.txt - Hostsæ ¼å¼ ({len(self.black_domains):,}ä¸ªåŸŸå)")
        print(f"   black.txt - é»‘åå•è§„åˆ™ ({len(self.black_domains):,}ä¸ªåŸŸå)")
        print(f"   white.txt - ç™½åå•è§„åˆ™ ({len([r for r in self.white_rules if r.startswith('@@')]):,}æ¡è§„åˆ™)")
        print(f"   info.json - è§„åˆ™ä¿¡æ¯")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.mdæ–‡ä»¶"""
        print("ğŸ“– ç”ŸæˆREADME.md...")
        
        # è¯»å–è§„åˆ™ä¿¡æ¯
        with open('rules/outputs/info.json', 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # ç”Ÿæˆè®¢é˜…é“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        # ç”ŸæˆREADMEå†…å®¹
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºAdGuardã€uBlock Originã€AdBlock Plusã€AdGuard Homeã€Pi-holeç­‰ã€‚

---

## è®¢é˜…åœ°å€

| è§„åˆ™ç±»å‹ | è§„åˆ™è¯´æ˜ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|:---------|:---------|:---------|:---------|
| **AdBlockè§„åˆ™** | é€‚ç”¨äºæµè§ˆå™¨å¹¿å‘Šæ’ä»¶ | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| **DNSè¿‡æ»¤è§„åˆ™** | é€‚ç”¨äºDNSè¿‡æ»¤è½¯ä»¶ | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| **Hostsè§„åˆ™** | é€‚ç”¨äºç³»ç»Ÿhostsæ–‡ä»¶ | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| **é»‘åå•è§„åˆ™** | çº¯é»‘åå•åŸŸå | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| **ç™½åå•è§„åˆ™** | æ’é™¤è¯¯æ‹¦åŸŸå | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['statistics']['final_blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['statistics']['whitelist_domains']:,} ä¸ª
- ç™½åå•ç§»é™¤ï¼š{info['statistics']['domains_removed_by_whitelist']:,} ä¸ªåŸŸå

---

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at']}** (åŒ—äº¬æ—¶é—´)

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print("ğŸ“„ README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½è§„åˆ™æº
            if not self.load_sources():
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è§„åˆ™æº")
                return False
            
            # 2. å¤„ç†æ‰€æœ‰è§„åˆ™æº
            self.process_all_sources()
            
            # 3. åº”ç”¨æ™ºèƒ½ç™½åå•é€»è¾‘
            self.apply_whitelist_logic()
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶
            self.generate_files()
            
            # 5. ç”ŸæˆREADME
            self.generate_readme()
            
            # ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            elapsed = end_time - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ è§„åˆ™ç”Ÿæˆå®Œæˆï¼")
            print(f"â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"ğŸ“Š æœ€ç»ˆé»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š æ”¶é›†åˆ°ç™½åå•åŸŸå: {len(self.all_white_domains):,}ä¸ª")
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print(f"ğŸ“– ä½¿ç”¨è¯´æ˜: README.md")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # è¿è¡Œç”Ÿæˆå™¨
    generator = AdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nâœ¨ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ”— æŸ¥çœ‹ README.md è·å–è®¢é˜…é“¾æ¥")
        print("ğŸ”„ å°†åœ¨ GitHub Actions è‡ªåŠ¨æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
