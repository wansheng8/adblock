#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ v3.3
ä¿®å¤ç‰ˆ - å®Œå…¨ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰æºæ–‡ä»¶ï¼Œæ— ä¾èµ–é—®é¢˜
"""

import os
import sys
import re
import json
import yaml
import time
import logging
import argparse
import hashlib
import threading
from datetime import datetime
from typing import Set, List, Optional, Tuple, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
from pathlib import Path

# æ£€æŸ¥å¹¶å¯¼å…¥ä¾èµ–
try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
    print("è¯·è¿è¡Œï¼špip install requests urllib3 pyyaml")
    sys.exit(1)

# ============================================
# é…ç½®ç®¡ç†å™¨
# ============================================
class Config:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path="config.yaml"):
        self.config_path = config_path
        self.data = self.load_config()
        self.validate_config()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f) or {}
            else:
                print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
                return self.get_default_config()
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self.get_default_config()
    
    def get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            'project': {
                'name': 'adblock-enhanced',
                'version': '3.3.0',
                'description': 'æ™ºèƒ½å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨',
                'author': 'wansheng8',
                'license': 'MIT'
            },
            'github': {
                'user': 'wansheng8',
                'repo': 'adblock-enhanced',
                'branch': 'main'
            },
            'performance': {
                'max_workers': 10,
                'timeout': 30,
                'retry_times': 3,
                'batch_size': 1000,
                'use_cache': False
            },
            'rules': {
                'backup_sources': {
                    'blacklist': [
                        "https://easylist.to/easylist/easylist.txt",
                        "https://easylist.to/easylist/easyprivacy.txt",
                        "https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts"
                    ],
                    'whitelist': [
                        "https://raw.githubusercontent.com/anudeepND/whitelist/master/domains/whitelist.txt"
                    ]
                },
                'exclude_domains': [
                    'localhost', 'local', 'broadcasthost',
                    '127.0.0.1', '0.0.0.0', '::1'
                ],
                'intelligent_filtering': {
                    'enable_essential_domain_whitelist': True,
                    'enable_safe_domains_check': True,
                    'enable_false_positive_filter': True,
                    'enable_domain_validation': True
                },
                'enhanced_blocking': {
                    'analytics': {'enabled': True},
                    'banner_ads': {'enabled': True},
                    'error_monitoring': {'enabled': True},
                    'element_hiding': {'enabled': True},
                    'script_blocking': {'enabled': True}
                }
            },
            'paths': {
                'sources_dir': 'rules/sources',
                'outputs_dir': 'rules/outputs',
                'logs_dir': 'logs',
                'reports_dir': 'reports',
                'backup_dir': 'backups'
            },
            'network': {
                'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'verify_ssl': True,
                'enable_backup_sources': True
            },
            'monitoring': {
                'log_level': 'INFO',
                'max_log_size_mb': 50,
                'log_retention_days': 30
            }
        }
    
    def validate_config(self):
        """éªŒè¯é…ç½®"""
        # ç¡®ä¿å¿…è¦çš„é…ç½®é¡¹å­˜åœ¨
        required = ['github', 'performance', 'paths']
        for section in required:
            if section not in self.data:
                self.data[section] = self.get_default_config()[section]
    
    def get(self, key, default=None):
        """è·å–é…ç½®å€¼"""
        keys = key.split('.')
        value = self.data
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k, default)
            else:
                return default
        return value if value is not None else default

# ============================================
# åŸŸåéªŒè¯å™¨
# ============================================
class DomainValidator:
    """åŸŸåéªŒè¯å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.exclude_domains = set(self.config.get('rules.exclude_domains', []))
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.domain_pattern = re.compile(
            r'^([a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$'
        )
        
        # å¸¸è§é¡¶çº§åŸŸå
        self.common_tlds = {
            'com', 'net', 'org', 'edu', 'gov', 'mil', 'int',
            'cn', 'uk', 'de', 'jp', 'fr', 'ru', 'br', 'in',
            'it', 'es', 'mx', 'kr', 'nl', 'ch', 'se', 'no',
            'dk', 'fi', 'pl', 'cz', 'hu', 'ro', 'gr', 'tr',
            'ar', 'cl', 'co', 'pe', 've', 'ec', 'bo', 'py',
            'uy', 'pa', 'cr', 'do', 'gt', 'sv', 'hn', 'ni',
            'pr', 'tt', 'jm', 'bs', 'bz', 'gy', 'sr', 'gf',
            'gp', 'mq', 'ht', 'cu', 'do', 'eu', 'asia', 'xxx',
            'xyz', 'online', 'site', 'top', 'win', 'vip', 'club',
            'shop', 'store', 'tech', 'website', 'space', 'digital',
            'news', 'blog', 'app', 'dev', 'io', 'ai', 'tv', 'me',
            'cc', 'us', 'ca', 'au', 'nz', 'sg', 'hk', 'tw', 'mo'
        }
    
    def validate_domain(self, domain):
        """éªŒè¯åŸŸåæœ‰æ•ˆæ€§"""
        domain = domain.strip().lower()
        
        # åŸºæœ¬æ£€æŸ¥
        if not domain:
            return False, "ç©ºåŸŸå"
        
        # é•¿åº¦æ£€æŸ¥
        min_len = self.config.get('rules.validation.min_domain_length', 3)
        max_len = self.config.get('rules.validation.max_domain_length', 253)
        
        if len(domain) < min_len:
            return False, f"åŸŸåå¤ªçŸ­ (min: {min_len})"
        if len(domain) > max_len:
            return False, f"åŸŸåå¤ªé•¿ (max: {max_len})"
        
        # æ£€æŸ¥æ’é™¤åˆ—è¡¨
        if domain in self.exclude_domains:
            return False, "æ’é™¤çš„åŸŸå"
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºIPåœ°å€
        if self._is_ip_address(domain):
            return False, "IPåœ°å€"
        
        # æ­£åˆ™è¡¨è¾¾å¼éªŒè¯
        if not self.domain_pattern.match(domain):
            return False, "æ ¼å¼æ— æ•ˆ"
        
        # æ£€æŸ¥TLD
        parts = domain.split('.')
        if len(parts) < 2:
            return False, "ç¼ºå°‘TLD"
        
        tld = parts[-1]
        min_tld_len = self.config.get('rules.validation.min_tld_length', 2)
        if len(tld) < min_tld_len:
            return False, f"TLDå¤ªçŸ­ (min: {min_tld_len})"
        
        # éªŒè¯TLDï¼ˆå¯é€‰ï¼‰
        if self.config.get('rules.validation.validate_tld', False):
            if not self._validate_tld(tld):
                return False, "æ— æ•ˆçš„TLD"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­çš„dot
        if '..' in domain:
            return False, "è¿ç»­çš„dot"
        
        # æ£€æŸ¥éƒ¨åˆ†æ˜¯å¦ä»¥è¿å­—ç¬¦å¼€å¤´æˆ–ç»“å°¾
        for part in parts:
            if part.startswith('-') or part.endswith('-'):
                return False, "éƒ¨åˆ†ä»¥è¿å­—ç¬¦å¼€å¤´æˆ–ç»“å°¾"
            if len(part) > 63:
                return False, "éƒ¨åˆ†å¤ªé•¿"
            
            # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
            if not self.config.get('rules.validation.allow_underscores', False):
                if '_' in part:
                    return False, "åŒ…å«ä¸‹åˆ’çº¿"
        
        return True, "æœ‰æ•ˆ"
    
    def _is_ip_address(self, domain):
        """æ£€æŸ¥æ˜¯å¦ä¸ºIPåœ°å€"""
        # IPv4
        ipv4_pattern = r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$'
        if re.match(ipv4_pattern, domain):
            parts = domain.split('.')
            try:
                return all(0 <= int(part) <= 255 for part in parts)
            except ValueError:
                return False
        
        # IPv6ç®€åŒ–æ£€æŸ¥
        if ':' in domain:
            return True
        
        return False
    
    def _validate_tld(self, tld):
        """éªŒè¯é¡¶çº§åŸŸå"""
        return tld in self.common_tlds
    
    def normalize_domain(self, domain):
        """æ ‡å‡†åŒ–åŸŸå"""
        domain = domain.strip().lower()
        
        # ç§»é™¤åè®®
        if '://' in domain:
            try:
                parsed = urlparse(domain)
                if parsed.netloc:
                    domain = parsed.netloc
                elif parsed.path:
                    domain = parsed.path
            except:
                pass
        
        # ç§»é™¤ç«¯å£
        if ':' in domain:
            domain = domain.split(':')[0]
        
        # ç§»é™¤wwwå‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]
        
        # ç§»é™¤æœ«å°¾çš„ç‚¹
        domain = domain.rstrip('.')
        
        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œè·¯å¾„
        if '/' in domain:
            domain = domain.split('/')[0]
        
        return domain

# ============================================
# ç½‘ç»œç®¡ç†å™¨
# ============================================
class NetworkManager:
    """ç½‘ç»œç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.session = self._create_session()
        self.cache = {}
        self.cache_lock = threading.Lock()
    
    def _create_session(self):
        """åˆ›å»ºHTTPä¼šè¯"""
        session = requests.Session()
        
        # é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=self.config.get('performance.retry_times', 3),
            backoff_factor=0.5,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=50, pool_maxsize=50)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # è®¾ç½®è¯·æ±‚å¤´
        session.headers.update({
            'User-Agent': self.config.get('network.user_agent', 'AdBlockGenerator/3.3'),
            'Accept': 'text/plain,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Encoding': self.config.get('network.accept_encoding', 'gzip, deflate'),
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive'
        })
        
        return session
    
    def fetch_url(self, url, timeout=30):
        """è·å–URLå†…å®¹"""
        try:
            response = self.session.get(
                url,
                timeout=timeout,
                verify=self.config.get('network.verify_ssl', True)
            )
            
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            if response.text and len(response.text) > 50:
                return response.text
            else:
                logging.warning(f"URLå†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º: {url}")
                return None
                
        except requests.RequestException as e:
            logging.warning(f"è·å–URLå¤±è´¥ {url}: {e}")
            return None
    
    def fetch_multiple_urls(self, urls, max_workers=10):
        """æ‰¹é‡è·å–URL"""
        results = {}
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_url = {
                executor.submit(self.fetch_url, url): url
                for url in urls
            }
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    content = future.result()
                    results[url] = content
                except Exception as e:
                    logging.error(f"æ‰¹é‡è·å–å¤±è´¥ {url}: {e}")
                    results[url] = None
        
        return results

# ============================================
# æ–‡ä»¶ç®¡ç†å™¨
# ============================================
class FileManager:
    """æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self._setup_directories()
    
    def _setup_directories(self):
        """è®¾ç½®ç›®å½•ç»“æ„"""
        directories = [
            self.config.get('paths.sources_dir'),
            self.config.get('paths.outputs_dir'),
            self.config.get('paths.logs_dir'),
            self.config.get('paths.reports_dir'),
            self.config.get('paths.backup_dir'),
        ]
        
        for directory in directories:
            if directory:
                os.makedirs(directory, exist_ok=True)
    
    def read_source_file(self, filename):
        """è¯»å–æºæ–‡ä»¶"""
        try:
            sources_dir = self.config.get('paths.sources_dir', 'rules/sources')
            filepath = os.path.join(sources_dir, filename)
            
            if not os.path.exists(filepath):
                logging.warning(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                return []
            
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = []
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        lines.append(line)
                return lines
                
        except Exception as e:
            logging.error(f"è¯»å–æºæ–‡ä»¶å¤±è´¥ {filename}: {e}")
            return []
    
    def save_file(self, filename, content, subdir='outputs'):
        """ä¿å­˜æ–‡ä»¶"""
        try:
            if subdir == 'outputs':
                base_dir = self.config.get('paths.outputs_dir', 'rules/outputs')
            elif subdir == 'reports':
                base_dir = self.config.get('paths.reports_dir', 'reports')
            else:
                base_dir = subdir
            
            filepath = os.path.join(base_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logging.info(f"ä¿å­˜æ–‡ä»¶: {filepath}")
            return True
            
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {filename}: {e}")
            return False
    
    def get_file_size(self, filename):
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            base_dir = self.config.get('paths.outputs_dir', 'rules/outputs')
            filepath = os.path.join(base_dir, filename)
            
            if os.path.exists(filepath):
                return os.path.getsize(filepath)
            else:
                return 0
        except:
            return 0

# ============================================
# è§„åˆ™å¤„ç†å™¨
# ============================================
class RuleProcessor:
    """è§„åˆ™å¤„ç†å™¨"""
    
    def __init__(self, config, validator):
        self.config = config
        self.validator = validator
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'valid_domains': 0,
            'invalid_domains': 0,
            'removed_by_whitelist': 0,
            'removed_by_safe_check': 0,
            'removed_by_suspicious': 0,
            'added_by_enhancement': 0,
            'whitelist_domains': 0,
            'element_hiding_rules': 0,
            'script_blocking_rules': 0
        }
        
        # å­˜å‚¨
        self.black_domains = set()
        self.white_domains = set()
        self.enhanced_domains = set()
        self.element_hiding_rules = set()
        self.script_blocking_rules = set()
        
        # åŠ è½½å†…ç½®è§„åˆ™
        self._load_builtin_rules()
    
    def _load_builtin_rules(self):
        """åŠ è½½å†…ç½®è§„åˆ™"""
        # å†…ç½®åˆ†æå·¥å…·åŸŸå
        self.analytics_domains = {
            'google-analytics.com', 'googletagmanager.com',
            'doubleclick.net', 'googlesyndication.com',
            'googleadservices.com', 'adservice.google.com',
            'facebook.com', 'fbcdn.net', 'twitter.com',
            'yandex.ru', 'yandex.net', 'mc.yandex.ru',
            'hotjar.com', 'mouseflow.com', 'crazyegg.com',
            'sentry.io', 'bugsnag.com', 'newrelic.com',
            'matomo.org', 'piwik.org', 'statcounter.com'
        }
        
        # å†…ç½®å¹¿å‘Šç½‘ç»œ
        self.ad_networks = {
            'adnxs.com', 'rubiconproject.com', 'criteo.com',
            'taboola.com', 'outbrain.com', 'revcontent.com',
            'amazon-adsystem.com', 'adsrvr.org', 'pubmatic.com',
            'openx.net', 'indexexchange.com', 'sonobi.com',
            'sharethrough.com', 'triplelift.com', 'mgid.com'
        }
        
        # å†…ç½®ç™½åå•åŸŸåï¼ˆé˜²æ­¢è¯¯æ‹¦æˆªï¼‰
        self.essential_domains = {
            'google.com', 'github.com', 'microsoft.com', 'apple.com',
            'amazon.com', 'cloudflare.com', 'baidu.com', 'tencent.com',
            'alibaba.com', 'stackoverflow.com', 'wikipedia.org',
            'gitlab.com', 'docker.com', 'npmjs.com', 'pypi.org',
            'ubuntu.com', 'debian.org', 'apache.org', 'mozilla.org'
        }
        
        # å†…ç½®å…ƒç´ éšè—è§„åˆ™
        self.builtin_element_hiding_rules = [
            '##div[class*="ad-"]',
            '##div[id*="ad-"]',
            '##div[class*="banner"]',
            '##div[id*="banner"]',
            '##div[class*="advert"]',
            '##div[id*="advert"]',
            '##div[class*="sponsor"]',
            '##div[id*="sponsor"]',
            '##div[class*="promo"]',
            '##div[id*="promo"]',
            '##iframe[src*="ad"]',
            '##iframe[src*="banner"]',
            '##img[src*="ad"]',
            '##img[alt*="å¹¿å‘Š"]',
            '##.ad-banner',
            '##.adsbygoogle',
            '##.ad-unit',
            '##.ad-container',
            '##.ad-wrapper'
        ]
        
        # å†…ç½®è„šæœ¬æ‹¦æˆªè§„åˆ™
        self.builtin_script_blocking_rules = [
            r'analytics\.js',
            r'ga\.js',
            r'gtm\.js',
            r'stats\.js',
            r'track\.js',
            r'beacon\.js',
            r'pixel\.js'
        ]
    
    def extract_domain_from_line(self, line):
        """ä»è§„åˆ™è¡Œä¸­æå–åŸŸå"""
        line = line.strip()
        
        # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
        if not line or line.startswith(('#', '!', '//')):
            return None, False
        
        is_whitelist = line.startswith('@@')
        if is_whitelist:
            line = line[2:]
        
        domain = None
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„è§„åˆ™
        patterns = [
            # ||domain.com^ æ ¼å¼
            (r'^\|\|([^\^]+)\^', 1),
            # domain.com^ æ ¼å¼
            (r'^([^\^]+)\^', 1),
            # 0.0.0.0 domain.com æ ¼å¼
            (r'^0\.0\.0\.0\s+([^\s]+)', 1),
            # 127.0.0.1 domain.com æ ¼å¼
            (r'^127\.0\.0\.1\s+([^\s]+)', 1),
            # :: domain.com æ ¼å¼ (IPv6)
            (r'^::\s+([^\s]+)', 1),
            # çº¯åŸŸåæ ¼å¼
            (r'^([a-zA-Z0-9.-]+)$', 1),
            # å¸¦æœ‰é€šé…ç¬¦çš„æ ¼å¼
            (r'^\|\|([^*\^]+)\^', 1),
            # ç‰¹æ®Šæ ¼å¼
            (r'^\|\|([^\^]+)\^\$?', 1),
        ]
        
        for pattern, group in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(group)
                break
        
        if domain:
            # æ ‡å‡†åŒ–åŸŸå
            domain = self.validator.normalize_domain(domain)
            
            # éªŒè¯åŸŸå
            is_valid, _ = self.validator.validate_domain(domain)
            if is_valid:
                return domain, is_whitelist
        
        return None, False
    
    def process_content(self, content, source_type='black'):
        """å¤„ç†è§„åˆ™å†…å®¹"""
        domains = set()
        lines = content.split('\n')
        
        for line in lines:
            domain, is_whitelist = self.extract_domain_from_line(line)
            if domain:
                self.stats['total_processed'] += 1
                if is_whitelist:
                    self.white_domains.add(domain)
                    self.stats['whitelist_domains'] += 1
                else:
                    domains.add(domain)
                    self.stats['valid_domains'] += 1
            else:
                self.stats['invalid_domains'] += 1
        
        return domains
    
    def apply_intelligent_filtering(self, domains, mode='normal'):
        """åº”ç”¨æ™ºèƒ½è¿‡æ»¤"""
        filtered = set(domains)
        
        # 1. åº”ç”¨ç™½åå•
        filtered = self._apply_whitelist(filtered)
        
        # 2. åº”ç”¨å¿…è¦åŸŸåç™½åå•
        if self.config.get('rules.intelligent_filtering.enable_essential_domain_whitelist', True):
            filtered = self._apply_essential_whitelist(filtered)
        
        # 3. å®‰å…¨åŸŸåæ£€æŸ¥
        if self.config.get('rules.intelligent_filtering.enable_safe_domains_check', True):
            filtered = self._filter_safe_domains(filtered)
        
        # 4. è¯¯æŠ¥è¿‡æ»¤
        if self.config.get('rules.intelligent_filtering.enable_false_positive_filter', True):
            filtered = self._filter_false_positives(filtered)
        
        # 5. åŸŸåéªŒè¯
        if self.config.get('rules.intelligent_filtering.enable_domain_validation', True):
            filtered = self._validate_domains(filtered)
        
        # 6. å¢å¼ºæ‹¦æˆª
        if mode == 'enhanced':
            filtered = self._enhance_blocking(filtered)
        
        return filtered
    
    def _apply_whitelist(self, domains):
        """åº”ç”¨ç™½åå•"""
        filtered = set()
        removed = 0
        
        for domain in domains:
            is_whitelisted = False
            
            # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
            if domain in self.white_domains:
                is_whitelisted = True
            else:
                # æ£€æŸ¥å­åŸŸååŒ¹é…
                for white_domain in self.white_domains:
                    if domain.endswith(f'.{white_domain}'):
                        is_whitelisted = True
                        break
            
            if not is_whitelisted:
                filtered.add(domain)
            else:
                removed += 1
        
        self.stats['removed_by_whitelist'] = removed
        return filtered
    
    def _apply_essential_whitelist(self, domains):
        """åº”ç”¨å¿…è¦åŸŸåç™½åå•"""
        filtered = set()
        removed = 0
        
        for domain in domains:
            is_essential = False
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¿…è¦åŸŸå
            for essential_domain in self.essential_domains:
                if domain == essential_domain or domain.endswith(f'.{essential_domain}'):
                    is_essential = True
                    break
            
            if not is_essential:
                filtered.add(domain)
            else:
                removed += 1
                # æ·»åŠ åˆ°ç™½åå•
                self.white_domains.add(domain)
        
        self.stats['removed_by_safe_check'] = removed
        return filtered
    
    def _filter_safe_domains(self, domains):
        """è¿‡æ»¤å®‰å…¨åŸŸå"""
        filtered = set()
        removed = 0
        
        # å®‰å…¨åŸŸåæ¨¡å¼
        safe_patterns = [
            r'^[a-z]{1,2}\.(com|net|org)$',  # è¶…çŸ­åŸŸå
            r'^[a-z0-9-]+\.(gov|edu|mil|int)$',  # æ”¿åºœ/æ•™è‚²
            r'^localhost(\.[a-z]+)?$',  # localhostç›¸å…³
        ]
        
        for domain in domains:
            is_safe = False
            
            for pattern in safe_patterns:
                if re.match(pattern, domain):
                    is_safe = True
                    break
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ¥åå¼€æºé¡¹ç›®
            open_source_domains = {
                'apache.org', 'mozilla.org', 'gnu.org', 'kernel.org',
                'python.org', 'nodejs.org', 'golang.org', 'rust-lang.org'
            }
            
            if any(domain == d or domain.endswith(f'.{d}') for d in open_source_domains):
                is_safe = True
            
            if not is_safe:
                filtered.add(domain)
            else:
                removed += 1
        
        self.stats['removed_by_safe_check'] += removed
        return filtered
    
    def _filter_false_positives(self, domains):
        """è¿‡æ»¤è¯¯æŠ¥"""
        filtered = set()
        removed = 0
        
        # å¯ç–‘åŸŸåæ¨¡å¼
        suspicious_patterns = [
            r'^[a-z]{1,2}\d+[a-z]+\.[a-z]+$',  # çŸ­åŸŸåå¸¦æ•°å­—
            r'^[a-z0-9]+-[a-z0-9]+-[a-z0-9]+\.[a-z]+$',  # å¤šä¸ªè¿å­—ç¬¦
            r'^\d+[a-z]+\.[a-z]+$',  # ä»¥æ•°å­—å¼€å¤´
        ]
        
        for domain in domains:
            is_suspicious = False
            
            for pattern in suspicious_patterns:
                if re.match(pattern, domain):
                    is_suspicious = True
                    break
            
            # æ£€æŸ¥åŸŸåé•¿åº¦
            if len(domain) < 5:  # éå¸¸çŸ­çš„åŸŸå
                is_suspicious = True
            
            # æ£€æŸ¥å¥‡æ€ªçš„TLDç»„åˆ
            parts = domain.split('.')
            if len(parts) >= 2:
                tld = parts[-1]
                if len(tld) > 6:  # éå¸¸é•¿çš„TLD
                    is_suspicious = True
            
            if not is_suspicious:
                filtered.add(domain)
            else:
                removed += 1
        
        self.stats['removed_by_suspicious'] = removed
        return filtered
    
    def _validate_domains(self, domains):
        """éªŒè¯åŸŸå"""
        filtered = set()
        
        for domain in domains:
            is_valid, _ = self.validator.validate_domain(domain)
            if is_valid:
                filtered.add(domain)
        
        return filtered
    
    def _enhance_blocking(self, domains):
        """å¢å¼ºæ‹¦æˆª"""
        enhanced = set(domains)
        added = 0
        
        # æ·»åŠ åˆ†æå·¥å…·åŸŸå
        if self.config.get('rules.enhanced_blocking.analytics.enabled', True):
            for domain in self.analytics_domains:
                if domain not in enhanced:
                    is_valid, _ = self.validator.validate_domain(domain)
                    if is_valid:
                        enhanced.add(domain)
                        added += 1
        
        # æ·»åŠ å¹¿å‘Šç½‘ç»œåŸŸå
        if self.config.get('rules.enhanced_blocking.banner_ads.enabled', True):
            for domain in self.ad_networks:
                if domain not in enhanced:
                    is_valid, _ = self.validator.validate_domain(domain)
                    if is_valid:
                        enhanced.add(domain)
                        added += 1
        
        # ç”Ÿæˆå…ƒç´ éšè—è§„åˆ™
        if self.config.get('rules.enhanced_blocking.element_hiding.enabled', True):
            self.element_hiding_rules.update(self.builtin_element_hiding_rules)
            self.stats['element_hiding_rules'] = len(self.element_hiding_rules)
        
        # ç”Ÿæˆè„šæœ¬æ‹¦æˆªè§„åˆ™
        if self.config.get('rules.enhanced_blocking.script_blocking.enabled', True):
            for pattern in self.builtin_script_blocking_rules:
                rule = f"||*{pattern}$script,important"
                self.script_blocking_rules.add(rule)
            self.stats['script_blocking_rules'] = len(self.script_blocking_rules)
        
        self.stats['added_by_enhancement'] = added
        return enhanced

# ============================================
# ä¸»ç”Ÿæˆå™¨
# ============================================
class AdBlockGenerator:
    """å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ä¸»ç±»"""
    
    def __init__(self, config_path="config.yaml"):
        # åˆå§‹åŒ–ç»„ä»¶
        self.config = Config(config_path)
        self.validator = DomainValidator(self.config)
        self.network = NetworkManager(self.config)
        self.processor = RuleProcessor(self.config, self.validator)
        self.files = FileManager(self.config)
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
        # çŠ¶æ€
        self.black_sources = []
        self.white_sources = []
        
        # ç‰ˆæœ¬ä¿¡æ¯
        self.version = self.config.get('project.version', '3.3.0')
        self.build_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = self.config.get('monitoring.log_level', 'INFO').upper()
        log_file = self.config.get('paths.error_log', 'logs/error.log')
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = os.path.dirname(log_file)
        if log_dir:
            os.makedirs(log_dir, exist_ok=True)
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=getattr(logging, log_level),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def load_sources_from_files(self):
        """ä»æ–‡ä»¶åŠ è½½è§„åˆ™æº"""
        print("ğŸ“‹ ä»æ–‡ä»¶åŠ è½½è§„åˆ™æº...")
        
        # è¯»å–ç”¨æˆ·è‡ªå®šä¹‰æºæ–‡ä»¶
        black_sources = self.files.read_source_file('black.txt')
        white_sources = self.files.read_source_file('white.txt')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æº
        if not black_sources:
            print("âš ï¸  é»‘åå•æºæ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
        else:
            print(f"ğŸ“„ ä» black.txt è¯»å–äº† {len(black_sources)} ä¸ªæº")
        
        if not white_sources:
            print("âš ï¸  ç™½åå•æºæ–‡ä»¶ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
        else:
            print(f"ğŸ“„ ä» white.txt è¯»å–äº† {len(white_sources)} ä¸ªæº")
        
        # å¦‚æœéœ€è¦ï¼Œæ·»åŠ å¤‡ç”¨æº
        if not black_sources and self.config.get('network.enable_backup_sources', True):
            print("ğŸ“¦ ä½¿ç”¨å†…ç½®å¤‡ç”¨é»‘åå•æº")
            black_sources = self.config.get('rules.backup_sources.blacklist', [])
        
        if not white_sources and self.config.get('network.enable_backup_sources', True):
            print("ğŸ“¦ ä½¿ç”¨å†…ç½®å¤‡ç”¨ç™½åå•æº")
            white_sources = self.config.get('rules.backup_sources.whitelist', [])
        
        self.black_sources = black_sources
        self.white_sources = white_sources
        
        print(f"âœ… æ€»å…±åŠ è½½äº† {len(self.black_sources)} ä¸ªé»‘åå•æºå’Œ {len(self.white_sources)} ä¸ªç™½åå•æº")
        return True
    
    def download_sources(self):
        """ä¸‹è½½è§„åˆ™æº"""
        print("ğŸ“¥ ä¸‹è½½è§„åˆ™æº...")
        
        # åˆå¹¶æ‰€æœ‰URL
        all_urls = list(set(self.black_sources + self.white_sources))
        
        if not all_urls:
            print("âŒ æ²¡æœ‰å¯ä¸‹è½½çš„æº")
            return [], []
        
        print(f"ğŸŒ å¼€å§‹ä¸‹è½½ {len(all_urls)} ä¸ªæº...")
        
        results = self.network.fetch_multiple_urls(
            all_urls,
            max_workers=self.config.get('performance.max_workers', 10)
        )
        
        # åˆ†ç¦»ç»“æœ
        black_content = []
        white_content = []
        
        successful_black = 0
        successful_white = 0
        
        for url in self.black_sources:
            if url in results and results[url]:
                black_content.append((url, results[url]))
                successful_black += 1
                print(f"  âœ… {url}")
            else:
                print(f"  âŒ {url}")
        
        for url in self.white_sources:
            if url in results and results[url]:
                white_content.append((url, results[url]))
                successful_white += 1
                print(f"  âœ… {url}")
            else:
                print(f"  âŒ {url}")
        
        print(f"ğŸ“Š ä¸‹è½½å®Œæˆ: {successful_black}/{len(self.black_sources)} é»‘åå•æºæˆåŠŸ, {successful_white}/{len(self.white_sources)} ç™½åå•æºæˆåŠŸ")
        
        # å¦‚æœæ²¡æœ‰æˆåŠŸçš„æºï¼Œä½¿ç”¨å†…ç½®è§„åˆ™
        if successful_black == 0:
            print("âš ï¸  æ‰€æœ‰é»‘åå•æºéƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨å†…ç½®è§„åˆ™")
            black_content = [("å†…ç½®è§„åˆ™", self._get_builtin_rules())]
        
        return black_content, white_content
    
    def _get_builtin_rules(self):
        """è·å–å†…ç½®è§„åˆ™"""
        return """
# å†…ç½®å¹¿å‘Šè¿‡æ»¤è§„åˆ™
||doubleclick.net^
||googlesyndication.com^
||googleadservices.com^
||adservice.google.com^
||facebook.com^$third-party
||twitter.com^$third-party
||analytics.google.com^
||stats.g.doubleclick.net^
||adnxs.com^
||rubiconproject.com^
||criteo.com^
||taboola.com^
||outbrain.com^
||revcontent.com^
||amazon-adsystem.com^
||adsrvr.org^
||pubmatic.com^
||openx.net^
||indexexchange.com^
||sonobi.com^
||sharethrough.com^
||triplelift.com^
||mgid.com^
||zemanta.com^
||content.ad^
||adblade.com^
||adbrite.com^
||adform.com^
||adition.com^
||casalemedia.com^
||contextweb.com^
||conversantmedia.com^
||districtm.io^
||eyereturn.com^
||getclicky.com^
||imrworldwide.com^
||infolinks.com^
||innovid.com^
||ipinyou.com^
||kargo.com^
||kiosked.com^
||lijit.com^
||linksynergy.com^
||media.net^
||mediamath.com^
||meetrics.net^
||mopub.com^
||pulpix.com^
||quantserve.com^
||sovrn.com^
||spotxchange.com^
||teads.tv^
||telaria.com^
||tremorhub.com^
||truex.com^
||undertone.com^
||unruly.co^
||videologygroup.com^
||yieldmo.com^
||yieldone.com^
||yldmgrimg.net^
"""
    
    def process_sources(self, black_content, white_content, mode='normal'):
        """å¤„ç†è§„åˆ™æº"""
        print("ğŸ”§ å¤„ç†è§„åˆ™æº...")
        
        # å¤„ç†é»‘åå•
        all_black_domains = set()
        for url, content in black_content:
            domains = self.processor.process_content(content, 'black')
            all_black_domains.update(domains)
        
        # å¤„ç†ç™½åå•
        for url, content in white_content:
            self.processor.process_content(content, 'white')
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(all_black_domains)} é»‘åå•åŸŸå, {len(self.processor.white_domains)} ç™½åå•åŸŸå")
        
        # åº”ç”¨è¿‡æ»¤
        filtered_domains = self.processor.apply_intelligent_filtering(all_black_domains, mode)
        
        # æ›´æ–°ç»Ÿè®¡
        self.processor.black_domains = all_black_domains
        self.processor.enhanced_domains = filtered_domains
        
        print(f"âœ… å¤„ç†å®Œæˆ: {len(filtered_domains)} ä¸ªè¿‡æ»¤ååŸŸå")
        return filtered_domains
    
    def generate_files(self, domains, mode='normal'):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ç”Ÿæˆå„ç§æ ¼å¼çš„æ–‡ä»¶
        files_to_generate = [
            ('ad.txt', self._generate_adblock_rules(domains, timestamp, mode)),
            ('dns.txt', self._generate_dns_rules(domains, timestamp)),
            ('hosts.txt', self._generate_hosts_rules(domains, timestamp)),
            ('black.txt', self._generate_black_rules(domains, timestamp)),
            ('white.txt', self._generate_white_rules(timestamp)),
            ('enhanced.txt', self._generate_enhanced_rules(domains, timestamp, mode)),
            ('info.json', self._generate_info_file(domains, timestamp, mode)),
        ]
        
        success = True
        for filename, content in files_to_generate:
            if not self.files.save_file(filename, content):
                success = False
            else:
                print(f"  âœ… ç”Ÿæˆ {filename}")
        
        if success:
            print("âœ… æ‰€æœ‰è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
        
        return success
    
    def _generate_adblock_rules(self, domains, timestamp, mode):
        """ç”ŸæˆAdblockè§„åˆ™"""
        lines = [
            f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{self.version}",
            f"! ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"! æ¨¡å¼: {mode}",
            f"! åŸŸåæ•°é‡: {len(domains)}",
            f"! ç™½åå•åŸŸå: {len(self.processor.white_domains)}",
            f"! é¡¹ç›®åœ°å€: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}",
            "!",
            "! ========== ç™½åå•è§„åˆ™ =========="
        ]
        
        # ç™½åå•è§„åˆ™
        for domain in sorted(self.processor.white_domains):
            lines.append(f"@@||{domain}^")
        
        # å…ƒç´ éšè—è§„åˆ™
        if self.processor.element_hiding_rules:
            lines.extend([
                "!",
                "! ========== å…ƒç´ éšè—è§„åˆ™ =========="
            ])
            for rule in sorted(self.processor.element_hiding_rules):
                lines.append(rule)
        
        # è„šæœ¬æ‹¦æˆªè§„åˆ™
        if self.processor.script_blocking_rules:
            lines.extend([
                "!",
                "! ========== è„šæœ¬æ‹¦æˆªè§„åˆ™ =========="
            ])
            for rule in sorted(self.processor.script_blocking_rules):
                lines.append(rule)
        
        lines.extend([
            "!",
            "! ========== é»‘åå•è§„åˆ™ =========="
        ])
        
        # é»‘åå•è§„åˆ™
        for domain in sorted(domains):
            lines.append(f"||{domain}^")
        
        return '\n'.join(lines)
    
    def _generate_dns_rules(self, domains, timestamp):
        """ç”ŸæˆDNSè§„åˆ™"""
        lines = [
            f"# DNSè¿‡æ»¤è§„åˆ™ v{self.version}",
            f"# ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"# åŸŸåæ•°é‡: {len(domains)}",
            f"# é¡¹ç›®åœ°å€: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}",
            "#"
        ]
        
        for domain in sorted(domains):
            lines.append(domain)
        
        return '\n'.join(lines)
    
    def _generate_hosts_rules(self, domains, timestamp):
        """ç”ŸæˆHostsè§„åˆ™"""
        lines = [
            f"# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{self.version}",
            f"# ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"# åŸŸåæ•°é‡: {len(domains)}",
            f"# é¡¹ç›®åœ°å€: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}",
            "#",
            "127.0.0.1 localhost",
            "::1 localhost",
            "# å¹¿å‘ŠåŸŸåå±è”½",
            ""
        ]
        
        for domain in sorted(domains):
            lines.append(f"0.0.0.0 {domain}")
        
        return '\n'.join(lines)
    
    def _generate_black_rules(self, domains, timestamp):
        """ç”Ÿæˆé»‘åå•è§„åˆ™"""
        lines = [
            f"! é»‘åå•è§„åˆ™ v{self.version}",
            f"! ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"! åŸŸåæ•°é‡: {len(domains)}",
            f"! é¡¹ç›®åœ°å€: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}",
            "!"
        ]
        
        for domain in sorted(domains):
            lines.append(f"||{domain}^")
        
        return '\n'.join(lines)
    
    def _generate_white_rules(self, timestamp):
        """ç”Ÿæˆç™½åå•è§„åˆ™"""
        lines = [
            f"! ç™½åå•è§„åˆ™ v{self.version}",
            f"! ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"! åŸŸåæ•°é‡: {len(self.processor.white_domains)}",
            f"! é¡¹ç›®åœ°å€: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}",
            "!"
        ]
        
        for domain in sorted(self.processor.white_domains):
            lines.append(f"@@||{domain}^")
        
        return '\n'.join(lines)
    
    def _generate_enhanced_rules(self, domains, timestamp, mode):
        """ç”Ÿæˆå¢å¼ºè§„åˆ™"""
        lines = [
            f"! å¢å¼ºå¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{self.version}",
            f"! ç”Ÿæˆæ—¶é—´: {timestamp}",
            f"! æ¨¡å¼: {mode}",
            f"! å¢å¼ºæ‹¦æˆªåŸŸå: {len(domains)}",
            f"! é¡¹ç›®åœ°å€: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}",
            "!",
            "! ========== å¢å¼ºæ‹¦æˆªè§„åˆ™ =========="
        ]
        
        # ç»Ÿè®¡å¢å¼ºæ‹¦æˆªçš„åŸŸå
        enhanced_count = 0
        for domain in sorted(domains):
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¢å¼ºæ‹¦æˆªçš„åŸŸå
            if (domain in self.processor.analytics_domains or 
                domain in self.processor.ad_networks):
                lines.append(f"||{domain}^$third-party,important")
                enhanced_count += 1
        
        # æ·»åŠ å¢å¼ºæ‹¦æˆªç»Ÿè®¡
        lines.insert(3, f"! å¢å¼ºæ‹¦æˆªåŸŸå: {enhanced_count}")
        
        return '\n'.join(lines)
    
    def _generate_info_file(self, domains, timestamp, mode):
        """ç”Ÿæˆä¿¡æ¯æ–‡ä»¶"""
        info = {
            'version': self.version,
            'build_date': self.build_date,
            'timestamp': timestamp,
            'mode': mode,
            'stats': self.processor.stats,
            'counts': {
                'blacklist': len(self.processor.black_domains),
                'whitelist': len(self.processor.white_domains),
                'filtered': len(domains),
                'enhanced_added': self.processor.stats['added_by_enhancement']
            },
            'github': {
                'user': self.config.get('github.user'),
                'repo': self.config.get('github.repo'),
                'branch': self.config.get('github.branch')
            },
            'sources': {
                'blacklist_count': len(self.black_sources),
                'whitelist_count': len(self.white_sources),
                'blacklist_sources': self.black_sources,
                'whitelist_sources': self.white_sources
            }
        }
        
        return json.dumps(info, indent=2, ensure_ascii=False)
    
    def generate_reports(self, domains, mode):
        """ç”ŸæˆæŠ¥å‘Š"""
        if not self.config.get('reports.generate_detailed_report', True):
            return
        
        print("ğŸ“Š ç”ŸæˆæŠ¥å‘Š...")
        
        # è¯¦ç»†æŠ¥å‘Š
        detailed_report = {
            'generated_at': datetime.now().isoformat(),
            'version': self.version,
            'mode': mode,
            'statistics': self.processor.stats,
            'domain_counts': {
                'total_blacklist': len(self.processor.black_domains),
                'total_whitelist': len(self.processor.white_domains),
                'filtered': len(domains)
            },
            'sources': {
                'blacklist': self.black_sources,
                'whitelist': self.white_sources
            },
            'top_domains': list(sorted(domains))[:50] if domains else []
        }
        
        self.files.save_file('detailed_report.json', json.dumps(detailed_report, indent=2, ensure_ascii=False), 'reports')
        print("  âœ… ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š")
    
    def generate_readme(self, domains):
        """ç”ŸæˆREADME.md"""
        base_url = f"https://raw.githubusercontent.com/{self.config.get('github.user')}/{self.config.get('github.repo')}/{self.config.get('github.branch')}"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{self.config.get('github.user')}/{self.config.get('github.repo')}@{self.config.get('github.branch')}"
        
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{self.version}

ä¸€ä¸ªç²¾å‡†çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œè‡ªåŠ¨æ›´æ–°ç»´æŠ¤ï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨ã€DNSè¿‡æ»¤å™¨å’ŒHostsæ–‡ä»¶ã€‚

## ğŸ“Š ç»Ÿè®¡æ•°æ®

- **é»‘åå•åŸŸå**: {len(self.processor.black_domains):,}
- **ç™½åå•åŸŸå**: {len(self.processor.white_domains):,}
- **è¿‡æ»¤ååŸŸå**: {len(domains):,}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ç‰ˆæœ¬**: {self.version}

## ğŸ“¥ è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/rules/outputs/ad.txt` | `{cdn_url}/rules/outputs/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/rules/outputs/dns.txt` | `{cdn_url}/rules/outputs/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/rules/outputs/hosts.txt` | `{cdn_url}/rules/outputs/hosts.txt` |
| å¢å¼ºè¿‡æ»¤è§„åˆ™ | Enhanced | `{base_url}/rules/outputs/enhanced.txt` | `{cdn_url}/rules/outputs/enhanced.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/rules/outputs/black.txt` | `{cdn_url}/rules/outputs/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/rules/outputs/white.txt` | `{cdn_url}/rules/outputs/white.txt` |

## ğŸ”§ ä½¿ç”¨è¯´æ˜

### Adblock/uBlock Origin
1. æ‰“å¼€æ‰©å±•è®¾ç½®
2. æ‰¾åˆ°"è‡ªå®šä¹‰è§„åˆ™"æˆ–"æˆ‘çš„è§„åˆ™"é€‰é¡¹
3. æ·»åŠ è®¢é˜…é“¾æ¥ï¼š`{base_url}/rules/outputs/ad.txt`

### DNSè¿‡æ»¤
1. å°†ä»¥ä¸‹é“¾æ¥æ·»åŠ åˆ°DNSè¿‡æ»¤è½¯ä»¶ï¼š
   - `{base_url}/rules/outputs/dns.txt`

### Hostsæ–‡ä»¶
1. ä¸‹è½½Hostsæ–‡ä»¶ï¼š
   - `{base_url}/rules/outputs/hosts.txt`
2. å°†å†…å®¹æ·»åŠ åˆ°ç³»ç»Ÿhostsæ–‡ä»¶

## ğŸš€ æ›´æ–°é¢‘ç‡

è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00

## ğŸ“ é¡¹ç›®ä¿¡æ¯

- **é¡¹ç›®åœ°å€**: https://github.com/{self.config.get('github.user')}/{self.config.get('github.repo')}
- **è®¸å¯è¯**: MIT License
- **ä½œè€…**: {self.config.get('project.author')}

---

*æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print("âœ… README.md ç”Ÿæˆå®Œæˆ")
    
    def run(self, mode='normal'):
        """è¿è¡Œç”Ÿæˆå™¨"""
        print("=" * 60)
        print(f"ğŸ¯ å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ v{self.version}")
        print(f"ğŸ“± æ¨¡å¼: {mode}")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. ä»æ–‡ä»¶åŠ è½½è§„åˆ™æº
            print("\næ­¥éª¤ 1/5: ä»æ–‡ä»¶åŠ è½½è§„åˆ™æº")
            if not self.load_sources_from_files():
                print("âŒ åŠ è½½è§„åˆ™æºå¤±è´¥")
                return False
            
            # 2. ä¸‹è½½è§„åˆ™æº
            print(f"\næ­¥éª¤ 2/5: ä¸‹è½½è§„åˆ™æº")
            black_content, white_content = self.download_sources()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹
            if not black_content:
                print("âŒ æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•é»‘åå•è§„åˆ™")
                return False
            
            # 3. å¤„ç†è§„åˆ™
            print(f"\næ­¥éª¤ 3/5: å¤„ç†è§„åˆ™")
            domains = self.process_sources(black_content, white_content, mode)
            
            if not domains:
                print("âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•åŸŸåè§„åˆ™")
            
            # 4. ç”Ÿæˆæ–‡ä»¶
            print(f"\næ­¥éª¤ 4/5: ç”Ÿæˆè§„åˆ™æ–‡ä»¶")
            if not self.generate_files(domains, mode):
                print("âŒ ç”Ÿæˆè§„åˆ™æ–‡ä»¶å¤±è´¥")
                return False
            
            # 5. ç”ŸæˆæŠ¥å‘Šå’ŒREADME
            print(f"\næ­¥éª¤ 5/5: ç”ŸæˆæŠ¥å‘Šå’ŒREADME")
            self.generate_reports(domains, mode)
            self.generate_readme(domains)
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print("=" * 60)
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.processor.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.processor.white_domains):,}ä¸ª")
            print(f"ğŸ“Š è¿‡æ»¤ååŸŸå: {len(domains):,}ä¸ª")
            
            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            for filename in ['ad.txt', 'dns.txt', 'hosts.txt', 'black.txt', 'white.txt', 'enhanced.txt']:
                size = self.files.get_file_size(filename)
                if size > 0:
                    size_mb = size / 1024 / 1024
                    print(f"  â€¢ {filename}: {size_mb:.2f} MB")
            
            print("=" * 60)
            
            return True
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            return False
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

# ============================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================
def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description=f'å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ v3.3',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--mode', '-m',
        choices=['normal', 'strict', 'loose', 'enhanced'],
        default='normal',
        help='è¿è¡Œæ¨¡å¼: normal(é»˜è®¤), strict(ä¸¥æ ¼), loose(å®½æ¾), enhanced(å¢å¼º)'
    )
    
    parser.add_argument(
        '--config', '-c',
        default='config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='è¯¦ç»†è¾“å‡º'
    )
    
    parser.add_argument(
        '--test', '-t',
        action='store_true',
        help='æµ‹è¯•æ¨¡å¼'
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ£€æŸ¥ä¾èµ–
    if not REQUESTS_AVAILABLE:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests urllib3 pyyaml")
        return 1
    
    # è¿è¡Œç”Ÿæˆå™¨
    generator = AdBlockGenerator(args.config)
    
    if args.test:
        # æµ‹è¯•æ¨¡å¼
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼è¿è¡Œä¸­...")
        success = generator.run('normal')
    else:
        # æ­£å¸¸æ¨¡å¼
        success = generator.run(args.mode)
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
        return 0
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")
        return 1

if __name__ == "__main__":
    sys.exit(main())
