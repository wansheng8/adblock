#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨
é’ˆå¯¹å¤„ç†æ…¢çš„é—®é¢˜è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–
"""

import os
import re
import json
import time
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, List, Optional, Tuple
import requests
from urllib.parse import urlparse
import hashlib

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # âš¡ æ€§èƒ½ä¼˜åŒ–è®¾ç½®
    'MAX_WORKERS': 20,           # å¢åŠ çº¿ç¨‹æ•°
    'TIMEOUT': 15,              # å‡å°‘è¶…æ—¶æ—¶é—´
    'RETRY_TIMES': 2,           # å‡å°‘é‡è¯•æ¬¡æ•°
    'BATCH_SIZE': 10000,        # æ‰¹é‡å¤„ç†å¤§å°
    'CACHE_ENABLED': True,      # å¯ç”¨ç®€å•ç¼“å­˜
    'SKIP_URL_VALIDATION': True, # è·³è¿‡URLéªŒè¯ï¼ˆåŠ é€Ÿï¼‰
    
    # æ–‡ä»¶è·¯å¾„ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # è¾“å‡ºæ–‡ä»¶ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰
    'AD_FILE': 'rules/outputs/ad.txt',
    'DNS_FILE': 'rules/outputs/dns.txt',
    'HOSTS_FILE': 'rules/outputs/hosts.txt',
    'BLACK_FILE': 'rules/outputs/black.txt',
    'WHITE_FILE': 'rules/outputs/white.txt',
    'INFO_FILE': 'rules/outputs/info.json',
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.WARNING,  # å‡å°‘æ—¥å¿—è¾“å‡º
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FastAdBlockGenerator:
    """å¿«é€Ÿå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'load_time': 0,
            'download_time': 0,
            'parse_time': 0,
            'process_time': 0,
            'write_time': 0,
            'total_time': 0,
            'urls_processed': 0,
            'domains_found': 0
        }
        
        # ç®€å•ç¼“å­˜
        self.url_cache = {}
        
        # åˆ›å»ºç›®å½•
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶ï¼ˆä»…å½“ä¸å­˜åœ¨æ—¶ï¼‰
        self.create_example_sources()
    
    def create_example_sources(self):
        """åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶"""
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("""# é»‘åå•è§„åˆ™æº
# æ¯è¡Œä¸€ä¸ªURL

# AdGuard åŸºç¡€è¿‡æ»¤å™¨ï¼ˆæ¨èï¼‰
https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt
https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/tracking.txt
""")
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("""# ç™½åå•è§„åˆ™æº
# æ¯è¡Œä¸€ä¸ªURL

# AdGuard ç™½åå•ï¼ˆæ¨èï¼‰
https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt
""")
    
    def load_sources_fast(self) -> bool:
        """å¿«é€ŸåŠ è½½è§„åˆ™æº"""
        print("ğŸ“‹ åŠ è½½è§„åˆ™æº...")
        start_time = time.time()
        
        # åŠ è½½é»‘åå•æºï¼ˆè·³è¿‡éªŒè¯ä»¥åŠ é€Ÿï¼‰
        if os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                self.black_urls = [line.strip() for line in f 
                                 if line.strip() and not line.startswith('#')]
        else:
            print(f"âŒ é»‘åå•æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['BLACK_SOURCE']}")
            return False
        
        # åŠ è½½ç™½åå•æº
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                self.white_urls = [line.strip() for line in f 
                                 if line.strip() and not line.startswith('#')]
        else:
            print(f"âš ï¸  ç™½åå•æºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­å¤„ç†")
        
        # ç®€å•çš„URLæ ¼å¼æ£€æŸ¥ï¼ˆå¿«é€Ÿï¼‰
        valid_black_urls = []
        for url in self.black_urls:
            if url.startswith('http://') or url.startswith('https://'):
                valid_black_urls.append(url)
            else:
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆURLï¼ˆéHTTP/HTTPSï¼‰: {url}")
        
        valid_white_urls = []
        for url in self.white_urls:
            if url.startswith('http://') or url.startswith('https://'):
                valid_white_urls.append(url)
            else:
                print(f"âš ï¸  è·³è¿‡æ— æ•ˆURLï¼ˆéHTTP/HTTPSï¼‰: {url}")
        
        self.black_urls = valid_black_urls
        self.white_urls = valid_white_urls
        
        if not self.black_urls:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é»‘åå•æºURL")
            return False
        
        self.stats['load_time'] = time.time() - start_time
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.black_urls)} é»‘åå•æº, {len(self.white_urls)} ç™½åå•æº")
        return True
    
    def download_url_fast(self, url: str) -> Optional[str]:
        """å¿«é€Ÿä¸‹è½½URLå†…å®¹ï¼ˆå¸¦ç®€å•ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥ç¼“å­˜
        if CONFIG['CACHE_ENABLED']:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            if cache_key in self.url_cache:
                return self.url_cache[cache_key]
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0',
                'Accept': 'text/plain,text/html',
                'Accept-Encoding': 'gzip, deflate'
            }
            
            response = requests.get(
                url, 
                headers=headers, 
                timeout=CONFIG['TIMEOUT'],
                verify=True,
                stream=False
            )
            
            if response.status_code == 200:
                content = response.text
                
                # ç¼“å­˜ç»“æœ
                if CONFIG['CACHE_ENABLED']:
                    cache_key = hashlib.md5(url.encode()).hexdigest()
                    self.url_cache[cache_key] = content
                
                return content
            else:
                logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: çŠ¶æ€ç  {response.status_code}")
                return None
                
        except Exception as e:
            logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
            return None
    
    def download_all_fast(self) -> List[Tuple[str, str, str]]:
        """å¿«é€Ÿä¸‹è½½æ‰€æœ‰URL"""
        print(f"ğŸ“¥ ä¸‹è½½è§„åˆ™æº ({len(self.black_urls) + len(self.white_urls)}ä¸ª)...")
        start_time = time.time()
        
        all_urls = []
        for url in self.black_urls:
            all_urls.append((url, 'black'))
        for url in self.white_urls:
            all_urls.append((url, 'white'))
        
        results = []
        successful = 0
        failed = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            future_to_url = {executor.submit(self.download_url_fast, url): (url, url_type) 
                           for url, url_type in all_urls}
            
            for future in concurrent.futures.as_completed(future_to_url):
                url, url_type = future_to_url[future]
                try:
                    content = future.result()
                    if content:
                        results.append((url, url_type, content))
                        successful += 1
                        if successful % 5 == 0:  # æ¯5ä¸ªæˆåŠŸæ˜¾ç¤ºä¸€æ¬¡
                            print(f"  âœ… å·²ä¸‹è½½ {successful}/{len(all_urls)}")
                    else:
                        failed += 1
                        print(f"  âŒ ä¸‹è½½å¤±è´¥: {url}")
                except Exception as e:
                    failed += 1
                    logger.error(f"ä¸‹è½½å¼‚å¸¸ {url}: {e}")
        
        self.stats['download_time'] = time.time() - start_time
        self.stats['urls_processed'] = len(all_urls)
        
        print(f"âœ… ä¸‹è½½å®Œæˆ: {successful}æˆåŠŸ, {failed}å¤±è´¥")
        
        if successful == 0:
            print("âŒ æ‰€æœ‰è§„åˆ™æºä¸‹è½½éƒ½å¤±è´¥äº†ï¼")
            return []
        
        return results
    
    def extract_domain_fast(self, line: str) -> Tuple[Optional[str], bool]:
        """å¿«é€Ÿæå–åŸŸå"""
        line = line.strip()
        if not line or line.startswith('!') or line.startswith('#'):
            return None, False
        
        # å¿«é€Ÿåˆ¤æ–­æ˜¯å¦ä¸ºç™½åå•
        is_whitelist = line.startswith('@@')
        if is_whitelist:
            line = line[2:]  # ç§»é™¤@@
        
        # å¸¸è§æ ¼å¼çš„å¿«é€Ÿæå–
        if line.startswith('||'):
            # æå– ||domain.com^ æ ¼å¼
            if '^' in line:
                domain = line[2:line.find('^')]
            else:
                domain = line[2:]
        elif re.match(r'^\d+\.\d+\.\d+\.\d+\s+', line):
            # æå– Hosts æ ¼å¼: 0.0.0.0 domain.com
            parts = line.split()
            domain = parts[1] if len(parts) > 1 else None
        elif line.startswith('*.'):
            # æå–é€šé…ç¬¦æ ¼å¼: *.domain.com
            domain = line[2:]
        elif '.' in line and not any(c in line for c in ' /$#%&?'):
            # ç®€å•åŸŸåæ ¼å¼
            domain = line.split('^')[0] if '^' in line else line
        else:
            return None, False
        
        # æ¸…ç†å’ŒéªŒè¯åŸŸå
        if domain:
            domain = domain.lower()
            domain = re.sub(r'^www\d*\.', '', domain)
            domain = re.sub(r'^\.+|\.+$', '', domain)
            
            # å¿«é€ŸéªŒè¯
            if (3 <= len(domain) <= 253 and 
                '.' in domain and
                not any(exclude in domain for exclude in ['localhost', '127.0.0.1', '0.0.0.0', '::1']) and
                re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]*[a-z0-9])?)+$', domain)):
                return domain, is_whitelist
        
        return None, False
    
    def parse_content_fast(self, content: str, source_type: str) -> Tuple[Set[str], Set[str]]:
        """å¿«é€Ÿè§£æè§„åˆ™å†…å®¹"""
        black_domains = set()
        white_domains = set()
        
        lines = content.split('\n')
        batch_size = CONFIG['BATCH_SIZE']
        
        # åˆ†æ‰¹å¤„ç†ä»¥æé«˜æ€§èƒ½
        for i in range(0, len(lines), batch_size):
            batch = lines[i:i + batch_size]
            for line in batch:
                domain, is_whitelist = self.extract_domain_fast(line)
                if domain:
                    if is_whitelist:
                        white_domains.add(domain)
                    else:
                        black_domains.add(domain)
        
        return black_domains, white_domains
    
    def process_results_fast(self, results: List[Tuple[str, str, str]]):
        """å¿«é€Ÿå¤„ç†ä¸‹è½½ç»“æœ"""
        print("ğŸ” è§£æå’Œå¤„ç†è§„åˆ™...")
        start_time = time.time()
        
        all_black_domains = set()
        all_white_domains = set()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¹¶è¡Œè§£ææ‰€æœ‰å†…å®¹
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = []
            for url, url_type, content in results:
                future = executor.submit(self.parse_content_fast, content, url_type)
                futures.append((future, url_type))
            
            for future, url_type in futures:
                black_domains, white_domains = future.result()
                
                if url_type == 'black':
                    all_black_domains.update(black_domains)
                    all_white_domains.update(white_domains)  # é»‘åå•æºä¸­çš„ç™½åå•
                else:
                    # ç™½åå•æºï¼šä¼˜å…ˆä½¿ç”¨
                    all_white_domains.update(white_domains)
        
        # ç¬¬äºŒé˜¶æ®µï¼šåº”ç”¨ç™½åå•è¿‡æ»¤
        print(f"ğŸ”„ åº”ç”¨ç™½åå•è¿‡æ»¤...")
        print(f"  åŸå§‹é»‘åå•: {len(all_black_domains):,} ä¸ª")
        print(f"  ç™½åå•: {len(all_white_domains):,} ä¸ª")
        
        # æ„å»ºç™½åå•å‰ç¼€æ ‘ä»¥åŠ é€ŸåŒ¹é…
        white_tree = {}
        for domain in all_white_domains:
            parts = domain.split('.')
            parts.reverse()
            node = white_tree
            for part in parts:
                if part not in node:
                    node[part] = {}
                node = node[part]
            node['*'] = True
        
        # ä½¿ç”¨å‰ç¼€æ ‘å¿«é€Ÿè¿‡æ»¤
        filtered_black_domains = set()
        for domain in all_black_domains:
            parts = domain.split('.')
            parts.reverse()
            node = white_tree
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•ä¸­
            is_whitelisted = False
            for part in parts:
                if '*' in node:
                    is_whitelisted = True
                    break
                if part in node:
                    node = node[part]
                else:
                    break
            else:
                if '*' in node:
                    is_whitelisted = True
            
            if not is_whitelisted:
                filtered_black_domains.add(domain)
        
        removed = len(all_black_domains) - len(filtered_black_domains)
        print(f"âœ… è¿‡æ»¤å®Œæˆ: ç§»é™¤ {removed} ä¸ªåŸŸå")
        print(f"  å‰©ä½™é»‘åå•: {len(filtered_black_domains):,} ä¸ª")
        
        self.black_domains = filtered_black_domains
        self.white_domains = all_white_domains
        self.stats['domains_found'] = len(self.black_domains)
        self.stats['parse_time'] = time.time() - start_time
    
    def generate_files_fast(self):
        """å¿«é€Ÿç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        start_time = time.time()
        
        # å‡†å¤‡æ’åºçš„åŸŸååˆ—è¡¨
        black_domains_sorted = sorted(self.black_domains)
        white_domains_sorted = sorted(self.white_domains)
        
        # ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶çš„å†…å®¹
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        version = datetime.now().strftime('%Y%m%d_%H%M')
        base_info = f"! ç”Ÿæˆæ—¶é—´: {timestamp}\n! ç‰ˆæœ¬: {version}\n"
        
        # 1. Adblockè§„åˆ™ (ad.txt)
        ad_content = f"""! å¹¿å‘Šè¿‡æ»¤è§„åˆ™
{base_info}! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª
! ç™½åå•åŸŸå: {len(self.white_domains):,} ä¸ª
! é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}

! ========== ç™½åå•è§„åˆ™ ==========
"""
        ad_content += '\n'.join(f'@@||{domain}^' for domain in white_domains_sorted)
        ad_content += '\n\n! ========== é»‘åå•è§„åˆ™ ==========\n'
        ad_content += '\n'.join(f'||{domain}^' for domain in black_domains_sorted)
        
        # 2. DNSè§„åˆ™ (dns.txt)
        dns_content = f"""# DNSè¿‡æ»¤è§„åˆ™
# ç”Ÿæˆæ—¶é—´: {timestamp}
# ç‰ˆæœ¬: {version}
# åŸŸåæ•°é‡: {len(self.black_domains):,}
# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}

"""
        dns_content += '\n'.join(black_domains_sorted)
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        hosts_content = f"""# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™
# ç”Ÿæˆæ—¶é—´: {timestamp}
# ç‰ˆæœ¬: {version}
# åŸŸåæ•°é‡: {len(self.black_domains):,}
# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}

127.0.0.1 localhost
::1 localhost

# å¹¿å‘ŠåŸŸåå±è”½
"""
        hosts_content += '\n'.join(f'0.0.0.0 {domain}' for domain in black_domains_sorted)
        
        # 4. é»‘åå•è§„åˆ™ (black.txt)
        black_content = f"""! é»‘åå•è§„åˆ™
! ç”Ÿæˆæ—¶é—´: {timestamp}
! ç‰ˆæœ¬: {version}
! åŸŸåæ•°é‡: {len(self.black_domains):,}

"""
        black_content += '\n'.join(f'||{domain}^' for domain in black_domains_sorted)
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        white_content = f"""! ç™½åå•è§„åˆ™
! ç”Ÿæˆæ—¶é—´: {timestamp}
! ç‰ˆæœ¬: {version}
! åŸŸåæ•°é‡: {len(self.white_domains):,}

"""
        white_content += '\n'.join(f'@@||{domain}^' for domain in white_domains_sorted)
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': version,
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains)
            },
            'performance': self.stats
        }
        
        # æ‰¹é‡å†™å…¥æ–‡ä»¶
        print("  å†™å…¥æ–‡ä»¶...")
        with open(CONFIG['AD_FILE'], 'w', encoding='utf-8') as f:
            f.write(ad_content)
        
        with open(CONFIG['DNS_FILE'], 'w', encoding='utf-8') as f:
            f.write(dns_content)
        
        with open(CONFIG['HOSTS_FILE'], 'w', encoding='utf-8') as f:
            f.write(hosts_content)
        
        with open(CONFIG['BLACK_FILE'], 'w', encoding='utf-8') as f:
            f.write(black_content)
        
        with open(CONFIG['WHITE_FILE'], 'w', encoding='utf-8') as f:
            f.write(white_content)
        
        with open(CONFIG['INFO_FILE'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        self.stats['write_time'] = time.time() - start_time
        print("âœ… è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme_fast(self):
        """å¿«é€Ÿç”ŸæˆREADME.md"""
        print("ğŸ“– ç”ŸæˆREADME.md...")
        
        # è¯»å–è§„åˆ™ä¿¡æ¯
        try:
            with open(CONFIG['INFO_FILE'], 'r', encoding='utf-8') as f:
                info = json.load(f)
        except:
            info = {
                'version': datetime.now().strftime('%Y%m%d'),
                'updated_at': datetime.now().isoformat(),
                'rules': {'blacklist_domains': 0, 'whitelist_domains': 0}
            }
        
        # ç”Ÿæˆé“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        # ç”ŸæˆREADMEå†…å®¹
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} æ›´æ–°å†…å®¹ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print("âœ… README.mdç”Ÿæˆå®Œæˆ")
    
    def run_fast(self):
        """å¿«é€Ÿè¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("âš¡ æ€§èƒ½ä¼˜åŒ–ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("=" * 60)
        
        total_start_time = time.time()
        
        try:
            # 1. å¿«é€ŸåŠ è½½è§„åˆ™æº
            print("\nğŸš€ æ­¥éª¤ 1/5: åŠ è½½è§„åˆ™æº")
            if not self.load_sources_fast():
                return False
            
            # 2. å¿«é€Ÿä¸‹è½½æ‰€æœ‰è§„åˆ™æº
            print(f"\nğŸš€ æ­¥éª¤ 2/5: ä¸‹è½½è§„åˆ™æº")
            results = self.download_all_fast()
            if not results:
                print("âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•è§„åˆ™æº")
                return False
            
            # 3. å¿«é€Ÿå¤„ç†ç»“æœ
            print(f"\nğŸš€ æ­¥éª¤ 3/5: è§£æå’Œå¤„ç†è§„åˆ™")
            self.process_results_fast(results)
            
            # 4. å¿«é€Ÿç”Ÿæˆè§„åˆ™æ–‡ä»¶
            print(f"\nğŸš€ æ­¥éª¤ 4/5: ç”Ÿæˆè§„åˆ™æ–‡ä»¶")
            self.generate_files_fast()
            
            # 5. å¿«é€Ÿç”ŸæˆREADME
            print(f"\nğŸš€ æ­¥éª¤ 5/5: ç”ŸæˆREADME.md")
            self.generate_readme_fast()
            
            # è®¡ç®—æ€»æ—¶é—´
            self.stats['total_time'] = time.time() - total_start_time
            self.stats['process_time'] = self.stats['total_time'] - (
                self.stats['load_time'] + self.stats['download_time'] + 
                self.stats['parse_time'] + self.stats['write_time']
            )
            
            # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            print("\n" + "=" * 60)
            print("ğŸ‰ å¤„ç†å®Œæˆï¼")
            print("=" * 60)
            print(f"â±ï¸  æ€»è€—æ—¶: {self.stats['total_time']:.2f}ç§’")
            print(f"ğŸ“Š æ€§èƒ½åˆ†æ:")
            print(f"  â€¢ åŠ è½½æºæ–‡ä»¶: {self.stats['load_time']:.2f}ç§’")
            print(f"  â€¢ ä¸‹è½½è§„åˆ™æº: {self.stats['download_time']:.2f}ç§’")
            print(f"  â€¢ è§£æå’Œå¤„ç†: {self.stats['parse_time']:.2f}ç§’")
            print(f"  â€¢ å†™å…¥æ–‡ä»¶: {self.stats['write_time']:.2f}ç§’")
            print(f"  â€¢ å…¶ä»–å¤„ç†: {self.stats['process_time']:.2f}ç§’")
            print("=" * 60)
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ“ˆ å¤„ç†æ•ˆç‡: {self.stats['domains_found'] / max(0.1, self.stats['total_time']):.0f} åŸŸå/ç§’")
            print("=" * 60)
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("ğŸ”— è®¢é˜…åœ°å€å·²åœ¨README.mdä¸­æ›´æ–°")
            print("=" * 60)
            
            return True
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            return False
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help' or sys.argv[1] == '-h':
            print("âš¡ æ€§èƒ½ä¼˜åŒ–ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  python run.py              # æ­£å¸¸è¿è¡Œ")
            print("  python run.py --test       # æ€§èƒ½æµ‹è¯•")
            print("  python run.py --simple     # æç®€æ¨¡å¼")
            print("  python run.py --benchmark  # åŸºå‡†æµ‹è¯•")
            return
        
        elif sys.argv[1] == '--test':
            print("ğŸ”§ æ€§èƒ½æµ‹è¯•æ¨¡å¼")
            CONFIG['MAX_WORKERS'] = 5
            CONFIG['TIMEOUT'] = 10
            CONFIG['RETRY_TIMES'] = 1
            CONFIG['CACHE_ENABLED'] = False
        
        elif sys.argv[1] == '--simple':
            print("ğŸ”§ æç®€æ¨¡å¼")
            CONFIG['MAX_WORKERS'] = 5
            CONFIG['TIMEOUT'] = 10
            CONFIG['BATCH_SIZE'] = 1000
        
        elif sys.argv[1] == '--benchmark':
            print("ğŸ“Š åŸºå‡†æµ‹è¯•æ¨¡å¼")
            import timeit
            
            # æµ‹è¯•åŸŸåæå–é€Ÿåº¦
            test_lines = [
                "||example.com^",
                "||ad.example.com^",
                "0.0.0.0 tracking.com",
                "@@||whitelist.com^",
                "||sub.domain.com^$third-party",
                "# æ³¨é‡Šè¡Œ",
                "! æ³¨é‡Šè¡Œ",
                "",
                "||another-example.com^"
            ]
            
            generator = FastAdBlockGenerator()
            
            # æµ‹è¯•åŸŸåæå–
            print("æµ‹è¯•åŸŸåæå–é€Ÿåº¦...")
            start = time.time()
            for line in test_lines * 1000:
                generator.extract_domain_fast(line)
            elapsed = time.time() - start
            print(f"  æå–é€Ÿåº¦: {len(test_lines) * 1000 / elapsed:.0f} è¡Œ/ç§’")
            
            # æµ‹è¯•è§£æé€Ÿåº¦
            print("\næµ‹è¯•è§£æé€Ÿåº¦...")
            test_content = "\n".join(test_lines * 100)
            start = time.time()
            black, white = generator.parse_content_fast(test_content, 'test')
            elapsed = time.time() - start
            print(f"  è§£æé€Ÿåº¦: {len(test_lines) * 100 / elapsed:.0f} è¡Œ/ç§’")
            print(f"  æ‰¾åˆ°åŸŸå: {len(black)} é»‘åå•, {len(white)} ç™½åå•")
            
            return
    
    # æ­£å¸¸è¿è¡Œ
    print("âš¡ æ­£åœ¨å¯åŠ¨æ€§èƒ½ä¼˜åŒ–ç‰ˆ...")
    print(f"é…ç½®: {CONFIG['MAX_WORKERS']}çº¿ç¨‹, {CONFIG['TIMEOUT']}ç§’è¶…æ—¶")
    
    generator = FastAdBlockGenerator()
    success = generator.run_fast()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
