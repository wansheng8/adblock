#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - è½»é‡ä¼˜åŒ–ç‰ˆ
ä¸ä½¿ç”¨tqdmä¾èµ–
"""

import os
import re
import json
import time
import hashlib
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, Dict, List, Optional, Tuple
import requests
import urllib.parse
from collections import defaultdict
import sys

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 3,
    'TIMEOUT': 60,
    'RETRY_TIMES': 3,
    
    # è§„åˆ™æºæ–‡ä»¶
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    'CHINA_SOURCE': 'rules/sources/china.txt',
    'ENHANCED_SOURCE': 'rules/sources/enhanced.txt',
    
    # è¾“å‡ºæ–‡ä»¶
    'OUTPUT_FILES': {
        'ad': 'rules/outputs/ad.txt',
        'dns': 'rules/outputs/dns.txt',
        'hosts': 'rules/outputs/hosts.txt',
        'black': 'rules/outputs/black.txt',
        'white': 'rules/outputs/white.txt',
        'info': 'rules/outputs/info.json',
        'smart_ad': 'rules/outputs/smart_ad.txt',
        'mobile_ad': 'rules/outputs/mobile_ad.txt',
    },
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    'PERFORMANCE': {
        'max_total_domains': 200000,  # æœ€å¤§åŸŸåæ€»æ•°
        'skip_some_sources': True,    # è·³è¿‡éƒ¨åˆ†å¤§æ–‡ä»¶æº
        'batch_size': 5000,           # æ‰¹é‡å¤„ç†å¤§å°
    },
    
    # æ’é™¤çš„åŸŸå
    'EXCLUDE_DOMAINS': [
        'localhost', 'local', 'broadcasthost',
        '127.0.0.1', '0.0.0.0', '::1'
    ],
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SimpleProgressBar:
    """ç®€å•çš„è¿›åº¦æ¡ï¼ˆä¸ä½¿ç”¨tqdmï¼‰"""
    
    @staticmethod
    def progress_bar(iteration, total, prefix='', suffix='', length=50, fill='â–ˆ'):
        """åˆ›å»ºæ–‡æœ¬è¿›åº¦æ¡"""
        percent = ("{0:.1f}").format(100 * (iteration / float(total)))
        filled_length = int(length * iteration // total)
        bar = fill * filled_length + '-' * (length - filled_length)
        return f'\r{prefix} |{bar}| {percent}% {suffix}'
    
    @staticmethod
    def print_progress(iteration, total, prefix='', suffix=''):
        """æ‰“å°è¿›åº¦æ¡"""
        print(SimpleProgressBar.progress_bar(iteration, total, prefix, suffix), end='\r')
        if iteration == total:
            print()

class LightweightAdBlockGenerator:
    """è½»é‡ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç²¾ç®€çš„æºæ–‡ä»¶
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æºï¼ˆç²¾ç®€é«˜æ•ˆç‰ˆï¼‰\n")
                f.write("# æ ¸å¿ƒå¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n")
                f.write("https://easylist.to/easylist/easylist.txt\n")
                f.write("https://easylist.to/easylist/easyprivacy.txt\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/filters.txt\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ChineseFilter.txt\n")
                f.write("# ä»…ä¿ç•™é«˜è´¨é‡æºï¼Œé¿å…è¿‡å¤šåŸŸå\n")
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n")
        
        # åˆ›å»ºä¸­æ–‡æºæ–‡ä»¶
        if not os.path.exists(CONFIG['CHINA_SOURCE']):
            with open(CONFIG['CHINA_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ä¸­æ–‡å¹¿å‘Šè§„åˆ™æº\n")
                f.write("https://easylist-downloads.adblockplus.org/easylistchina.txt\n")
                f.write("https://raw.githubusercontent.com/cjx82630/cjxlist/master/cjx-annoyance.txt\n")
    
    def load_sources(self):
        """åŠ è½½è§„åˆ™æºURL"""
        logger.info("åŠ è½½è§„åˆ™æº...")
        
        # é»‘åå•æº
        with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # è·³è¿‡å¯èƒ½çš„å¤§æ–‡ä»¶æºä»¥æé«˜æ€§èƒ½
                    if CONFIG['PERFORMANCE']['skip_some_sources']:
                        if any(skip in line for skip in [
                            'blocklistproject',
                            'hblock',
                            'big.oisd.nl',
                            'oisd.nl',
                            'hagezi'
                        ]):
                            logger.info(f"è·³è¿‡å¯èƒ½çš„å¤§æ–‡ä»¶æº: {line}")
                            continue
                    self.black_urls.append(line)
        
        # ç™½åå•æº
        with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.white_urls.append(line)
        
        # ä¸­æ–‡æº
        try:
            with open(CONFIG['CHINA_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.black_urls.append(line)
        except FileNotFoundError:
            pass
        
        logger.info(f"åŠ è½½ {len(self.black_urls)} ä¸ªé»‘åå•æº")
        logger.info(f"åŠ è½½ {len(self.white_urls)} ä¸ªç™½åå•æº")
    
    def download_url(self, url: str) -> Optional[str]:
        """ä¸‹è½½URLå†…å®¹"""
        for attempt in range(CONFIG['RETRY_TIMES']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                    return None
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæœ‰æ•ˆæ€§"""
        if not domain or domain in CONFIG['EXCLUDE_DOMAINS']:
            return False
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(domain) < 4 or len(domain) > 253:
            return False
        
        # å¿…é¡»åŒ…å«ç‚¹å·
        if '.' not in domain:
            return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        parts = domain.split('.')
        for part in parts:
            if not part:  # ä¸èƒ½æœ‰ç©ºçš„æ®µ
                return False
            if len(part) > 63:
                return False
            # å…è®¸å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        # é¡¶çº§åŸŸåè‡³å°‘2ä¸ªå­—ç¬¦
        if len(parts[-1]) < 2:
            return False
        
        return True
    
    def extract_domain_fast(self, line: str) -> Optional[str]:
        """å¿«é€ŸåŸŸåæå–"""
        line = line.strip()
        
        # å¿«é€Ÿè·³è¿‡
        if not line or len(line) < 4:
            return None
        
        # è·³è¿‡æ³¨é‡Š
        if line[0] in '!#/':
            return None
        
        # å¸¸è§æ¨¡å¼åŒ¹é…
        if '||' in line and '^' in line:
            # å¤„ç† ||domain.com^ æ ¼å¼
            match = re.match(r'^\|\|([a-zA-Z0-9.-]+)\^', line)
            if match:
                domain = match.group(1).lower()
                domain = domain.replace('www.', '').replace('*.', '')
                if self.is_valid_domain(domain):
                    return domain
        
        elif line.startswith('0.0.0.0 ') or line.startswith('127.0.0.1 '):
            # å¤„ç† hosts æ ¼å¼
            parts = line.split()
            if len(parts) >= 2:
                domain = parts[1].lower()
                domain = domain.replace('www.', '').replace('*.', '')
                if self.is_valid_domain(domain):
                    return domain
        
        elif re.match(r'^[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', line):
            # çº¯åŸŸåæ ¼å¼
            domain = line.lower()
            domain = domain.replace('www.', '').replace('*.', '')
            if self.is_valid_domain(domain):
                return domain
        
        return None
    
    def parse_content_fast(self, content: str) -> tuple:
        """å¿«é€Ÿè§£æè§„åˆ™å†…å®¹"""
        black_domains = set()
        white_domains = set()
        
        lines = content.split('\n')
        total_lines = len(lines)
        
        # æ˜¾ç¤ºè¿›åº¦
        for i, line in enumerate(lines):
            if i % 10000 == 0 and i > 0:
                logger.debug(f"è§£æè¿›åº¦: {i}/{total_lines} è¡Œ")
            
            domain = self.extract_domain_fast(line)
            if domain:
                if line.startswith('@@'):
                    white_domains.add(domain)
                else:
                    black_domains.add(domain)
        
        return black_domains, white_domains
    
    def download_and_parse_all(self):
        """ä¸‹è½½å¹¶è§£ææ‰€æœ‰è§„åˆ™"""
        logger.info("å¼€å§‹ä¸‹è½½å’Œè§£æè§„åˆ™...")
        
        all_urls = self.black_urls + self.white_urls
        total_urls = len(all_urls)
        
        results = []
        failed_urls = []
        
        # æ˜¾ç¤ºè¿›åº¦
        print(f"æ€»å…±æœ‰ {total_urls} ä¸ªURLéœ€è¦å¤„ç†")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            # æäº¤ä¸‹è½½ä»»åŠ¡
            future_to_url = {executor.submit(self.download_url, url): url for url in all_urls}
            
            # å¤„ç†ç»“æœ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_url):
                url = future_to_url[future]
                completed += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if completed % 5 == 0 or completed == total_urls:
                    SimpleProgressBar.print_progress(completed, total_urls, prefix='ä¸‹è½½è¿›åº¦:', suffix='å®Œæˆ')
                
                try:
                    content = future.result()
                    if content:
                        black_domains, white_domains = self.parse_content_fast(content)
                        results.append((black_domains, white_domains))
                        logger.debug(f"å¤„ç†å®Œæˆ: {url} ({len(black_domains)} åŸŸå)")
                    else:
                        failed_urls.append(url)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
                    failed_urls.append(url)
        
        print()  # æ¢è¡Œ
        
        # åˆå¹¶ç»“æœ
        for black_domains, white_domains in results:
            self.black_domains.update(black_domains)
            self.white_domains.update(white_domains)
        
        if failed_urls:
            logger.warning(f"æœ‰ {len(failed_urls)} ä¸ªURLå¤„ç†å¤±è´¥")
        
        logger.info(f"è§£æå®Œæˆ: é»‘åå•åŸŸå {len(self.black_domains):,} ä¸ª")
        logger.info(f"ç™½åå•åŸŸå {len(self.white_domains):,} ä¸ª")
    
    def apply_whitelist_simple(self):
        """ç®€å•åº”ç”¨ç™½åå•"""
        if not self.white_domains:
            logger.warning("æ²¡æœ‰ç™½åå•åŸŸå")
            return
        
        logger.info("åº”ç”¨ç™½åå•...")
        
        original_count = len(self.black_domains)
        
        # ç›´æ¥åŒ¹é…ç§»é™¤
        self.black_domains -= self.white_domains
        
        # åªæ£€æŸ¥ç›´æ¥å­åŸŸåï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
        white_suffixes = {f".{domain}" for domain in self.white_domains}
        
        to_remove = set()
        for black_domain in self.black_domains:
            for suffix in white_suffixes:
                if black_domain.endswith(suffix):
                    to_remove.add(black_domain)
                    break
        
        self.black_domains -= to_remove
        
        removed = original_count - len(self.black_domains)
        logger.info(f"ç™½åå•åº”ç”¨å®Œæˆ: ç§»é™¤ {removed} ä¸ªåŸŸåï¼Œå‰©ä½™ {len(self.black_domains):,} ä¸ª")
    
    def filter_domains(self):
        """è¿‡æ»¤åŸŸåï¼Œä¿ç•™é«˜è´¨é‡åŸŸå"""
        logger.info("è¿‡æ»¤åŸŸå...")
        
        original_count = len(self.black_domains)
        
        # å¦‚æœåŸŸåå¤ªå¤šï¼Œè¿›è¡Œç­›é€‰
        if len(self.black_domains) > CONFIG['PERFORMANCE']['max_total_domains']:
            logger.info(f"åŸŸåè¿‡å¤š ({len(self.black_domains):,})ï¼Œè¿›è¡Œç­›é€‰...")
            
            # å°†åŸŸåè½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿æ’åº
            domains_list = list(self.black_domains)
            
            # æŒ‰åŸŸåè´¨é‡æ’åºï¼ˆè¾ƒçŸ­çš„åŸŸåé€šå¸¸æ›´é‡è¦ï¼‰
            domains_list.sort(key=lambda x: (len(x.split('.')), len(x)))
            
            # å–å‰Nä¸ª
            domains_list = domains_list[:CONFIG['PERFORMANCE']['max_total_domains']]
            self.black_domains = set(domains_list)
            
            logger.info(f"ç­›é€‰ååŸŸå: {len(self.black_domains):,} ä¸ª")
        
        # ç§»é™¤ä¸€äº›æ˜æ˜¾ä¸æ˜¯å¹¿å‘Šçš„åŸŸå
        good_domains = set()
        ad_keywords = ['ad', 'ads', 'adv', 'track', 'analytics', 'pixel', 'beacon', 'doubleclick', 'googlead']
        
        for domain in self.black_domains:
            # åŒ…å«å¹¿å‘Šå…³é”®è¯çš„åŸŸåä¼˜å…ˆä¿ç•™
            has_ad_keyword = any(keyword in domain for keyword in ad_keywords)
            
            # åŸŸåé•¿åº¦é€‚ä¸­ï¼ˆå¤ªé•¿çš„å¯èƒ½æ˜¯è·¯å¾„ï¼‰
            is_reasonable_length = 4 <= len(domain) <= 50
            
            # ä¸æ˜¯çº¯æ•°å­—åŸŸå
            not_all_numbers = not all(c.isdigit() or c == '.' for c in domain)
            
            if has_ad_keyword or (is_reasonable_length and not_all_numbers):
                good_domains.add(domain)
        
        self.black_domains = good_domains
        logger.info(f"æœ€ç»ˆåŸŸåæ•°: {len(self.black_domains):,} ä¸ª")
    
    def generate_files_efficient(self):
        """é«˜æ•ˆç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        logger.info("ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # å…ˆè¿‡æ»¤åŸŸå
        self.filter_domains()
        
        # æ’åºåŸŸå
        sorted_domains = sorted(self.black_domains)
        sorted_white_domains = sorted(self.white_domains)
        
        # 1. Adblockè§„åˆ™ (ad.txt) - æœ€å¸¸ç”¨
        logger.info("ç”Ÿæˆ ad.txt...")
        with open(CONFIG['OUTPUT_FILES']['ad'], 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(sorted_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(sorted_white_domains):,} ä¸ª\n")
            f.write(f"! ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n")
            f.write("! æ¥æº: https://github.com/wansheng8/adblock\n\n")
            
            # æ‰¹é‡å†™å…¥æé«˜æ€§èƒ½
            batch_size = CONFIG['PERFORMANCE']['batch_size']
            total_batches = (len(sorted_domains) + batch_size - 1) // batch_size
            
            for i in range(total_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(sorted_domains))
                batch = sorted_domains[start_idx:end_idx]
                
                for domain in batch:
                    f.write(f"||{domain}^\n")
                
                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == total_batches - 1:
                    SimpleProgressBar.print_progress(i + 1, total_batches, prefix='ç”Ÿæˆad.txt:', suffix='å®Œæˆ')
        
        print()  # æ¢è¡Œ
        
        # 2. DNSè§„åˆ™ (dns.txt) - ç¬¬äºŒå¸¸ç”¨
        logger.info("ç”Ÿæˆ dns.txt...")
        with open(CONFIG['OUTPUT_FILES']['dns'], 'w', encoding='utf-8') as f:
            f.write(f"# DNSè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(sorted_domains):,}\n")
            f.write(f"# ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n\n")
            
            batch_size = CONFIG['PERFORMANCE']['batch_size']
            total_batches = (len(sorted_domains) + batch_size - 1) // batch_size
            
            for i in range(total_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(sorted_domains))
                batch = sorted_domains[start_idx:end_idx]
                
                for domain in batch:
                    f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt) - å¯é€‰ï¼Œå¯ä»¥è·³è¿‡ä»¥å‡å°‘æ—¶é—´
        logger.info("ç”Ÿæˆ hosts.txt...")
        with open(CONFIG['OUTPUT_FILES']['hosts'], 'w', encoding='utf-8') as f:
            f.write(f"# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(sorted_domains):,}\n")
            f.write(f"# ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n\n")
            
            # åªå†™å‰10ä¸‡æ¡ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
            max_hosts = min(100000, len(sorted_domains))
            for i, domain in enumerate(sorted_domains[:max_hosts]):
                f.write(f"0.0.0.0 {domain}\n")
                if i % 10000 == 0 and i > 0:
                    logger.debug(f"hosts.txt è¿›åº¦: {i}/{max_hosts}")
        
        # 4. é»‘åå•è§„åˆ™ (black.txt)
        logger.info("ç”Ÿæˆ black.txt...")
        with open(CONFIG['OUTPUT_FILES']['black'], 'w', encoding='utf-8') as f:
            for domain in sorted_domains[:100000]:  # é™åˆ¶æ•°é‡
                f.write(f"||{domain}^\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        logger.info("ç”Ÿæˆ white.txt...")
        with open(CONFIG['OUTPUT_FILES']['white'], 'w', encoding='utf-8') as f:
            f.write("# ç™½åå•è§„åˆ™\n")
            f.write("# è¿™äº›åŸŸåä¸ä¼šè¢«æ‹¦æˆª\n\n")
            for domain in sorted_white_domains:
                f.write(f"@@||{domain}^\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': datetime.now().strftime('%Y%m%d'),
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains),
                'total_domains': len(self.black_domains) + len(self.white_domains)
            },
            'performance': {
                'max_domains': CONFIG['PERFORMANCE']['max_total_domains'],
                'optimized': True,
                'source_count': len(self.black_urls) + len(self.white_urls)
            }
        }
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        logger.info("è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme_simple(self):
        """ç”Ÿæˆç®€å•çš„README.mdæ–‡ä»¶"""
        logger.info("ç”ŸæˆREADME.md...")
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        version = info['version']
        
        readme_content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ | è¯´æ˜ |
|----------|----------|----------|----------|------|
| å¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` | ä¸»è§„åˆ™ï¼Œæ¨èä½¿ç”¨ |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` | Pi-hole/AdGuard Home |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` | ç³»ç»ŸHostsæ–‡ä»¶ |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` | çº¯é»‘åå•åŸŸå |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` | æ’é™¤è¯¯æ€ |

**ç‰ˆæœ¬ {version} è§„åˆ™ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- æ€»åŸŸåæ•°ï¼š{info['rules']['total_domains']:,} ä¸ª
- è§„åˆ™æºï¼š{info['performance']['source_count']} ä¸ª

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*

## ä½¿ç”¨å»ºè®®

1. **AdGuard/uBlock Origin**ï¼šä½¿ç”¨ `ad.txt` æ–‡ä»¶
2. **Pi-hole/AdGuard Home**ï¼šä½¿ç”¨ `dns.txt` æ–‡ä»¶
3. **ç³»ç»ŸHosts**ï¼šä½¿ç”¨ `hosts.txt` æ–‡ä»¶ï¼ˆå‰10ä¸‡æ¡ï¼‰
4. **è¯¯æŠ¥å¤„ç†**ï¼šæŸ¥çœ‹ `white.txt` æˆ–æäº¤Issue

## ç‰¹ç‚¹

- **è½»é‡é«˜æ•ˆ**ï¼šç»è¿‡ä¼˜åŒ–ï¼Œç”Ÿæˆé€Ÿåº¦å¿«
- **è´¨é‡ä¼˜å…ˆ**ï¼šç­›é€‰é«˜è´¨é‡å¹¿å‘ŠåŸŸå
- **è‡ªåŠ¨æ›´æ–°**ï¼šæ¯æ—¥è‡ªåŠ¨æ›´æ–°
- **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒAdblockã€DNSã€Hostsæ ¼å¼

---
*ç”Ÿæˆå™¨ä»£ç ï¼šhttps://github.com/wansheng8/adblock*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - è½»é‡ä¼˜åŒ–ç‰ˆ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½è§„åˆ™æº
            self.load_sources()
            
            # 2. ä¸‹è½½å’Œè§£æè§„åˆ™
            self.download_and_parse_all()
            
            # 3. åº”ç”¨ç™½åå•
            self.apply_whitelist_simple()
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶
            self.generate_files_efficient()
            
            # 5. ç”ŸæˆREADME.md
            self.generate_readme_simple()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"âœ… ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    print("\nğŸš€ å¯åŠ¨è½»é‡ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨...")
    generator = LightweightAdBlockGenerator()
    
    # è¿è¡Œç”Ÿæˆå™¨
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
