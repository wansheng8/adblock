#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨
æ‰€æœ‰åŠŸèƒ½éƒ½åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
"""

import os
import re
import json
import time
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, Dict, List, Optional
import requests

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 5,
    'TIMEOUT': 30,
    'RETRY_TIMES': 3,
    
    # è§„åˆ™æºæ–‡ä»¶
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # è¾“å‡ºæ–‡ä»¶ï¼ˆå›ºå®šåç§°ï¼‰
    'OUTPUT_FILES': {
        'ad': 'rules/outputs/ad.txt',      # Adblockè§„åˆ™
        'dns': 'rules/outputs/dns.txt',    # DNSè§„åˆ™
        'hosts': 'rules/outputs/hosts.txt', # Hostsè§„åˆ™
        'black': 'rules/outputs/black.txt', # é»‘åå•è§„åˆ™
        'white': 'rules/outputs/white.txt', # ç™½åå•è§„åˆ™
        'info': 'rules/outputs/info.json'  # è§„åˆ™ä¿¡æ¯
    }
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AdBlockGenerator:
    """å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.setup_directories()
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n")
            logger.info(f"åˆ›å»ºç¤ºä¾‹é»‘åå•æº: {CONFIG['BLACK_SOURCE']}")
            
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n")
            logger.info(f"åˆ›å»ºç¤ºä¾‹ç™½åå•æº: {CONFIG['WHITE_SOURCE']}")
    
    def load_sources(self):
        """åŠ è½½è§„åˆ™æºURL"""
        # é»‘åå•æº
        with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.black_urls.append(line)
        
        # ç™½åå•æº
        with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.white_urls.append(line)
        
        logger.info(f"åŠ è½½ {len(self.black_urls)} ä¸ªé»‘åå•æº")
        logger.info(f"åŠ è½½ {len(self.white_urls)} ä¸ªç™½åå•æº")
    
    def download_url(self, url: str) -> Optional[str]:
        """ä¸‹è½½URLå†…å®¹"""
        for attempt in range(CONFIG['RETRY_TIMES']):
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                    return None
    
    def parse_content(self, content: str) -> tuple:
        """è§£æè§„åˆ™å†…å®¹"""
        black_domains = set()
        black_rules = set()
        white_domains = set()
        white_rules = set()
        
        for line in content.split('\n'):
            line = line.strip()
            if not line or line.startswith('!'):
                continue
            
            # ç™½åå•è§„åˆ™
            if line.startswith('@@'):
                domain = self.extract_domain(line)
                if domain:
                    white_domains.add(domain)
                    white_rules.add(f"@@||{domain}^")
                else:
                    white_rules.add(line)
            
            # é»‘åå•è§„åˆ™
            else:
                domain = self.extract_domain(line)
                if domain:
                    black_domains.add(domain)
                else:
                    if re.search(r'[a-zA-Z0-9]', line):
                        black_rules.add(line)
        
        return black_domains, black_rules, white_domains, white_rules
    
    def extract_domain(self, text: str) -> Optional[str]:
        """æå–åŸŸå"""
        text = text.strip()
        
        # ç§»é™¤æ³¨é‡Š
        if '#' in text:
            text = text.split('#')[0].strip()
        
        # å¤„ç†å„ç§æ ¼å¼
        patterns = [
            r'^@@\|\|([^\^\$]+)\^',  # @@||domain.com^
            r'^\|\|([^\^\$]+)\^',    # ||domain.com^
            r'^@@([^\|\^\$]+)$',     # @@domain.com
            r'^([a-zA-Z0-9.-]+)$',   # domain.com
            r'^\d+\.\d+\.\d+\.\d+\s+([a-zA-Z0-9.-]+)',  # 127.0.0.1 domain.com
            r'^\*\.([a-zA-Z0-9.-]+)',  # *.domain.com
        ]
        
        for pattern in patterns:
            match = re.match(pattern, text)
            if match:
                domain = match.group(1).lower()
                domain = re.sub(r'^www\.', '', domain)
                if self.is_valid_domain(domain):
                    return domain
        
        return None
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæœ‰æ•ˆæ€§"""
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        local_domains = ['localhost', 'local', 'broadcasthost']
        if domain in local_domains:
            return False
        
        # åŸºæœ¬åŸŸåæ ¼å¼æ£€æŸ¥
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        for part in parts:
            if not part or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def download_and_parse_all(self):
        """ä¸‹è½½å¹¶è§£ææ‰€æœ‰è§„åˆ™"""
        logger.info("å¼€å§‹ä¸‹è½½å’Œè§£æè§„åˆ™...")
        
        # ä¸‹è½½æ‰€æœ‰URL
        all_urls = [(url, 'black') for url in self.black_urls] + \
                   [(url, 'white') for url in self.white_urls]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = {executor.submit(self.download_url, url): (url, type_) for url, type_ in all_urls}
            
            for future in concurrent.futures.as_completed(futures):
                url, type_ = futures[future]
                try:
                    content = future.result()
                    if content:
                        black_domains, black_rules, white_domains, white_rules = self.parse_content(content)
                        
                        if type_ == 'black':
                            self.black_domains.update(black_domains)
                            self.black_rules.update(black_rules)
                            # é»‘åå•æºä¸­çš„ç™½åå•ä¹ŸåŠ å…¥
                            self.white_domains.update(white_domains)
                            self.white_rules.update(white_rules)
                        else:
                            self.white_domains.update(white_domains)
                            self.white_rules.update(white_rules)
                            # ç™½åå•æºä¸­çš„é»‘åå•ä¹ŸåŠ å…¥
                            self.black_domains.update(black_domains)
                            self.black_rules.update(black_rules)
                            
                        logger.debug(f"å¤„ç†å®Œæˆ: {url}")
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
        
        logger.info(f"è§£æå®Œæˆ: é»‘åå•åŸŸå {len(self.black_domains):,} ä¸ª")
        logger.info(f"ç™½åå•åŸŸå {len(self.white_domains):,} ä¸ª")
    
    def apply_whitelist(self):
        """åº”ç”¨ç™½åå•"""
        if not self.white_domains:
            return
        
        original = len(self.black_domains)
        self.black_domains -= self.white_domains
        
        # ç®€å•å­åŸŸååŒ¹é…
        if len(self.white_domains) < 10000:
            to_remove = set()
            for black_domain in self.black_domains:
                for white_domain in self.white_domains:
                    if black_domain.endswith(f".{white_domain}"):
                        to_remove.add(black_domain)
                        break
            
            self.black_domains -= to_remove
            removed = original - len(self.black_domains)
            logger.info(f"ç™½åå•åº”ç”¨å®Œæˆ: ç§»é™¤ {removed} ä¸ªåŸŸå")
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        logger.info("ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # 1. Adblockè§„åˆ™ (ad.txt)
        with open(CONFIG['OUTPUT_FILES']['ad'], 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(self.white_domains):,} ä¸ª\n\n")
            
            # ç™½åå•è§„åˆ™
            for rule in sorted(self.white_rules):
                f.write(f"{rule}\n")
            
            f.write("\n")
            
            # é»‘åå•åŸŸåè§„åˆ™
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
            
            f.write("\n")
            
            # å…¶ä»–è§„åˆ™
            for rule in sorted(self.black_rules):
                f.write(f"{rule}\n")
        
        # 2. DNSè§„åˆ™ (dns.txt)
        with open(CONFIG['OUTPUT_FILES']['dns'], 'w', encoding='utf-8') as f:
            f.write(f"# DNSè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n\n")
            
            for domain in sorted(self.black_domains):
                f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        with open(CONFIG['OUTPUT_FILES']['hosts'], 'w', encoding='utf-8') as f:
            f.write(f"# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n\n")
            
            for domain in sorted(self.black_domains):
                f.write(f"0.0.0.0 {domain}\n")
        
        # 4. é»‘åå•è§„åˆ™ (black.txt)
        with open(CONFIG['OUTPUT_FILES']['black'], 'w', encoding='utf-8') as f:
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        with open(CONFIG['OUTPUT_FILES']['white'], 'w', encoding='utf-8') as f:
            for rule in sorted(self.white_rules):
                f.write(f"{rule}\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': datetime.now().strftime('%Y%m%d'),
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains),
                'blacklist_rules': len(self.black_rules),
                'whitelist_rules': len(self.white_rules)
            }
        }
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        logger.info("è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.mdæ–‡ä»¶"""
        logger.info("ç”ŸæˆREADME.md...")
        
        # è·å–è§„åˆ™ä¿¡æ¯
        with open(CONFIG['OUTPUT_FILES']['info'], 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # ç”Ÿæˆé“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        readme_content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} è§„åˆ™ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- å…¶ä»–è§„åˆ™ï¼šé»‘åå• {info['rules']['blacklist_rules']:,} æ¡ï¼Œç™½åå• {info['rules']['whitelist_rules']:,} æ¡

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 50)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("=" * 50)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½è§„åˆ™æº
            self.load_sources()
            
            # 2. ä¸‹è½½å’Œè§£æè§„åˆ™
            self.download_and_parse_all()
            
            # 3. åº”ç”¨ç™½åå•
            self.apply_whitelist()
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶
            self.generate_files()
            
            # 5. ç”ŸæˆREADME.md
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 50)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("=" * 50)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # è¿è¡Œç”Ÿæˆå™¨
    generator = AdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
