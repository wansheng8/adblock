#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - ç»“æ„åŒ–æµç¨‹ç‰ˆ
æµç¨‹ï¼šé‡‡é›† â†’ åˆ†ç±» â†’ å»é‡ â†’ ç”Ÿæˆ
"""

import os
import re
import json
import time
import concurrent.futures
from datetime import datetime, timedelta
from typing import Set, List, Tuple, Dict, Optional
import requests

# é…ç½®ä¿¡æ¯
CONFIG = {
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    'MAX_WORKERS': 8,
    'TIMEOUT': 20,
    'RETRY': 3,
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # å…³é”®å¹¿å‘ŠåŸŸåï¼ˆå¿…é¡»æ‹¦æˆªï¼‰
    'CRITICAL_BLACK_DOMAINS': {
        'doubleclick.net',
        'google-analytics.com',
        'googlesyndication.com',
        'googleadservices.com',
        'adservice.google.com',
        'ads.google.com',
        'scorecardresearch.com',
        'outbrain.com',
        'taboola.com',
        'criteo.com',
        'adnxs.com',
        'amazon-adsystem.com',
        'facebook.net',
        'ads.facebook.com',
        'analytics.google.com'
    },
    
    # å¿…é¡»æ”¾è¡Œçš„åŸŸåï¼ˆçœŸæ­£çš„ç™½åå•ï¼‰
    'CRITICAL_WHITE_DOMAINS': {
        'google.com',
        'github.com',
        'microsoft.com',
        'apple.com',
        'baidu.com',
        'qq.com',
        'zhihu.com',
        'bilibili.com'
    }
}

class StructuredAdBlockGenerator:
    def __init__(self):
        # é˜¶æ®µ1ï¼šé‡‡é›†çš„æ•°æ®
        self.raw_black_rules = []    # åŸå§‹é»‘åå•è§„åˆ™
        self.raw_white_rules = []    # åŸå§‹ç™½åå•è§„åˆ™
        
        # é˜¶æ®µ2ï¼šåˆ†ç±»åçš„æ•°æ®
        self.classified_black_domains = set()  # åˆ†ç±»å‡ºçš„é»‘åå•åŸŸå
        self.classified_white_domains = set()  # åˆ†ç±»å‡ºçš„ç™½åå•åŸŸå
        self.classified_complex_black = []     # åˆ†ç±»å‡ºçš„å¤æ‚é»‘åå•è§„åˆ™
        self.classified_complex_white = []     # åˆ†ç±»å‡ºçš„å¤æ‚ç™½åå•è§„åˆ™
        
        # é˜¶æ®µ3ï¼šå»é‡åçš„æ•°æ®
        self.unique_black_domains = set()      # å»é‡åçš„é»‘åå•åŸŸå
        self.unique_white_domains = set()      # å»é‡åçš„ç™½åå•åŸŸå
        self.unique_complex_black = []         # å»é‡åçš„å¤æ‚é»‘åå•
        self.unique_complex_white = []         # å»é‡åçš„å¤æ‚ç™½åå•
        
        # é˜¶æ®µ4ï¼šæœ€ç»ˆè¾“å‡ºæ•°æ®
        self.final_black_domains = set()       # æœ€ç»ˆé»‘åå•åŸŸå
        self.final_white_rules = []            # æœ€ç»ˆç™½åå•è§„åˆ™
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.statistics = {
            'total_urls': 0,
            'total_lines': 0,
            'black_domains_found': 0,
            'white_domains_found': 0,
            'complex_rules_found': 0,
            'duplicates_removed': 0,
            'processing_time': 0
        }
        
        # åˆ›å»ºç›®å½•
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºé»˜è®¤è§„åˆ™æº
        self.create_default_sources()
    
    # ========== é˜¶æ®µ1ï¼šé‡‡é›† ==========
    
    def create_default_sources(self):
        """åˆ›å»ºé»˜è®¤è§„åˆ™æºæ–‡ä»¶"""
        # é»‘åå•æº
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æº\n")
                f.write("# æ¯è¡Œä¸€ä¸ªURL\n\n")
                f.write("# ä¸»è¦å¹¿å‘Šè§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n\n")
                f.write("# EasyListè§„åˆ™\n")
                f.write("https://easylist.to/easylist/easylist.txt\n\n")
                f.write("# ä¸­æ–‡è§„åˆ™\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ADGUARD_FILTER.txt\n")
        
        # ç™½åå•æº
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("# åªåŒ…å«ç™½åå•è§„åˆ™\n\n")
                f.write("# AdGuardç™½åå•\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n\n")
                f.write("# æ‰‹åŠ¨æ·»åŠ çš„ç™½åå•\n")
                f.write("@@||google.com^\n")
                f.write("@@||github.com^\n")
                f.write("@@||baidu.com^\n")
                f.write("@@||qq.com^\n")
    
    def download_content(self, url: str) -> Optional[str]:
        """ä¸‹è½½è§„åˆ™å†…å®¹"""
        for i in range(CONFIG['RETRY']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain, */*'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(1)
                else:
                    print(f"  âŒ ä¸‹è½½å¤±è´¥: {url}")
        return None
    
    def collect_rules_from_url(self, url: str, is_whitelist_source: bool = False):
        """ä»å•ä¸ªURLé‡‡é›†è§„åˆ™"""
        print(f"  ğŸ“¥ é‡‡é›†: {url}")
        content = self.download_content(url)
        if not content:
            return
        
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            if not line or line.startswith('!') or line.startswith('#'):
                continue
            
            self.statistics['total_lines'] += 1
            
            if is_whitelist_source:
                self.raw_white_rules.append(line)
            else:
                self.raw_black_rules.append(line)
        
        print(f"  âœ“ é‡‡é›†å®Œæˆ: {len(lines)} è¡Œ")
    
    def collect_all_sources(self):
        """é‡‡é›†æ‰€æœ‰è§„åˆ™æº"""
        print("=" * 60)
        print("ğŸ“¥ é˜¶æ®µ1: é‡‡é›†é»‘/ç™½åå•æº")
        print("=" * 60)
        
        # è¯»å–é»‘åå•æºURL
        blacklist_urls = []
        if os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        blacklist_urls.append((line, False))
        
        # è¯»å–ç™½åå•æºURL
        whitelist_urls = []
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        if line.startswith('http'):
                            whitelist_urls.append((line, True))
                        else:
                            # ç›´æ¥æ·»åŠ åˆ°åŸå§‹ç™½åå•è§„åˆ™
                            self.raw_white_rules.append(line)
        
        all_urls = blacklist_urls + whitelist_urls
        self.statistics['total_urls'] = len(all_urls)
        
        if not all_urls:
            print("  âš ï¸ æœªæ‰¾åˆ°è§„åˆ™æºURL")
            return
        
        print(f"  å‘ç° {len(blacklist_urls)} ä¸ªé»‘åå•æº")
        print(f"  å‘ç° {len(whitelist_urls)} ä¸ªç™½åå•æº")
        
        # å¹¶è¡Œé‡‡é›†
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = []
            for url, is_whitelist in all_urls:
                future = executor.submit(self.collect_rules_from_url, url, is_whitelist)
                futures.append(future)
            
            completed = 0
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result(timeout=30)
                    completed += 1
                    print(f"  âœ… [{completed}/{len(all_urls)}] é‡‡é›†å®Œæˆ")
                except Exception as e:
                    print(f"  âŒ é‡‡é›†å¤±è´¥: {e}")
        
        print(f"âœ… é‡‡é›†å®Œæˆ:")
        print(f"   åŸå§‹é»‘åå•è§„åˆ™: {len(self.raw_black_rules):,} æ¡")
        print(f"   åŸå§‹ç™½åå•è§„åˆ™: {len(self.raw_white_rules):,} æ¡")
    
    # ========== é˜¶æ®µ2ï¼šåˆ†ç±» ==========
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        local_domains = {'localhost', 'local', 'broadcasthost', '0.0.0.0', '127.0.0.1', '::1'}
        if domain in local_domains:
            return False
        
        # æ’é™¤IPåœ°å€
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', domain):
            return False
        
        # æ£€æŸ¥åŸŸåæ ¼å¼
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        for part in parts:
            if not part or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def extract_domain_from_rule(self, rule: str) -> Tuple[Optional[str], bool]:
        """ä»è§„åˆ™ä¸­æå–åŸŸå"""
        rule = rule.strip()
        if not rule:
            return None, False
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯ç™½åå•è§„åˆ™
        is_whitelist = rule.startswith('@@')
        
        # å¦‚æœæ˜¯ç™½åå•è§„åˆ™ï¼Œç§»é™¤@@å‰ç¼€
        if is_whitelist:
            rule = rule[2:]
        
        # åŒ¹é…å¸¸è§çš„åŸŸåæ ¼å¼
        patterns = [
            r'^\|\|([a-zA-Z0-9.-]+)\^',     # ||domain.com^
            r'^\|\|([a-zA-Z0-9.-]+)/',      # ||domain.com/
            r'^([a-zA-Z0-9.-]+)\^$',        # domain.com^
            r'^([a-zA-Z0-9.-]+)$',          # domain.com
            r'^\*\.([a-zA-Z0-9.-]+)',       # *.domain.com
        ]
        
        for pattern in patterns:
            match = re.match(pattern, rule)
            if match:
                domain = match.group(1).lower().strip()
                
                # ç§»é™¤wwwå‰ç¼€
                if domain.startswith('www.'):
                    domain = domain[4:]
                
                if self.is_valid_domain(domain):
                    return domain, is_whitelist
        
        return None, is_whitelist
    
    def classify_rule(self, rule: str, is_from_whitelist: bool):
        """åˆ†ç±»å•æ¡è§„åˆ™"""
        domain, extracted_is_whitelist = self.extract_domain_from_rule(rule)
        
        # ç¡®å®šè§„åˆ™ç±»å‹
        is_whitelist_rule = is_from_whitelist or extracted_is_whitelist
        
        if domain:
            # åŸŸåè§„åˆ™
            if is_whitelist_rule:
                self.classified_white_domains.add(domain)
                self.statistics['white_domains_found'] += 1
            else:
                self.classified_black_domains.add(domain)
                self.statistics['black_domains_found'] += 1
        else:
            # å¤æ‚è§„åˆ™
            if is_whitelist_rule:
                self.classified_complex_white.append(rule)
                self.statistics['complex_rules_found'] += 1
            else:
                if len(rule) > 3:
                    self.classified_complex_black.append(rule)
                    self.statistics['complex_rules_found'] += 1
    
    def classify_all_rules(self):
        """åˆ†ç±»æ‰€æœ‰è§„åˆ™"""
        print("\n" + "=" * 60)
        print("ğŸ” é˜¶æ®µ2: åˆ†ç±»é»‘/ç™½åå•")
        print("=" * 60)
        
        print("  åˆ†ç±»é»‘åå•è§„åˆ™...")
        for rule in self.raw_black_rules:
            self.classify_rule(rule, False)
        
        print("  åˆ†ç±»ç™½åå•è§„åˆ™...")
        for rule in self.raw_white_rules:
            self.classify_rule(rule, True)
        
        print(f"âœ… åˆ†ç±»å®Œæˆ:")
        print(f"   åˆ†ç±»å‡ºçš„é»‘åå•åŸŸå: {len(self.classified_black_domains):,} ä¸ª")
        print(f"   åˆ†ç±»å‡ºçš„ç™½åå•åŸŸå: {len(self.classified_white_domains):,} ä¸ª")
        print(f"   å¤æ‚é»‘åå•è§„åˆ™: {len(self.classified_complex_black):,} æ¡")
        print(f"   å¤æ‚ç™½åå•è§„åˆ™: {len(self.classified_complex_white):,} æ¡")
    
    # ========== é˜¶æ®µ3ï¼šå»é‡ ==========
    
    def deduplicate_data(self):
        """å»é‡æ‰€æœ‰æ•°æ®"""
        print("\n" + "=" * 60)
        print("âœ¨ é˜¶æ®µ3: é»‘/ç™½åå•å»é‡")
        print("=" * 60)
        
        # å»é‡é»‘åå•åŸŸå
        original_black_count = len(self.classified_black_domains)
        self.unique_black_domains = self.classified_black_domains.copy()
        black_duplicates = original_black_count - len(self.unique_black_domains)
        
        # å»é‡ç™½åå•åŸŸå
        original_white_count = len(self.classified_white_domains)
        self.unique_white_domains = self.classified_white_domains.copy()
        white_duplicates = original_white_count - len(self.unique_white_domains)
        
        # å»é‡å¤æ‚è§„åˆ™
        original_complex_black = len(self.classified_complex_black)
        self.unique_complex_black = list(set(self.classified_complex_black))
        complex_black_duplicates = original_complex_black - len(self.unique_complex_black)
        
        original_complex_white = len(self.classified_complex_white)
        self.unique_complex_white = list(set(self.classified_complex_white))
        complex_white_duplicates = original_complex_white - len(self.unique_complex_white)
        
        total_duplicates = (black_duplicates + white_duplicates + 
                           complex_black_duplicates + complex_white_duplicates)
        
        self.statistics['duplicates_removed'] = total_duplicates
        
        print(f"âœ… å»é‡å®Œæˆ:")
        print(f"   é»‘åå•åŸŸåå»é‡: {black_duplicates} ä¸ªé‡å¤")
        print(f"   ç™½åå•åŸŸåå»é‡: {white_duplicates} ä¸ªé‡å¤")
        print(f"   å¤æ‚é»‘åå•å»é‡: {complex_black_duplicates} æ¡é‡å¤")
        print(f"   å¤æ‚ç™½åå•å»é‡: {complex_white_duplicates} æ¡é‡å¤")
        print(f"   æ€»è®¡ç§»é™¤é‡å¤: {total_duplicates:,} æ¡")
        print(f"   å”¯ä¸€é»‘åå•åŸŸå: {len(self.unique_black_domains):,} ä¸ª")
        print(f"   å”¯ä¸€ç™½åå•åŸŸå: {len(self.unique_white_domains):,} ä¸ª")
    
    # ========== é˜¶æ®µ4ï¼šç”Ÿæˆ ==========
    
    def apply_whitelist_logic(self):
        """åº”ç”¨ç™½åå•é€»è¾‘"""
        print("\n" + "=" * 60)
        print("âš™ï¸  é˜¶æ®µ4: åº”ç”¨ç™½åå•é€»è¾‘")
        print("=" * 60)
        
        # æœ€ç»ˆé»‘åå• = å”¯ä¸€é»‘åå• - ç™½åå•åŸŸå
        self.final_black_domains = self.unique_black_domains.copy()
        
        # ç§»é™¤ç™½åå•åŸŸåï¼ˆåªç§»é™¤å®Œå…¨åŒ¹é…ï¼‰
        domains_to_remove = set()
        for black_domain in self.final_black_domains:
            if black_domain in self.unique_white_domains:
                domains_to_remove.add(black_domain)
        
        self.final_black_domains -= domains_to_remove
        
        # ç¡®ä¿å…³é”®å¹¿å‘ŠåŸŸåä¸è¢«ç§»é™¤
        for critical_domain in CONFIG['CRITICAL_BLACK_DOMAINS']:
            if critical_domain not in self.final_black_domains:
                self.final_black_domains.add(critical_domain)
        
        # ç¡®ä¿çœŸæ­£çš„ç™½åå•åŸŸåè¢«ä¿ç•™
        for white_domain in CONFIG['CRITICAL_WHITE_DOMAINS']:
            if white_domain in self.final_black_domains:
                self.final_black_domains.remove(white_domain)
            if white_domain not in self.unique_white_domains:
                self.unique_white_domains.add(white_domain)
        
        # å‡†å¤‡æœ€ç»ˆç™½åå•è§„åˆ™
        for domain in self.unique_white_domains:
            self.final_white_rules.append(f"@@||{domain}^")
        self.final_white_rules.extend(self.unique_complex_white)
        
        print(f"âœ… ç™½åå•é€»è¾‘åº”ç”¨å®Œæˆ:")
        print(f"   ç§»é™¤ {len(domains_to_remove)} ä¸ªç™½åå•åŸŸå")
        print(f"   æœ€ç»ˆé»‘åå•åŸŸå: {len(self.final_black_domains):,} ä¸ª")
        print(f"   æœ€ç»ˆç™½åå•è§„åˆ™: {len(self.final_white_rules):,} æ¡")
    
    def generate_file_by_type(self, file_type: str, version: str, timestamp: str):
        """æ ¹æ®ç±»å‹ç”Ÿæˆæ–‡ä»¶"""
        print(f"  ğŸ“„ ç”Ÿæˆ {file_type}.txt...")
        
        if file_type == 'ad':
            # AdBlockæ ¼å¼
            with open('rules/outputs/ad.txt', 'w', encoding='utf-8') as f:
                f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - {version}\n")
                f.write(f"! æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write(f"! é»‘åå•åŸŸå: {len(self.final_black_domains):,} ä¸ª\n")
                f.write(f"! ç™½åå•è§„åˆ™: {len(self.final_white_rules):,} æ¡\n")
                f.write("!\n\n")
                
                # ç™½åå•è§„åˆ™
                if self.final_white_rules:
                    f.write("! ====== ç™½åå•è§„åˆ™ ======\n")
                    for rule in sorted(set(self.final_white_rules)):
                        f.write(f"{rule}\n")
                    f.write("\n")
                
                # é»‘åå•åŸŸåè§„åˆ™
                f.write("! ====== åŸŸåé»‘åå• ======\n")
                for domain in sorted(self.final_black_domains):
                    f.write(f"||{domain}^\n")
                
                # å¤æ‚é»‘åå•è§„åˆ™
                if self.unique_complex_black:
                    f.write("\n! ====== å¤æ‚è§„åˆ™ ======\n")
                    for rule in sorted(set(self.unique_complex_black)):
                        f.write(f"{rule}\n")
        
        elif file_type == 'dns':
            # DNSæ ¼å¼
            with open('rules/outputs/dns.txt', 'w', encoding='utf-8') as f:
                f.write(f"# DNSå¹¿å‘Šè¿‡æ»¤è§„åˆ™ - {version}\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write(f"# åŸŸåæ•°é‡: {len(self.final_black_domains):,} ä¸ª\n")
                f.write("#\n\n")
                
                # å…³é”®åŸŸååœ¨å‰
                critical_domains = []
                other_domains = []
                
                for domain in sorted(self.final_black_domains):
                    if domain in CONFIG['CRITICAL_BLACK_DOMAINS']:
                        critical_domains.append(domain)
                    else:
                        other_domains.append(domain)
                
                if critical_domains:
                    f.write("# å…³é”®å¹¿å‘ŠåŸŸå\n")
                    for domain in critical_domains:
                        f.write(f"{domain}\n")
                    f.write("\n")
                
                f.write("# å…¶ä»–å¹¿å‘ŠåŸŸå\n")
                for domain in other_domains:
                    f.write(f"{domain}\n")
        
        elif file_type == 'hosts':
            # Hostsæ ¼å¼
            with open('rules/outputs/hosts.txt', 'w', encoding='utf-8') as f:
                f.write(f"# Hostså¹¿å‘Šè¿‡æ»¤è§„åˆ™ - {version}\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write(f"# åŸŸåæ•°é‡: {len(self.final_black_domains):,} ä¸ª\n")
                f.write("#\n\n")
                f.write("127.0.0.1 localhost\n")
                f.write("::1 localhost\n")
                f.write("#\n")
                f.write("# å¹¿å‘ŠåŸŸå\n\n")
                
                # åˆ†æ‰¹å†™å…¥
                batch_size = 1000
                domains = sorted(self.final_black_domains)
                for i in range(0, len(domains), batch_size):
                    batch = domains[i:i+batch_size]
                    f.write(f"# åŸŸå {i+1}-{i+len(batch)}\n")
                    for domain in batch:
                        f.write(f"0.0.0.0 {domain}\n")
                    f.write("\n")
        
        elif file_type == 'black':
            # çº¯é»‘åå•
            with open('rules/outputs/black.txt', 'w', encoding='utf-8') as f:
                f.write(f"# é»‘åå•è§„åˆ™ - {version}\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write("#\n\n")
                for domain in sorted(self.final_black_domains):
                    f.write(f"||{domain}^\n")
        
        elif file_type == 'white':
            # çº¯ç™½åå•
            with open('rules/outputs/white.txt', 'w', encoding='utf-8') as f:
                f.write(f"# ç™½åå•è§„åˆ™ - {version}\n")
                f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
                f.write(f"# è§„åˆ™æ•°é‡: {len(set(self.final_white_rules)):,} æ¡\n")
                f.write("#\n\n")
                
                unique_rules = sorted(set(self.final_white_rules))
                domain_rules = [r for r in unique_rules if r.startswith('@@||')]
                other_rules = [r for r in unique_rules if r not in domain_rules]
                
                if domain_rules:
                    f.write("# åŸŸåç™½åå•\n")
                    for rule in domain_rules:
                        f.write(f"{rule}\n")
                    f.write("\n")
                
                if other_rules:
                    f.write("# å…¶ä»–ç™½åå•è§„åˆ™\n")
                    for rule in other_rules:
                        f.write(f"{rule}\n")
        
        elif file_type == 'info':
            # è§„åˆ™ä¿¡æ¯
            info = {
                'version': version,
                'updated_at': timestamp,
                'timezone': 'Asia/Shanghai (UTC+8)',
                'statistics': {
                    'total_urls': self.statistics['total_urls'],
                    'total_lines': self.statistics['total_lines'],
                    'black_domains_found': self.statistics['black_domains_found'],
                    'white_domains_found': self.statistics['white_domains_found'],
                    'complex_rules_found': self.statistics['complex_rules_found'],
                    'duplicates_removed': self.statistics['duplicates_removed'],
                    'final_blacklist_domains': len(self.final_black_domains),
                    'final_whitelist_rules': len(set(self.final_white_rules))
                }
            }
            
            with open('rules/outputs/info.json', 'w', encoding='utf-8') as f:
                json.dump(info, f, indent=2, ensure_ascii=False)
    
    def generate_all_files(self):
        """ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶"""
        print("\n" + "=" * 60)
        print("ğŸš€ é˜¶æ®µ5: ç”Ÿæˆè§„åˆ™æ–‡ä»¶")
        print("=" * 60)
        
        # è·å–æ—¶é—´
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        version = datetime.now().strftime('%Y%m%d')
        
        # ç”Ÿæˆå„ç§ç±»å‹çš„æ–‡ä»¶
        file_types = ['ad', 'dns', 'hosts', 'black', 'white', 'info']
        
        for file_type in file_types:
            self.generate_file_by_type(file_type, version, timestamp)
        
        print(f"âœ… æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
        for file_type in file_types:
            if file_type == 'ad':
                print(f"   ad.txt - AdBlockæ ¼å¼ ({len(self.final_black_domains):,}ä¸ªåŸŸå)")
            elif file_type == 'dns':
                print(f"   dns.txt - DNSæ ¼å¼ ({len(self.final_black_domains):,}ä¸ªåŸŸå)")
            elif file_type == 'hosts':
                print(f"   hosts.txt - Hostsæ ¼å¼ ({len(self.final_black_domains):,}ä¸ªåŸŸå)")
            elif file_type == 'black':
                print(f"   black.txt - é»‘åå•è§„åˆ™")
            elif file_type == 'white':
                print(f"   white.txt - ç™½åå•è§„åˆ™ ({len(set(self.final_white_rules)):,}æ¡)")
            elif file_type == 'info':
                print(f"   info.json - è§„åˆ™ä¿¡æ¯")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.md"""
        print("\n" + "=" * 60)
        print("ğŸ“– ç”ŸæˆREADME.md")
        print("=" * 60)
        
        with open('rules/outputs/info.json', 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªç»“æ„åŒ–ç”Ÿæˆçš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºAdGuardã€uBlock Originã€AdBlock Plusç­‰ã€‚

---

## è®¢é˜…åœ°å€

| è§„åˆ™ç±»å‹ | è§„åˆ™è¯´æ˜ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|:---------|:---------|:---------|:---------|
| **AdBlockè§„åˆ™** | é€‚ç”¨äºæµè§ˆå™¨å¹¿å‘Šæ’ä»¶ | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| **DNSè¿‡æ»¤è§„åˆ™** | é€‚ç”¨äºDNSè¿‡æ»¤è½¯ä»¶ | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| **Hostsè§„åˆ™** | é€‚ç”¨äºç³»ç»Ÿhostsæ–‡ä»¶ | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| **é»‘åå•è§„åˆ™** | çº¯é»‘åå•åŸŸå | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| **ç™½åå•è§„åˆ™** | æ’é™¤è¯¯æ‹¦åŸŸå | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['statistics']['final_blacklist_domains']:,} ä¸ª
- ç™½åå•è§„åˆ™ï¼š{info['statistics']['final_whitelist_rules']:,} æ¡

---

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at']}** (åŒ—äº¬æ—¶é—´)

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print("âœ… README.mdç”Ÿæˆå®Œæˆ")
    
    def run_validation(self):
        """è¿è¡ŒéªŒè¯æ£€æŸ¥"""
        print("\n" + "=" * 60)
        print("ğŸ” éªŒè¯æ£€æŸ¥")
        print("=" * 60)
        
        # æ£€æŸ¥å…³é”®å¹¿å‘ŠåŸŸå
        missing_critical = []
        for domain in CONFIG['CRITICAL_BLACK_DOMAINS']:
            if domain not in self.final_black_domains:
                missing_critical.append(domain)
        
        if missing_critical:
            print(f"âš ï¸  è­¦å‘Š: ç¼ºå¤± {len(missing_critical)} ä¸ªå…³é”®å¹¿å‘ŠåŸŸå")
            for domain in missing_critical[:3]:
                print(f"   - {domain}")
        else:
            print("âœ… æ‰€æœ‰å…³é”®å¹¿å‘ŠåŸŸåå·²åŒ…å«")
        
        # æ£€æŸ¥ç™½åå•æ•°é‡
        white_count = len(set(self.final_white_rules))
        if white_count > 1000:
            print(f"âš ï¸  è­¦å‘Š: ç™½åå•è§„åˆ™è¿‡å¤š ({white_count} æ¡)")
        else:
            print(f"âœ… ç™½åå•æ•°é‡åˆç† ({white_count} æ¡)")
        
        # æ£€æŸ¥é»‘åå•æ•°é‡
        black_count = len(self.final_black_domains)
        if black_count < 10000:
            print(f"âš ï¸  è­¦å‘Š: é»‘åå•åŸŸåè¿‡å°‘ ({black_count:,} ä¸ª)")
        else:
            print(f"âœ… é»‘åå•æ•°é‡åˆç† ({black_count:,} ä¸ª)")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - ç»“æ„åŒ–æµç¨‹")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # é˜¶æ®µ1: é‡‡é›†
            self.collect_all_sources()
            
            # é˜¶æ®µ2: åˆ†ç±»
            self.classify_all_rules()
            
            # é˜¶æ®µ3: å»é‡
            self.deduplicate_data()
            
            # é˜¶æ®µ4: åº”ç”¨ç™½åå•é€»è¾‘
            self.apply_whitelist_logic()
            
            # é˜¶æ®µ5: ç”Ÿæˆæ–‡ä»¶
            self.generate_all_files()
            
            # ç”ŸæˆREADME
            self.generate_readme()
            
            # éªŒè¯æ£€æŸ¥
            self.run_validation()
            
            # ç»Ÿè®¡
            end_time = time.time()
            elapsed = end_time - start_time
            self.statistics['processing_time'] = elapsed
            
            print("\n" + "=" * 60)
            print("ğŸ‰ æµç¨‹å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            print(f"   é»‘åå•åŸŸå: {len(self.final_black_domains):,} ä¸ª")
            print(f"   ç™½åå•è§„åˆ™: {len(set(self.final_white_rules)):,} æ¡")
            print("ğŸ“ æ–‡ä»¶ä½ç½®: rules/outputs/")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    generator = StructuredAdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nâœ¨ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ”— æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
