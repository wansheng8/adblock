#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - ç®€åŒ–ç™½åå•ç‰ˆ
"""

import os
import re
import json
import time
import concurrent.futures
from datetime import datetime
from typing import Set, List, Optional
import requests

# é…ç½®ä¿¡æ¯
CONFIG = {
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    'MAX_WORKERS': 5,
    'TIMEOUT': 25,
    'RETRY': 2,
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # çœŸæ­£çš„ç™½åå•åŸŸåï¼ˆåªæ”¾è¡Œè¿™äº›ï¼‰
    'TRUE_WHITELIST_DOMAINS': {
        'google.com',
        'github.com',
        'microsoft.com',
        'apple.com',
        'baidu.com',
        'qq.com',
        'zhihu.com',
        'bilibili.com',
        'weibo.com',
        'taobao.com'
    }
}

class SimpleAdBlockGenerator:
    def __init__(self):
        self.black_domains = set()      # é»‘åå•åŸŸå
        self.final_blacklist = set()    # æœ€ç»ˆé»‘åå•
        
        # ç»Ÿè®¡
        self.stats = {
            'lines_processed': 0,
            'domains_found': 0,
            'whitelist_ignored': 0
        }
        
        # åˆ›å»ºç›®å½•
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºé»˜è®¤è§„åˆ™æº
        self.create_default_sources()
    
    def create_default_sources(self):
        """åˆ›å»ºé»˜è®¤è§„åˆ™æºæ–‡ä»¶"""
        # é»‘åå•æº - åªä½¿ç”¨2-3ä¸ªä¸»è¦æº
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æº\n")
                f.write("# åªä½¿ç”¨2-3ä¸ªä¸»è¦æº\n\n")
                f.write("# AdGuardå¹¿å‘Šè§„åˆ™ï¼ˆä¸»è¦æºï¼‰\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n\n")
                f.write("# EasyListè§„åˆ™ï¼ˆä¸»è¦æºï¼‰\n")
                f.write("https://easylist.to/easylist/easylist.txt\n\n")
                f.write("# ä¸­æ–‡è§„åˆ™ï¼ˆå¯é€‰ï¼‰\n")
                f.write("# https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ADGUARD_FILTER.txt\n")
        
        # ç™½åå•æº - åªæ”¾è¡ŒçœŸæ­£éœ€è¦çš„
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("# åªæ”¾è¡ŒçœŸæ­£éœ€è¦çš„åŸŸå\n\n")
                f.write("# é‡è¦ç½‘ç«™ä¸»åŸŸå\n")
                f.write("google.com\n")
                f.write("github.com\n")
                f.write("baidu.com\n")
                f.write("qq.com\n")
                f.write("zhihu.com\n")
    
    def download_content(self, url: str) -> Optional[str]:
        """ä¸‹è½½è§„åˆ™å†…å®¹"""
        for i in range(CONFIG['RETRY']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain, */*'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(2)
        return None
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        if domain in ['localhost', 'local', 'broadcasthost', '0.0.0.0', '127.0.0.1', '::1']:
            return False
        
        # æ’é™¤IPåœ°å€
        if re.match(r'^\d+\.\d+\.\d+\.\d+$', domain):
            return False
        
        # åŸºæœ¬åŸŸåæ ¼å¼
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        # æ£€æŸ¥æ¯éƒ¨åˆ†
        for part in parts:
            if not part or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def extract_domain_from_line(self, line: str) -> Optional[str]:
        """ä»è¡Œä¸­æå–åŸŸå"""
        line = line.strip()
        if not line:
            return None
        
        # è·³è¿‡æ³¨é‡Š
        if line.startswith('!') or line.startswith('#'):
            return None
        
        # å¦‚æœæ˜¯ç™½åå•è§„åˆ™ï¼ˆä»¥@@å¼€å¤´ï¼‰ï¼Œç›´æ¥è·³è¿‡
        if line.startswith('@@'):
            self.stats['whitelist_ignored'] += 1
            return None
        
        # ç®€å•çš„åŸŸåæå–
        patterns = [
            r'^\|\|([a-zA-Z0-9.-]+)\^',    # ||domain.com^
            r'^([a-zA-Z0-9.-]+)\^$',       # domain.com^
            r'^([a-zA-Z0-9.-]+)$',         # domain.com
            r'^\*\.([a-zA-Z0-9.-]+)',      # *.domain.com
            r'^\d+\.\d+\.\d+\.\d+\s+([a-zA-Z0-9.-]+)',  # 0.0.0.0 domain.com
        ]
        
        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(1).lower().strip()
                
                # ç§»é™¤wwwå‰ç¼€
                if domain.startswith('www.'):
                    domain = domain[4:]
                
                if self.is_valid_domain(domain):
                    return domain
        
        return None
    
    def process_blacklist_url(self, url: str):
        """å¤„ç†é»‘åå•URL"""
        print(f"  ğŸ“¥ å¤„ç†é»‘åå•: {url}")
        content = self.download_content(url)
        if not content:
            return
        
        domains_found = 0
        for line in content.split('\n'):
            self.stats['lines_processed'] += 1
            
            domain = self.extract_domain_from_line(line)
            if domain:
                # å¦‚æœè¿™ä¸ªåŸŸååœ¨æˆ‘ä»¬çš„ç™½åå•ä¸­ï¼Œè·³è¿‡
                if domain in CONFIG['TRUE_WHITELIST_DOMAINS']:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç™½åå•åŸŸåçš„å­åŸŸå
                is_whitelist_subdomain = False
                for white_domain in CONFIG['TRUE_WHITELIST_DOMAINS']:
                    if domain == white_domain or domain.endswith(f".{white_domain}"):
                        is_whitelist_subdomain = True
                        break
                
                if not is_whitelist_subdomain:
                    self.black_domains.add(domain)
                    domains_found += 1
        
        print(f"  âœ“ æ‰¾åˆ° {domains_found} ä¸ªåŸŸå")
    
    def load_whitelist(self):
        """åŠ è½½ç™½åå•"""
        print("âœ… åŠ è½½ç™½åå•...")
        
        whitelist_domains = set(CONFIG['TRUE_WHITELIST_DOMAINS'])
        
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        # å¦‚æœæ˜¯URLï¼Œä¸‹è½½å¹¶å¤„ç†
                        if line.startswith('http'):
                            print(f"  ğŸ“¥ ä¸‹è½½ç™½åå•æº: {line}")
                            content = self.download_content(line)
                            if content:
                                for content_line in content.split('\n'):
                                    domain = self.extract_domain_from_line(content_line)
                                    if domain:
                                        whitelist_domains.add(domain)
                        else:
                            # ç›´æ¥æ·»åŠ åŸŸå
                            domain = self.extract_domain_from_line(line)
                            if domain:
                                whitelist_domains.add(domain)
        
        print(f"  ç™½åå•åŸŸå: {len(whitelist_domains)} ä¸ª")
        if whitelist_domains:
            print("  ç™½åå•ç¤ºä¾‹:", list(whitelist_domains)[:5])
        
        return whitelist_domains
    
    def apply_whitelist(self, whitelist_domains: Set[str]):
        """åº”ç”¨ç™½åå•"""
        print("ğŸ”„ åº”ç”¨ç™½åå•...")
        
        original_count = len(self.black_domains)
        
        # ç§»é™¤å®Œå…¨åŒ¹é…çš„ç™½åå•åŸŸå
        domains_to_remove = set()
        for domain in self.black_domains:
            if domain in whitelist_domains:
                domains_to_remove.add(domain)
        
        self.final_blacklist = self.black_domains - domains_to_remove
        
        removed = original_count - len(self.final_blacklist)
        print(f"  ç§»é™¤ {removed} ä¸ªç™½åå•åŸŸå")
        print(f"  æœ€ç»ˆé»‘åå•: {len(self.final_blacklist):,} ä¸ªåŸŸå")
    
    def generate_files(self, whitelist_domains: Set[str]):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        version = datetime.now().strftime('%Y%m%d')
        
        # æ’åºåŸŸå
        sorted_blacklist = sorted(self.final_blacklist)
        
        # 1. AdBlockè§„åˆ™ (ad.txt)
        with open('rules/outputs/ad.txt', 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"! æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.final_blacklist):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(whitelist_domains)} ä¸ª\n")
            f.write("!\n\n")
            
            # é»‘åå•è§„åˆ™
            f.write("! ====== é»‘åå• ======\n")
            for domain in sorted_blacklist:
                f.write(f"||{domain}^\n")
        
        # 2. DNSè§„åˆ™ (dns.txt)
        with open('rules/outputs/dns.txt', 'w', encoding='utf-8') as f:
            f.write(f"# DNSå¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_blacklist):,} ä¸ª\n")
            f.write("#\n\n")
            
            for domain in sorted_blacklist:
                f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        with open('rules/outputs/hosts.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Hostså¹¿å‘Šè¿‡æ»¤è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.final_blacklist):,} ä¸ª\n")
            f.write("#\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n")
            f.write("#\n")
            f.write("# å¹¿å‘ŠåŸŸå\n\n")
            
            for i in range(0, len(sorted_blacklist), 1000):
                batch = sorted_blacklist[i:i+1000]
                for domain in batch:
                    f.write(f"0.0.0.0 {domain}\n")
                f.write("\n")
        
        # 4. çº¯é»‘åå• (black.txt)
        with open('rules/outputs/black.txt', 'w', encoding='utf-8') as f:
            for domain in sorted_blacklist:
                f.write(f"||{domain}^\n")
        
        # 5. çº¯ç™½åå• (white.txt)
        with open('rules/outputs/white.txt', 'w', encoding='utf-8') as f:
            f.write(f"# ç™½åå•è§„åˆ™ v{version}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {timestamp}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(whitelist_domains)} ä¸ª\n")
            f.write("#\n\n")
            
            for domain in sorted(whitelist_domains):
                f.write(f"@@||{domain}^\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': version,
            'updated_at': timestamp,
            'statistics': {
                'lines_processed': self.stats['lines_processed'],
                'final_blacklist_domains': len(self.final_blacklist),
                'whitelist_domains': len(whitelist_domains),
                'whitelist_ignored': self.stats['whitelist_ignored']
            }
        }
        
        with open('rules/outputs/info.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ æ–‡ä»¶ç”Ÿæˆå®Œæˆ:")
        print(f"   é»‘åå•åŸŸå: {len(self.final_blacklist):,} ä¸ª")
        print(f"   ç™½åå•åŸŸå: {len(whitelist_domains)} ä¸ª")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("ğŸš€ å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - ç®€åŒ–ç‰ˆ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½ç™½åå•ï¼ˆå…ˆåšï¼Œç”¨äºè¿‡æ»¤ï¼‰
            whitelist_domains = self.load_whitelist()
            
            # 2. å¤„ç†é»‘åå•æº
            print("\nğŸ” å¤„ç†é»‘åå•æº...")
            
            blacklist_urls = []
            if os.path.exists(CONFIG['BLACK_SOURCE']):
                with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            blacklist_urls.append(line)
            
            if not blacklist_urls:
                print("  âš ï¸ æœªæ‰¾åˆ°é»‘åå•æº")
                return False
            
            print(f"  æ‰¾åˆ° {len(blacklist_urls)} ä¸ªé»‘åå•æº")
            
            # å¹¶è¡Œå¤„ç†é»‘åå•URL
            with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
                futures = []
                for url in blacklist_urls:
                    future = executor.submit(self.process_blacklist_url, url)
                    futures.append(future)
                
                completed = 0
                for future in concurrent.futures.as_completed(futures):
                    try:
                        future.result(timeout=30)
                        completed += 1
                        print(f"  âœ… [{completed}/{len(blacklist_urls)}] å®Œæˆ")
                    except Exception as e:
                        print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            
            # 3. åº”ç”¨ç™½åå•
            self.apply_whitelist(whitelist_domains)
            
            # 4. ç”Ÿæˆæ–‡ä»¶
            self.generate_files(whitelist_domains)
            
            # 5. ç”ŸæˆREADME
            self.generate_readme()
            
            # 6. è¿è¡ŒéªŒè¯
            self.run_validation(whitelist_domains)
            
            elapsed = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ è§„åˆ™ç”Ÿæˆå®Œæˆï¼")
            print(f"â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.final_blacklist):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(whitelist_domains)}ä¸ª")
            print("ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_readme(self):
        """ç”ŸæˆREADME.md"""
        print("ğŸ“– ç”ŸæˆREADME.md...")
        
        with open('rules/outputs/info.json', 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ç®€æ´é«˜æ•ˆçš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™ï¼Œä¸“æ³¨äºæ‹¦æˆªå¹¿å‘ŠåŸŸåã€‚

---

## è®¢é˜…åœ°å€

| è§„åˆ™ç±»å‹ | è§„åˆ™è¯´æ˜ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|:---------|:---------|:---------|:---------|
| **AdBlockè§„åˆ™** | é€‚ç”¨äºæµè§ˆå™¨å¹¿å‘Šæ’ä»¶ | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| **DNSè¿‡æ»¤è§„åˆ™** | é€‚ç”¨äºDNSè¿‡æ»¤è½¯ä»¶ | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| **Hostsè§„åˆ™** | é€‚ç”¨äºç³»ç»Ÿhostsæ–‡ä»¶ | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| **é»‘åå•è§„åˆ™** | çº¯é»‘åå•åŸŸå | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| **ç™½åå•è§„åˆ™** | æ’é™¤è¯¯æ‹¦åŸŸå | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['statistics']['final_blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['statistics']['whitelist_domains']} ä¸ª

---

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at']}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
    
    def run_validation(self, whitelist_domains: Set[str]):
        """è¿è¡ŒéªŒè¯"""
        print("\nğŸ” è¿è¡ŒéªŒè¯...")
        
        # æ£€æŸ¥å…³é”®å¹¿å‘ŠåŸŸåæ˜¯å¦è¢«åŒ…å«
        critical_domains = [
            'doubleclick.net',
            'google-analytics.com',
            'googlesyndication.com',
            'googleadservices.com',
            'adservice.google.com'
        ]
        
        missing = []
        for domain in critical_domains:
            if domain not in self.final_blacklist:
                missing.append(domain)
        
        if missing:
            print(f"âš ï¸  è­¦å‘Š: ç¼ºå¤± {len(missing)} ä¸ªå…³é”®å¹¿å‘ŠåŸŸå")
            for domain in missing:
                print(f"   - {domain}")
        else:
            print("âœ… æ‰€æœ‰å…³é”®å¹¿å‘ŠåŸŸåå·²åŒ…å«")
        
        # æ£€æŸ¥ç™½åå•æ•°é‡
        white_count = len(whitelist_domains)
        if white_count > 100:
            print(f"âš ï¸  è­¦å‘Š: ç™½åå•è¿‡å¤š ({white_count} ä¸ª)")
        else:
            print(f"âœ… ç™½åå•æ•°é‡åˆç† ({white_count} ä¸ª)")

def main():
    """ä¸»å‡½æ•°"""
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    generator = SimpleAdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nâœ¨ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ”— æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
