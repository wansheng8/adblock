#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç²¾ç®€ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨
åªä½¿ç”¨ç”¨æˆ·é…ç½®çš„æºï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
"""

import os
import re
import json
import time
import socket
import ssl
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, List, Optional, Tuple
import requests
from urllib.parse import urlparse

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 10,
    'TIMEOUT': 60,  # å¢åŠ è¶…æ—¶æ—¶é—´
    'RETRY_TIMES': 5,
    
    # æ–‡ä»¶è·¯å¾„ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # è¾“å‡ºæ–‡ä»¶ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰
    'AD_FILE': 'rules/outputs/ad.txt',
    'DNS_FILE': 'rules/outputs/dns.txt',
    'HOSTS_FILE': 'rules/outputs/hosts.txt',
    'BLACK_FILE': 'rules/outputs/black.txt',
    'WHITE_FILE': 'rules/outputs/white.txt',
    'INFO_FILE': 'rules/outputs/info.json',
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AdBlockGenerator:
    """å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        
        # ä¸‹è½½ç»Ÿè®¡
        self.download_stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'failed_urls': []
        }
        
        # åˆ›å»ºç›®å½•
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶
        self.create_example_sources()
    
    def create_example_sources(self):
        """åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶ï¼ˆåªåˆ›å»ºæ–‡ä»¶ç»“æ„ï¼Œä¸é¢„è®¾å†…å®¹ï¼‰"""
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æº\n")
                f.write("# æ¯è¡Œä¸€ä¸ªURLï¼Œå¿…é¡»æ˜¯å¯å…¬å¼€è®¿é—®çš„è§„åˆ™åˆ—è¡¨\n")
                f.write("# ç¤ºä¾‹ï¼š\n")
                f.write("# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n")
                f.write("# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/tracking.txt\n\n")
                f.write("# è¯·åœ¨æ­¤å¤„æ·»åŠ æ‚¨çš„è§„åˆ™æºURLï¼š\n")
            
            logger.info(f"åˆ›å»ºç©ºç™½é»‘åå•æºæ–‡ä»¶: {CONFIG['BLACK_SOURCE']}")
            print(f"âš ï¸  è¯·ç¼–è¾‘ {CONFIG['BLACK_SOURCE']} æ·»åŠ æ‚¨çš„è§„åˆ™æºURL")
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("# æ¯è¡Œä¸€ä¸ªURLï¼Œå¿…é¡»æ˜¯å¯å…¬å¼€è®¿é—®çš„ç™½åå•è§„åˆ™åˆ—è¡¨\n")
                f.write("# ç¤ºä¾‹ï¼š\n")
                f.write("# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n")
                f.write("# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist_domains.txt\n\n")
                f.write("# è¯·åœ¨æ­¤å¤„æ·»åŠ æ‚¨çš„ç™½åå•æºURLï¼š\n")
            
            logger.info(f"åˆ›å»ºç©ºç™½ç™½åå•æºæ–‡ä»¶: {CONFIG['WHITE_SOURCE']}")
            print(f"âš ï¸  è¯·ç¼–è¾‘ {CONFIG['WHITE_SOURCE']} æ·»åŠ æ‚¨çš„ç™½åå•æºURL")
    
    def check_network(self):
        """æ£€æŸ¥ç½‘ç»œè¿æ¥"""
        print("ğŸ” æ£€æŸ¥ç½‘ç»œè¿æ¥...")
        
        test_urls = [
            "https://raw.githubusercontent.com",
            "https://github.com",
            "https://www.google.com"
        ]
        
        for url in test_urls:
            try:
                response = requests.head(url, timeout=10)
                if response.status_code < 400:
                    print(f"  âœ… å¯ä»¥è®¿é—® {url}")
                    return True
            except:
                print(f"  âŒ æ— æ³•è®¿é—® {url}")
        
        print("âŒ ç½‘ç»œè¿æ¥æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œ")
        return False
    
    def validate_url(self, url: str) -> Tuple[bool, str]:
        """éªŒè¯URLæ ¼å¼å’Œå¯è¾¾æ€§"""
        try:
            # æ£€æŸ¥URLæ ¼å¼
            result = urlparse(url)
            if not all([result.scheme, result.netloc]):
                return False, "URLæ ¼å¼ä¸æ­£ç¡®"
            
            # å¿…é¡»æ˜¯HTTPæˆ–HTTPS
            if result.scheme not in ['http', 'https']:
                return False, "åªæ”¯æŒHTTP/HTTPSåè®®"
            
            # æ£€æŸ¥æ˜¯å¦å¯è®¿é—®ï¼ˆå¿«é€ŸHEADè¯·æ±‚ï¼‰
            try:
                response = requests.head(url, timeout=10, allow_redirects=True)
                if response.status_code >= 400:
                    return False, f"æœåŠ¡å™¨è¿”å›é”™è¯¯: {response.status_code}"
                
                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower()
                if 'text/plain' not in content_type and 'text/html' not in content_type:
                    logger.warning(f"URL {url} å†…å®¹ç±»å‹ä¸æ˜¯æ–‡æœ¬: {content_type}")
                
                return True, "URLéªŒè¯é€šè¿‡"
                
            except requests.exceptions.RequestException as e:
                return False, f"æ— æ³•è®¿é—®URL: {str(e)}"
                
        except Exception as e:
            return False, f"URLè§£æé”™è¯¯: {str(e)}"
    
    def load_sources(self) -> bool:
        """åŠ è½½è§„åˆ™æºï¼ŒéªŒè¯URL"""
        print("ğŸ“‹ åŠ è½½è§„åˆ™æº...")
        
        # æ£€æŸ¥ç½‘ç»œ
        if not self.check_network():
            return False
        
        # æ£€æŸ¥æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            print(f"âŒ é»‘åå•æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['BLACK_SOURCE']}")
            return False
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            print(f"âŒ ç™½åå•æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['WHITE_SOURCE']}")
            return False
        
        # åŠ è½½é»‘åå•æº
        with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
            raw_lines = f.readlines()
            
        # æå–å’ŒéªŒè¯URL
        valid_urls = []
        for line_num, line in enumerate(raw_lines, 1):
            line = line.strip()
            if line and not line.startswith('#'):
                print(f"  éªŒè¯é»‘åå•æºç¬¬{line_num}è¡Œ: {line}")
                valid, message = self.validate_url(line)
                if valid:
                    valid_urls.append(line)
                    print(f"    âœ… {message}")
                else:
                    print(f"    âŒ {message}")
        
        self.black_urls = valid_urls
        
        # åŠ è½½ç™½åå•æº
        with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
            raw_lines = f.readlines()
            
        valid_urls = []
        for line_num, line in enumerate(raw_lines, 1):
            line = line.strip()
            if line and not line.startswith('#'):
                print(f"  éªŒè¯ç™½åå•æºç¬¬{line_num}è¡Œ: {line}")
                valid, message = self.validate_url(line)
                if valid:
                    valid_urls.append(line)
                    print(f"    âœ… {message}")
                else:
                    print(f"    âŒ {message}")
        
        self.white_urls = valid_urls
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„URL
        if not self.black_urls:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é»‘åå•æºURL")
            print("ğŸ’¡ è¯·ç¼–è¾‘ rules/sources/black.txt æ·»åŠ è§„åˆ™æº")
            return False
        
        if not self.white_urls:
            print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„ç™½åå•æºURLï¼ˆå¯ä»¥è·³è¿‡ï¼Œä½†æ¨èæ·»åŠ ï¼‰")
            print("ğŸ’¡ æ‚¨å¯ä»¥ç¼–è¾‘ rules/sources/white.txt æ·»åŠ ç™½åå•æº")
            # ç™½åå•æºå¯ä»¥ä¸ºç©ºï¼Œä¸è¿”å›False
        
        logger.info(f"åŠ è½½ {len(self.black_urls)} ä¸ªé»‘åå•æº")
        logger.info(f"åŠ è½½ {len(self.white_urls)} ä¸ªç™½åå•æº")
        
        return True
    
    def download_url_with_diagnosis(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        """ä¸‹è½½URLå†…å®¹ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯è¯Šæ–­"""
        for attempt in range(CONFIG['RETRY_TIMES']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain,text/html,application/xhtml+xml',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Cache-Control': 'max-age=0'
                }
                
                logger.debug(f"å°è¯•ä¸‹è½½ {url} (å°è¯• {attempt + 1}/{CONFIG['RETRY_TIMES']})")
                
                # è®¾ç½®æ›´è¯¦ç»†çš„è¶…æ—¶
                response = requests.get(
                    url, 
                    headers=headers, 
                    timeout=(15, 45),  # è¿æ¥è¶…æ—¶15ç§’ï¼Œè¯»å–è¶…æ—¶45ç§’
                    verify=True,
                    allow_redirects=True,
                    stream=False
                )
                
                response.raise_for_status()
                
                # æ£€æŸ¥å†…å®¹
                if not response.text:
                    raise ValueError("å“åº”å†…å®¹ä¸ºç©º")
                
                logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {url} (å¤§å°: {len(response.text):,} å­—èŠ‚)")
                return response.text, None
                
            except socket.timeout:
                error_msg = f"è¿æ¥è¶…æ—¶ (å°è¯• {attempt + 1}/{CONFIG['RETRY_TIMES']})"
                logger.warning(f"{error_msg}: {url}")
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(3)
                else:
                    return None, f"è¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–URLæ˜¯å¦æ­£ç¡®"
                    
            except requests.exceptions.SSLError as e:
                error_msg = f"SSLè¯ä¹¦é”™è¯¯: {str(e)}"
                logger.warning(f"{error_msg}: {url}")
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    return None, error_msg
                    
            except requests.exceptions.ConnectionError as e:
                error_msg = f"è¿æ¥é”™è¯¯: {str(e)}"
                logger.warning(f"{error_msg}: {url}")
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    return None, f"æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥URLæˆ–ç½‘ç»œ"
                    
            except requests.exceptions.HTTPError as e:
                error_msg = f"HTTPé”™è¯¯ {e.response.status_code if e.response else 'æœªçŸ¥'}: {str(e)}"
                logger.warning(f"{error_msg}: {url}")
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    return None, error_msg
                    
            except requests.exceptions.RequestException as e:
                error_msg = f"è¯·æ±‚é”™è¯¯: {str(e)}"
                logger.warning(f"{error_msg}: {url}")
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    return None, error_msg
                    
            except Exception as e:
                error_msg = f"æœªçŸ¥é”™è¯¯: {str(e)}"
                logger.error(f"{error_msg}: {url}")
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    return None, error_msg
        
        return None, "ä¸‹è½½å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"
    
    def download_all_urls(self) -> Tuple[bool, List[Tuple[str, str, str]]]:
        """ä¸‹è½½æ‰€æœ‰URLï¼Œè¿”å›(æ˜¯å¦æˆåŠŸ, [(url, content, error)])"""
        print("ğŸ“¥ ä¸‹è½½è§„åˆ™æº...")
        
        all_urls = [(url, 'black') for url in self.black_urls] + \
                   [(url, 'white') for url in self.white_urls]
        
        self.download_stats['total'] = len(all_urls)
        results = []
        failed_urls = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            future_to_url = {}
            for url, url_type in all_urls:
                future = executor.submit(self.download_url_with_diagnosis, url)
                future_to_url[future] = (url, url_type)
            
            # å¤„ç†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_url):
                url, url_type = future_to_url[future]
                try:
                    content, error = future.result()
                    if content:
                        results.append((url, url_type, content, None))
                        self.download_stats['success'] += 1
                        print(f"  âœ… ä¸‹è½½æˆåŠŸ: {url}")
                    else:
                        results.append((url, url_type, None, error))
                        self.download_stats['failed'] += 1
                        self.download_stats['failed_urls'].append(url)
                        failed_urls.append((url, error))
                        print(f"  âŒ ä¸‹è½½å¤±è´¥: {url}")
                        print(f"     é”™è¯¯: {error}")
                except Exception as e:
                    error_msg = f"ä»»åŠ¡æ‰§è¡Œé”™è¯¯: {str(e)}"
                    results.append((url, url_type, None, error_msg))
                    self.download_stats['failed'] += 1
                    self.download_stats['failed_urls'].append(url)
                    failed_urls.append((url, error_msg))
                    print(f"  âŒ ä¸‹è½½å¤±è´¥: {url}")
                    print(f"     é”™è¯¯: {error_msg}")
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦çš„æºéƒ½å¤±è´¥äº†
        if self.download_stats['success'] == 0:
            print("\nâŒ æ‰€æœ‰è§„åˆ™æºä¸‹è½½éƒ½å¤±è´¥äº†ï¼")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
            print("   1. ç½‘ç»œè¿æ¥é—®é¢˜")
            print("   2. URLåœ°å€ä¸æ­£ç¡®")
            print("   3. æºç½‘ç«™æš‚æ—¶ä¸å¯ç”¨")
            print("   4. éœ€è¦ç§‘å­¦ä¸Šç½‘ï¼ˆæŸäº›æºå¯èƒ½éœ€è¦ï¼‰")
            print("\nğŸ”§ è§£å†³æ–¹æ¡ˆï¼š")
            print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print("   2. éªŒè¯URLæ˜¯å¦æ­£ç¡®ï¼ˆå¤åˆ¶åˆ°æµè§ˆå™¨ä¸­æµ‹è¯•ï¼‰")
            print("   3. ç¼–è¾‘ rules/sources/ ä¸­çš„æ–‡ä»¶ï¼Œæ›´æ¢å…¶ä»–æº")
            return False, results
        
        # æ£€æŸ¥é»‘åå•æºæ˜¯å¦å…¨éƒ¨å¤±è´¥
        black_success = any(1 for url, url_type, content, error in results 
                          if url_type == 'black' and content)
        
        if not black_success:
            print("\nâŒ æ‰€æœ‰é»‘åå•æºéƒ½ä¸‹è½½å¤±è´¥äº†ï¼")
            print("ğŸ’¡ è¯·æ£€æŸ¥ rules/sources/black.txt ä¸­çš„URL")
            return False, results
        
        # å¦‚æœæœ‰å¤±è´¥çš„URLï¼Œä½†ä¸æ˜¯å…¨éƒ¨å¤±è´¥ï¼Œç»§ç»­å¤„ç†
        if failed_urls:
            print(f"\nâš ï¸  éƒ¨åˆ†æºä¸‹è½½å¤±è´¥ ({len(failed_urls)}/{len(all_urls)})")
            print("å°†ä½¿ç”¨æˆåŠŸä¸‹è½½çš„æºç»§ç»­å¤„ç†")
        
        return True, results
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæœ‰æ•ˆæ€§"""
        if not domain:
            return False
        
        domain = domain.strip().lower()
        
        # æ’é™¤åˆ—è¡¨
        exclude = ['localhost', 'local', '127.0.0.1', '0.0.0.0', '::1', 
                  'broadcasthost', 'ip6-localhost', 'ip6-loopback']
        if domain in exclude:
            return False
        
        # é•¿åº¦æ£€æŸ¥
        if len(domain) < 3 or len(domain) > 253:
            return False
        
        # å¿…é¡»æœ‰ç‚¹
        if '.' not in domain:
            return False
        
        # æ£€æŸ¥æ ¼å¼
        if not re.match(r'^[a-z0-9]([a-z0-9\-\.]*[a-z0-9])?$', domain):
            return False
        
        # ä¸èƒ½æœ‰ä¸¤ä¸ªè¿ç»­çš„ç‚¹æˆ–ç ´æŠ˜å·
        if '..' in domain or '--' in domain:
            return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        for part in parts:
            # æ¯éƒ¨åˆ†é•¿åº¦
            if len(part) < 1 or len(part) > 63:
                return False
            
            # å¼€å§‹å’Œç»“æŸå­—ç¬¦
            if part.startswith('-') or part.endswith('-'):
                return False
        
        return True
    
    def extract_domain_and_type(self, line: str) -> Tuple[Optional[str], bool]:
        """æå–åŸŸåå’Œè§„åˆ™ç±»å‹ï¼ˆæ˜¯å¦ç™½åå•ï¼‰"""
        if not line:
            return None, False
        
        line = line.strip()
        
        # è·³è¿‡æ³¨é‡Š
        if line.startswith('!') or line.startswith('#'):
            return None, False
        
        is_whitelist = line.startswith('@@')
        
        # æ¸…ç†è§„åˆ™
        if is_whitelist:
            line = line[2:]  # ç§»é™¤@@
        
        # å¸¸è§æ ¼å¼åŒ¹é…
        patterns = [
            # ||domain.com^ æ ¼å¼
            (r'^\|\|([^\^\$\*\/:]+)', 1),
            (r'^\|\|([^\^]+)\^', 1),
            
            # domain.com^ æ ¼å¼
            (r'^([a-zA-Z0-9.-]+)\^', 1),
            
            # çº¯åŸŸå
            (r'^([a-zA-Z0-9]([a-zA-Z0-9\-]*[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$', 0),
            
            # Hostsæ ¼å¼: 0.0.0.0 domain.com
            (r'^\d+\.\d+\.\d+\.\d+\s+([^\s#]+)', 1),
            
            # é€šé…ç¬¦: *.domain.com
            (r'^\*\.([a-zA-Z0-9.-]+)', 1),
        ]
        
        for pattern, group in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(group if group > 0 else 0).lower()
                
                # æ¸…ç†åŸŸå
                domain = re.sub(r'^www\d*\.', '', domain)  # ç§»é™¤wwwå‰ç¼€
                domain = re.sub(r'^\.+|\.+$', '', domain)  # ç§»é™¤å¼€å¤´ç»“å°¾çš„ç‚¹
                domain = re.sub(r'\s+', '', domain)        # ç§»é™¤ç©ºæ ¼
                
                if self.is_valid_domain(domain):
                    return domain, is_whitelist
        
        return None, False
    
    def parse_content(self, content: str, source_url: str, source_type: str):
        """è§£æè§„åˆ™å†…å®¹"""
        lines = content.split('\n')
        black_domains_from_source = set()
        white_domains_from_source = set()
        
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line:
                continue
            
            domain, is_whitelist = self.extract_domain_and_type(line)
            
            if domain:
                if is_whitelist:
                    white_domains_from_source.add(domain)
                    
                    # å¦‚æœæ˜¯é»‘åå•æºä¸­çš„ç™½åå•ï¼Œè®°å½•ä¸‹æ¥
                    if source_type == 'black':
                        logger.debug(f"é»‘åå•æº {source_url} ç¬¬{line_num}è¡Œå‘ç°ç™½åå•: {domain}")
                else:
                    black_domains_from_source.add(domain)
        
        logger.info(f"è§£æ {source_url}: å‘ç° {len(black_domains_from_source)} é»‘åå•åŸŸå, {len(white_domains_from_source)} ç™½åå•åŸŸå")
        
        return black_domains_from_source, white_domains_from_source
    
    def process_downloaded_content(self, results: List[Tuple[str, str, str, str]]):
        """å¤„ç†ä¸‹è½½çš„å†…å®¹"""
        print("ğŸ” è§£æè§„åˆ™å†…å®¹...")
        
        all_black_domains = set()
        all_white_domains = set()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰é»‘åå•æºä¸­çš„åŸŸåï¼ˆåŒ…æ‹¬å…¶ä¸­çš„ç™½åå•ï¼‰
        for url, url_type, content, error in results:
            if content and url_type == 'black':
                black_domains, white_domains = self.parse_content(content, url, 'black')
                all_black_domains.update(black_domains)
                
                # è®°å½•ä»é»‘åå•æºä¸­æ‰¾åˆ°çš„ç™½åå•
                if white_domains:
                    logger.info(f"ä»é»‘åå•æº {url} ä¸­å‘ç° {len(white_domains)} ä¸ªç™½åå•åŸŸå")
                    # æš‚æ—¶ä¿å­˜ï¼Œåç»­å¤„ç†
                    all_white_domains.update(white_domains)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¤„ç†ç™½åå•æºï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        for url, url_type, content, error in results:
            if content and url_type == 'white':
                black_domains, white_domains = self.parse_content(content, url, 'white')
                # ç™½åå•æºä¸­çš„ç™½åå•ä¼˜å…ˆçº§æœ€é«˜
                all_white_domains.update(white_domains)
                
                # ç™½åå•æºä¸­çš„é»‘åå•é€šå¸¸åº”è¯¥å¿½ç•¥ï¼Œä½†å…ˆè®°å½•ä¸‹æ¥
                if black_domains:
                    logger.warning(f"ç™½åå•æº {url} ä¸­åŒ…å« {len(black_domains)} ä¸ªé»‘åå•åŸŸåï¼Œå°†å¿½ç•¥")
        
        logger.info(f"æ”¶é›†å®Œæˆ: æ€»å…±å‘ç° {len(all_black_domains)} ä¸ªé»‘åå•åŸŸåï¼Œ{len(all_white_domains)} ä¸ªç™½åå•åŸŸå")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šåº”ç”¨ç™½åå•ï¼ˆç§»é™¤é»‘åå•ä¸­çš„ç™½åå•åŸŸåï¼‰
        print("ğŸ”„ åº”ç”¨ç™½åå•è¿‡æ»¤...")
        
        original_count = len(all_black_domains)
        
        # 1. ç›´æ¥ç§»é™¤å®Œå…¨åŒ¹é…çš„ç™½åå•
        domains_to_remove = all_black_domains.intersection(all_white_domains)
        all_black_domains -= domains_to_remove
        
        removed_direct = len(domains_to_remove)
        logger.info(f"ç›´æ¥åŒ¹é…ç§»é™¤ {removed_direct} ä¸ªåŸŸå")
        
        # 2. ç§»é™¤å­åŸŸååŒ¹é…çš„
        # ä¼˜åŒ–ï¼šæŒ‰åŸŸåé•¿åº¦æ’åºï¼Œé•¿çš„ä¼˜å…ˆåŒ¹é…
        white_domains_sorted = sorted(all_white_domains, key=len, reverse=True)
        more_to_remove = set()
        
        for black_domain in all_black_domains:
            for white_domain in white_domains_sorted:
                if black_domain.endswith(f".{white_domain}"):
                    more_to_remove.add(black_domain)
                    break
        
        all_black_domains -= more_to_remove
        
        removed_total = original_count - len(all_black_domains)
        logger.info(f"å­åŸŸååŒ¹é…ç§»é™¤ {len(more_to_remove)} ä¸ªåŸŸå")
        logger.info(f"æ€»å…±ç§»é™¤ {removed_total} ä¸ªåŸŸåï¼Œå‰©ä½™ {len(all_black_domains)} ä¸ªé»‘åå•åŸŸå")
        
        self.black_domains = all_black_domains
        self.white_domains = all_white_domains
        
        # ç”Ÿæˆè§„åˆ™é›†
        for domain in self.black_domains:
            self.black_rules.add(f"||{domain}^")
        
        for domain in self.white_domains:
            self.white_rules.add(f"@@||{domain}^")
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„åŸŸå
        if len(self.black_domains) == 0:
            print("âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„é»‘åå•åŸŸå")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
            print("   1. è§„åˆ™æºå†…å®¹ä¸ºç©º")
            print("   2. æ‰€æœ‰åŸŸåéƒ½è¢«ç™½åå•è¿‡æ»¤äº†")
            print("   3. è§„åˆ™æºæ ¼å¼ä¸æ­£ç¡®")
        
        # 1. Adblockè§„åˆ™ (ad.txt)
        with open(CONFIG['AD_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
            f.write(f"! ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d_%H%M')}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(self.white_domains):,} ä¸ª\n")
            f.write(f"! ä¸‹è½½ç»Ÿè®¡: {self.download_stats['success']}/{self.download_stats['total']} æˆåŠŸ\n")
            f.write(f"! é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n\n")
            
            # ç™½åå•è§„åˆ™
            if self.white_rules:
                f.write("! ========== ç™½åå•è§„åˆ™ ==========\n")
                for rule in sorted(self.white_rules):
                    f.write(f"{rule}\n")
                f.write("\n")
            
            # é»‘åå•è§„åˆ™
            f.write("! ========== é»‘åå•è§„åˆ™ ==========\n")
            if self.black_domains:
                for domain in sorted(self.black_domains):
                    f.write(f"||{domain}^\n")
            else:
                f.write("! æš‚æ— é»‘åå•åŸŸå\n")
        
        # 2. DNSè§„åˆ™ (dns.txt)
        with open(CONFIG['DNS_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"# DNSè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n")
            f.write(f"# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n\n")
            
            if self.black_domains:
                for domain in sorted(self.black_domains):
                    f.write(f"{domain}\n")
            else:
                f.write("# æš‚æ— åŸŸå\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        with open(CONFIG['HOSTS_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n")
            f.write(f"# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}\n\n")
            f.write("# æœ¬åœ°ä¸»æœº\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n\n")
            f.write("# å¹¿å‘ŠåŸŸåå±è”½\n")
            
            if self.black_domains:
                for domain in sorted(self.black_domains):
                    f.write(f"0.0.0.0 {domain}\n")
            else:
                f.write("# æš‚æ— åŸŸå\n")
        
        # 4. é»‘åå•è§„åˆ™ (black.txt)
        with open(CONFIG['BLACK_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"! é»‘åå•è§„åˆ™\n")
            f.write(f"! ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n")
            f.write(f"! åŸŸåæ•°é‡: {len(self.black_domains):,}\n\n")
            
            if self.black_domains:
                for domain in sorted(self.black_domains):
                    f.write(f"||{domain}^\n")
            else:
                f.write("! æš‚æ— åŸŸå\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        with open(CONFIG['WHITE_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"! ç™½åå•è§„åˆ™\n")
            f.write(f"! ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n")
            f.write(f"! åŸŸåæ•°é‡: {len(self.white_domains):,}\n\n")
            
            if self.white_domains:
                for domain in sorted(self.white_domains):
                    f.write(f"@@||{domain}^\n")
            else:
                f.write("! æš‚æ— åŸŸå\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': datetime.now().strftime('%Y%m%d_%H%M'),
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains)
            },
            'download_stats': self.download_stats,
            'sources': {
                'blacklist': len(self.black_urls),
                'whitelist': len(self.white_urls)
            }
        }
        
        with open(CONFIG['INFO_FILE'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print("âœ… è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.mdï¼ˆåªåŒ…å«3ä¸ªéƒ¨åˆ†ï¼‰"""
        print("ğŸ“– ç”ŸæˆREADME.md...")
        
        # è¯»å–è§„åˆ™ä¿¡æ¯
        try:
            with open(CONFIG['INFO_FILE'], 'r', encoding='utf-8') as f:
                info = json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–è§„åˆ™ä¿¡æ¯å¤±è´¥: {e}")
            info = {
                'version': datetime.now().strftime('%Y%m%d'),
                'updated_at': datetime.now().isoformat(),
                'rules': {'blacklist_domains': 0, 'whitelist_domains': 0}
            }
        
        # ç”Ÿæˆé“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        # åªåŒ…å«3ä¸ªéƒ¨åˆ†çš„README
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} æ›´æ–°å†…å®¹ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- ä¸‹è½½æˆåŠŸç‡ï¼š{info.get('download_stats', {}).get('success', 0)}/{info.get('download_stats', {}).get('total', 0)}

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print("âœ… README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("åªä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰çš„è§„åˆ™æº")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½å¹¶éªŒè¯è§„åˆ™æº
            print("\næ­¥éª¤ 1/5: åŠ è½½å’ŒéªŒè¯è§„åˆ™æº")
            if not self.load_sources():
                print("\nâŒ è§„åˆ™æºåŠ è½½å¤±è´¥")
                print("ğŸ’¡ è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
                print("   1. æ£€æŸ¥ rules/sources/black.txt å’Œ white.txt æ–‡ä»¶")
                print("   2. ç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼ˆä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰")
                print("   3. éªŒè¯URLæ˜¯å¦å¯å…¬å¼€è®¿é—®")
                print("   4. é‡æ–°è¿è¡Œç¨‹åº")
                return False
            
            # 2. ä¸‹è½½æ‰€æœ‰è§„åˆ™æº
            print(f"\næ­¥éª¤ 2/5: ä¸‹è½½è§„åˆ™æº ({self.download_stats['total']}ä¸ª)")
            success, results = self.download_all_urls()
            if not success:
                print("\nâŒ è§„åˆ™æºä¸‹è½½å¤±è´¥")
                if self.download_stats['failed_urls']:
                    print("å¤±è´¥çš„URLï¼š")
                    for url in self.download_stats['failed_urls']:
                        print(f"  - {url}")
                print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
                print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                print("   2. å°†å¤±è´¥çš„URLå¤åˆ¶åˆ°æµè§ˆå™¨ä¸­æµ‹è¯•")
                print("   3. å¦‚æœURLéœ€è¦ç§‘å­¦ä¸Šç½‘ï¼Œè¯·é…ç½®ä»£ç†æˆ–æ›´æ¢å…¶ä»–æº")
                print("   4. ç¼–è¾‘ rules/sources/ ä¸­çš„æ–‡ä»¶ï¼Œæ›´æ¢å¯ç”¨çš„æº")
                return False
            
            # 3. è§£æå’Œå¤„ç†è§„åˆ™
            print("\næ­¥éª¤ 3/5: è§£æå’Œå¤„ç†è§„åˆ™")
            self.process_downloaded_content(results)
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶
            print("\næ­¥éª¤ 4/5: ç”Ÿæˆè§„åˆ™æ–‡ä»¶")
            self.generate_files()
            
            # 5. ç”ŸæˆREADME
            print("\næ­¥éª¤ 5/5: ç”ŸæˆREADME.md")
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print("=" * 60)
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ“ˆ ä¸‹è½½æˆåŠŸç‡: {self.download_stats['success']}/{self.download_stats['total']}")
            print("=" * 60)
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("ğŸ”— è®¢é˜…åœ°å€å·²åœ¨README.mdä¸­æ›´æ–°")
            print("=" * 60)
            
            # æ˜¾ç¤ºä¸‹è½½å¤±è´¥çš„URLï¼ˆå¦‚æœæœ‰ï¼‰
            if self.download_stats['failed'] > 0:
                print("\nâš ï¸  ä»¥ä¸‹URLä¸‹è½½å¤±è´¥ï¼š")
                for url in self.download_stats['failed_urls']:
                    print(f"  - {url}")
                print("ğŸ’¡ è¯·æ£€æŸ¥è¿™äº›URLæ˜¯å¦æ­£ç¡®æˆ–å¯è®¿é—®")
            
            return True
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            return False
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            print("\nğŸ’¡ å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š")
            print("   1. ç½‘ç»œè¿æ¥é—®é¢˜ - æ£€æŸ¥ç½‘ç»œ")
            print("   2. è§„åˆ™æºæ ¼å¼é—®é¢˜ - æ£€æŸ¥ rules/sources/ ä¸­çš„URL")
            print("   3. ç£ç›˜ç©ºé—´ä¸è¶³ - æ£€æŸ¥ç£ç›˜ç©ºé—´")
            print("   4. å†…å­˜ä¸è¶³ - å‡å°‘å¹¶å‘æ•°ï¼ˆä¿®æ”¹run.pyä¸­çš„MAX_WORKERSï¼‰")
            
            return False

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help' or sys.argv[1] == '-h':
            print("ä½¿ç”¨æ–¹æ³•:")
            print("  python run.py              # æ­£å¸¸è¿è¡Œ")
            print("  python run.py --test <URL> # æµ‹è¯•URL")
            print("  python run.py --config     # æ˜¾ç¤ºå½“å‰é…ç½®")
            print("  python run.py --list       # åˆ—å‡ºå½“å‰é…ç½®çš„æº")
            return
        
        elif sys.argv[1] == '--test' and len(sys.argv) > 2:
            url = sys.argv[2]
            print(f"ğŸ” æµ‹è¯•URL: {url}")
            generator = AdBlockGenerator()
            content, error = generator.download_url_with_diagnosis(url)
            if content:
                print(f"âœ… æµ‹è¯•æˆåŠŸ")
                print(f"   å†…å®¹é•¿åº¦: {len(content):,} å­—èŠ‚")
                print(f"   å‰200å­—ç¬¦: {content[:200]}...")
                
                # å°è¯•è§£æå†…å®¹
                black, white = generator.parse_content(content, url, 'test')
                print(f"   è§£æç»“æœ: {len(black)} é»‘åå•åŸŸå, {len(white)} ç™½åå•åŸŸå")
                if black:
                    print(f"   ç¤ºä¾‹åŸŸå: {list(black)[:5]}")
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {error}")
            return
        
        elif sys.argv[1] == '--config':
            print("å½“å‰é…ç½®:")
            print(f"  GitHubç”¨æˆ·: {CONFIG['GITHUB_USER']}")
            print(f"  ä»“åº“: {CONFIG['GITHUB_REPO']}")
            print(f"  åˆ†æ”¯: {CONFIG['GITHUB_BRANCH']}")
            print(f"  è¶…æ—¶æ—¶é—´: {CONFIG['TIMEOUT']}ç§’")
            print(f"  æœ€å¤§é‡è¯•: {CONFIG['RETRY_TIMES']}æ¬¡")
            print(f"  å¹¶å‘æ•°: {CONFIG['MAX_WORKERS']}")
            return
        
        elif sys.argv[1] == '--list':
            generator = AdBlockGenerator()
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if os.path.exists(CONFIG['BLACK_SOURCE']):
                print("é»‘åå•æº:")
                with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f, 1):
                        line = line.strip()
                        if line and not line.startswith('#'):
                            print(f"  [{i}] {line}")
            else:
                print(f"é»‘åå•æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['BLACK_SOURCE']}")
            
            print()
            
            if os.path.exists(CONFIG['WHITE_SOURCE']):
                print("ç™½åå•æº:")
                with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f, 1):
                        line = line.strip()
                        if line and not line.startswith('#'):
                            print(f"  [{i}] {line}")
            else:
                print(f"ç™½åå•æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['WHITE_SOURCE']}")
            
            return
    
    # æ­£å¸¸è¿è¡Œ
    generator = AdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")
        print("ğŸ’¡ è¯·æŒ‰ç…§ä¸Šé¢çš„æç¤ºæ£€æŸ¥å’Œä¿®å¤é—®é¢˜")

if __name__ == "__main__":
    main()
