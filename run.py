#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ
"""

import os
import re
import json
import time
import hashlib
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, Dict, List, Optional, Tuple
import requests
import urllib.parse
from collections import defaultdict
import sys
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 3,  # å‡å°‘å¹¶å‘ï¼Œé¿å…è¢«é™é€Ÿ
    'TIMEOUT': 60,     # å¢åŠ è¶…æ—¶æ—¶é—´
    'RETRY_TIMES': 3,
    
    # è§„åˆ™æºæ–‡ä»¶
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    'CHINA_SOURCE': 'rules/sources/china.txt',
    'ENHANCED_SOURCE': 'rules/sources/enhanced.txt',
    
    # è¾“å‡ºæ–‡ä»¶
    'OUTPUT_FILES': {
        'ad': 'rules/outputs/ad.txt',
        'dns': 'rules/outputs/dns.txt',
        'hosts': 'rules/outputs/hosts.txt',
        'black': 'rules/outputs/black.txt',
        'white': 'rules/outputs/white.txt',
        'info': 'rules/outputs/info.json',
        'smart_ad': 'rules/outputs/smart_ad.txt',
        'mobile_ad': 'rules/outputs/mobile_ad.txt',
    },
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    'PERFORMANCE': {
        'max_total_domains': 300000,  # æœ€å¤§åŸŸåæ€»æ•°
        'skip_some_sources': True,    # è·³è¿‡éƒ¨åˆ†å¤§æ–‡ä»¶æº
        'batch_size': 10000,          # æ‰¹é‡å¤„ç†å¤§å°
        'enable_progress_bar': True,  # å¯ç”¨è¿›åº¦æ¡
        'use_bloom_filter': False,    # ä½¿ç”¨å¸ƒéš†è¿‡æ»¤å™¨å»é‡ï¼ˆéœ€è¦å®‰è£…pybloom-liveï¼‰
    },
    
    # æ’é™¤çš„åŸŸå
    'EXCLUDE_DOMAINS': [
        'localhost', 'local', 'broadcasthost',
        '127.0.0.1', '0.0.0.0', '::1'
    ],
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.stages = {}
        self.current_stage = None
    
    def start_stage(self, name: str):
        """å¼€å§‹ä¸€ä¸ªé˜¶æ®µ"""
        self.current_stage = name
        self.stages[name] = {'start': time.time(), 'items_processed': 0}
        logger.info(f"å¼€å§‹é˜¶æ®µ: {name}")
    
    def update_progress(self, items: int = 1):
        """æ›´æ–°è¿›åº¦"""
        if self.current_stage and self.current_stage in self.stages:
            self.stages[self.current_stage]['items_processed'] += items
    
    def end_stage(self):
        """ç»“æŸå½“å‰é˜¶æ®µ"""
        if self.current_stage and self.current_stage in self.stages:
            end_time = time.time()
            stage_info = self.stages[self.current_stage]
            elapsed = end_time - stage_info['start']
            items = stage_info['items_processed']
            logger.info(f"å®Œæˆé˜¶æ®µ {self.current_stage}: å¤„ç† {items} ä¸ªé¡¹ç›®ï¼Œè€—æ—¶ {elapsed:.2f}ç§’")
            self.current_stage = None

class OptimizedDomainFilter:
    """ä¼˜åŒ–ç‰ˆåŸŸåè¿‡æ»¤å™¨"""
    
    @staticmethod
    def optimize_domains(domains: Set[str]) -> Set[str]:
        """ä¼˜åŒ–åŸŸåé›†åˆï¼Œç§»é™¤é‡å¤å’Œä½è´¨é‡åŸŸå"""
        logger.info(f"å¼€å§‹ä¼˜åŒ–åŸŸåé›†åˆ: {len(domains):,} ä¸ª")
        
        # 1. å»é‡
        unique_domains = set(domains)
        logger.info(f"å»é‡å: {len(unique_domains):,} ä¸ª")
        
        # 2. ç§»é™¤æ— æ•ˆåŸŸå
        valid_domains = set()
        for domain in unique_domains:
            if OptimizedDomainFilter.is_valid_domain(domain):
                valid_domains.add(domain)
        
        logger.info(f"æœ‰æ•ˆåŸŸå: {len(valid_domains):,} ä¸ª")
        
        # 3. æŒ‰åŸŸåè´¨é‡æ’åºå¹¶æˆªå–
        if len(valid_domains) > CONFIG['PERFORMANCE']['max_total_domains']:
            logger.info(f"åŸŸåè¿‡å¤šï¼Œæˆªå–å‰ {CONFIG['PERFORMANCE']['max_total_domains']:,} ä¸ª")
            # æŒ‰åŸŸåé•¿åº¦å’Œè´¨é‡æ’åº
            sorted_domains = sorted(valid_domains, 
                                   key=lambda x: (len(x.split('.')), -len(x)))
            valid_domains = set(sorted_domains[:CONFIG['PERFORMANCE']['max_total_domains']])
        
        return valid_domains
    
    @staticmethod
    def is_valid_domain(domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæœ‰æ•ˆæ€§"""
        if not domain or domain in CONFIG['EXCLUDE_DOMAINS']:
            return False
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(domain) < 3 or len(domain) > 253:
            return False
        
        # å¿…é¡»åŒ…å«ç‚¹å·
        if '.' not in domain:
            return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        parts = domain.split('.')
        for part in parts:
            if not part:  # ä¸èƒ½æœ‰ç©ºçš„æ®µ
                return False
            if len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        # é¡¶çº§åŸŸåè‡³å°‘2ä¸ªå­—ç¬¦
        if len(parts[-1]) < 2:
            return False
        
        return True

class OptimizedAdBlockGenerator:
    """ä¼˜åŒ–ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        self.progress = ProgressTracker()
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç²¾ç®€çš„æºæ–‡ä»¶
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æºï¼ˆç²¾ç®€ç‰ˆï¼‰\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n")
                f.write("https://easylist.to/easylist/easylist.txt\n")
                f.write("https://easylist.to/easylist/easyprivacy.txt\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/filters.txt\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/ChineseFilter/master/ChineseFilter.txt\n")
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n")
    
    def load_sources(self):
        """åŠ è½½è§„åˆ™æºURL"""
        self.progress.start_stage("åŠ è½½è§„åˆ™æº")
        
        # é»‘åå•æº
        with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    # è·³è¿‡å¯èƒ½çš„å¤§æ–‡ä»¶æºä»¥æé«˜æ€§èƒ½
                    if CONFIG['PERFORMANCE']['skip_some_sources']:
                        if any(skip in line for skip in [
                            'blocklistproject',
                            'hblock',
                            'big.oisd.nl'
                        ]):
                            logger.info(f"è·³è¿‡å¯èƒ½çš„å¤§æ–‡ä»¶æº: {line}")
                            continue
                    self.black_urls.append(line)
        
        # ç™½åå•æº
        with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.white_urls.append(line)
        
        self.progress.end_stage()
        logger.info(f"åŠ è½½ {len(self.black_urls)} ä¸ªé»‘åå•æº")
        logger.info(f"åŠ è½½ {len(self.white_urls)} ä¸ªç™½åå•æº")
    
    def download_url(self, url: str) -> Optional[str]:
        """ä¸‹è½½URLå†…å®¹"""
        for attempt in range(CONFIG['RETRY_TIMES']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/plain'
                }
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                
                # æ£€æŸ¥å†…å®¹å¤§å°
                content_length = len(response.content)
                if content_length > 10 * 1024 * 1024:  # 10MB
                    logger.warning(f"å†…å®¹è¿‡å¤§ ({content_length/1024/1024:.1f}MB): {url}")
                
                return response.text
            except Exception as e:
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(3)
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                    return None
    
    def extract_domain_simple(self, line: str) -> Optional[str]:
        """ç®€å•é«˜æ•ˆçš„åŸŸåæå–"""
        line = line.strip()
        
        # å¿«é€Ÿè·³è¿‡
        if not line or len(line) < 3:
            return None
        
        # è·³è¿‡æ³¨é‡Š
        if line[0] in '!#/':
            return None
        
        # å¸¸è§æ¨¡å¼åŒ¹é…
        patterns = [
            (r'^\|\|([a-zA-Z0-9.-]+)\^', 1),  # ||domain.com^
            (r'^@@\|\|([a-zA-Z0-9.-]+)\^', 1), # @@||domain.com^
            (r'^([a-zA-Z0-9.-]+)$', 1),       # domain.com
            (r'^0\.0\.0\.0\s+([a-zA-Z0-9.-]+)', 1), # 0.0.0.0 domain.com
            (r'^127\.0\.0\.1\s+([a-zA-Z0-9.-]+)', 1), # 127.0.0.1 domain.com
            (r'^\*\.([a-zA-Z0-9.-]+)', 1),    # *.domain.com
        ]
        
        for pattern, group in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(group).lower().strip()
                # ç®€å•æ¸…ç†
                domain = re.sub(r'^www\.', '', domain)
                domain = re.sub(r'^m\.', '', domain)
                domain = re.sub(r'^static\.', '', domain)
                
                # å¿«é€ŸéªŒè¯
                if (domain and '.' in domain and 
                    len(domain) >= 4 and len(domain) <= 253 and
                    domain not in CONFIG['EXCLUDE_DOMAINS']):
                    return domain
        
        return None
    
    def parse_content_fast(self, content: str) -> tuple:
        """å¿«é€Ÿè§£æè§„åˆ™å†…å®¹"""
        black_domains = set()
        white_domains = set()
        
        lines = content.split('\n')
        batch_size = CONFIG['PERFORMANCE']['batch_size']
        
        for i in range(0, len(lines), batch_size):
            batch = lines[i:i+batch_size]
            for line in batch:
                domain = self.extract_domain_simple(line)
                if domain:
                    if line.startswith('@@'):
                        white_domains.add(domain)
                    else:
                        black_domains.add(domain)
            
            self.progress.update_progress(len(batch))
        
        return black_domains, white_domains
    
    def download_and_parse_all(self):
        """ä¸‹è½½å¹¶è§£ææ‰€æœ‰è§„åˆ™"""
        logger.info("å¼€å§‹ä¸‹è½½å’Œè§£æè§„åˆ™...")
        self.progress.start_stage("ä¸‹è½½è§£æè§„åˆ™")
        
        all_urls = self.black_urls + self.white_urls
        total_urls = len(all_urls)
        
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            # æäº¤ä¸‹è½½ä»»åŠ¡
            future_to_url = {executor.submit(self.download_url, url): url for url in all_urls}
            
            # å¤„ç†ç»“æœ
            for i, future in enumerate(concurrent.futures.as_completed(future_to_url), 1):
                url = future_to_url[future]
                try:
                    content = future.result()
                    if content:
                        black_domains, white_domains = self.parse_content_fast(content)
                        results.append((black_domains, white_domains))
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if i % 5 == 0 or i == total_urls:
                        logger.info(f"å¤„ç†è¿›åº¦: {i}/{total_urls}")
                    
                    self.progress.update_progress()
                        
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
        
        # åˆå¹¶ç»“æœ
        for black_domains, white_domains in results:
            self.black_domains.update(black_domains)
            self.white_domains.update(white_domains)
        
        self.progress.end_stage()
        logger.info(f"è§£æå®Œæˆ: é»‘åå•åŸŸå {len(self.black_domains):,} ä¸ª")
        logger.info(f"ç™½åå•åŸŸå {len(self.white_domains):,} ä¸ª")
    
    def apply_whitelist_fast(self):
        """å¿«é€Ÿåº”ç”¨ç™½åå•"""
        if not self.white_domains:
            logger.warning("æ²¡æœ‰ç™½åå•åŸŸå")
            return
        
        self.progress.start_stage("åº”ç”¨ç™½åå•")
        
        original_count = len(self.black_domains)
        
        # ç›´æ¥åŒ¹é…ç§»é™¤
        self.black_domains -= self.white_domains
        
        # ç®€å•çš„å­åŸŸååŒ¹é…ï¼ˆåªæ£€æŸ¥ä¸€çº§å­åŸŸåï¼‰
        white_suffixes = set()
        for white_domain in self.white_domains:
            white_suffixes.add(f".{white_domain}")
        
        to_remove = set()
        batch_size = CONFIG['PERFORMANCE']['batch_size']
        black_list = list(self.black_domains)
        
        for i in range(0, len(black_list), batch_size):
            batch = black_list[i:i+batch_size]
            for black_domain in batch:
                # æ£€æŸ¥æ˜¯å¦ä»¥ä»»ä½•ç™½åå•åç¼€ç»“å°¾
                for suffix in white_suffixes:
                    if black_domain.endswith(suffix):
                        to_remove.add(black_domain)
                        break
            
            self.progress.update_progress(len(batch))
        
        self.black_domains -= to_remove
        
        removed = original_count - len(self.black_domains)
        self.progress.end_stage()
        logger.info(f"ç™½åå•åº”ç”¨å®Œæˆ: ç§»é™¤ {removed} ä¸ªåŸŸåï¼Œå‰©ä½™ {len(self.black_domains):,} ä¸ª")
    
    def optimize_domains(self):
        """ä¼˜åŒ–åŸŸåé›†åˆ"""
        self.progress.start_stage("ä¼˜åŒ–åŸŸå")
        
        # ä½¿ç”¨ä¼˜åŒ–è¿‡æ»¤å™¨
        self.black_domains = OptimizedDomainFilter.optimize_domains(self.black_domains)
        
        self.progress.end_stage()
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        logger.info("ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        self.progress.start_stage("ç”Ÿæˆæ–‡ä»¶")
        
        # ä¼˜åŒ–åŸŸå
        self.optimize_domains()
        
        # 1. Adblockè§„åˆ™ (ad.txt)
        with open(CONFIG['OUTPUT_FILES']['ad'], 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(self.white_domains):,} ä¸ª\n")
            f.write(f"! ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n")
            f.write("! æ¥æº: https://github.com/wansheng8/adblock\n\n")
            
            # é»‘åå•åŸŸåè§„åˆ™ï¼ˆæ‰¹é‡å†™å…¥æé«˜æ€§èƒ½ï¼‰
            domains = sorted(self.black_domains)
            for i in range(0, len(domains), CONFIG['PERFORMANCE']['batch_size']):
                batch = domains[i:i+CONFIG['PERFORMANCE']['batch_size']]
                for domain in batch:
                    f.write(f"||{domain}^\n")
                
                self.progress.update_progress(len(batch))
        
        # 2. DNSè§„åˆ™ (dns.txt)
        with open(CONFIG['OUTPUT_FILES']['dns'], 'w', encoding='utf-8') as f:
            f.write(f"# DNSè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n")
            f.write(f"# ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n\n")
            
            domains = sorted(self.black_domains)
            for i in range(0, len(domains), CONFIG['PERFORMANCE']['batch_size']):
                batch = domains[i:i+CONFIG['PERFORMANCE']['batch_size']]
                for domain in batch:
                    f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        with open(CONFIG['OUTPUT_FILES']['hosts'], 'w', encoding='utf-8') as f:
            f.write(f"# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n")
            f.write(f"# ç‰ˆæœ¬: {datetime.now().strftime('%Y%m%d')}\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n\n")
            
            domains = sorted(self.black_domains)
            for i in range(0, len(domains), CONFIG['PERFORMANCE']['batch_size']):
                batch = domains[i:i+CONFIG['PERFORMANCE']['batch_size']]
                for domain in batch:
                    f.write(f"0.0.0.0 {domain}\n")
        
        # 4. é»‘åå•è§„åˆ™ (black.txt) - ç®€åŒ–çš„adblockæ ¼å¼
        with open(CONFIG['OUTPUT_FILES']['black'], 'w', encoding='utf-8') as f:
            domains = sorted(self.black_domains)
            for domain in domains:
                f.write(f"||{domain}^\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        with open(CONFIG['OUTPUT_FILES']['white'], 'w', encoding='utf-8') as f:
            f.write("# ç™½åå•è§„åˆ™\n")
            f.write("# è¿™äº›åŸŸåä¸ä¼šè¢«æ‹¦æˆª\n\n")
            for domain in sorted(self.white_domains):
                f.write(f"@@||{domain}^\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': datetime.now().strftime('%Y%m%d'),
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains),
                'total_domains': len(self.black_domains) + len(self.white_domains)
            },
            'performance': {
                'max_domains': CONFIG['PERFORMANCE']['max_total_domains'],
                'optimized': True
            }
        }
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        self.progress.end_stage()
        logger.info("è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.mdæ–‡ä»¶"""
        logger.info("ç”ŸæˆREADME.md...")
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        version = info['version']
        
        readme_content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {version} è§„åˆ™ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- æ€»åŸŸåæ•°ï¼š{info['rules']['total_domains']:,} ä¸ª

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*

## æ€§èƒ½ä¼˜åŒ–è¯´æ˜

ä¸ºç¡®ä¿ç”Ÿæˆé€Ÿåº¦å’Œè§„åˆ™è´¨é‡ï¼Œæœ¬è§„åˆ™é›†è¿›è¡Œäº†ä»¥ä¸‹ä¼˜åŒ–ï¼š

1. **åŸŸåæ•°é‡é™åˆ¶**ï¼šé™åˆ¶åœ¨ {info['performance']['max_domains']:,} ä¸ªé«˜è´¨é‡åŸŸåå†…
2. **æ™ºèƒ½è¿‡æ»¤**ï¼šè‡ªåŠ¨ç§»é™¤æ— æ•ˆå’Œä½è´¨é‡åŸŸå
3. **æ‰¹é‡å¤„ç†**ï¼šä½¿ç”¨æ‰¹é‡å¤„ç†æé«˜æ€§èƒ½
4. **èµ„æºä¼˜åŒ–**ï¼šä¼˜åŒ–å†…å­˜ä½¿ç”¨å’ŒCPUå ç”¨

## ä½¿ç”¨å»ºè®®

1. **AdGuard/uBlock Origin**ï¼šä½¿ç”¨ `ad.txt` æ–‡ä»¶
2. **Pi-hole/AdGuard Home**ï¼šä½¿ç”¨ `dns.txt` æ–‡ä»¶
3. **ç³»ç»ŸHosts**ï¼šä½¿ç”¨ `hosts.txt` æ–‡ä»¶
4. **è¯¯æŠ¥å¤„ç†**ï¼šæŸ¥çœ‹ `white.txt` æˆ–æäº¤Issue

---
*ç”Ÿæˆå™¨ä»£ç ï¼šhttps://github.com/wansheng8/adblock*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 50)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆ")
        print("=" * 50)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½è§„åˆ™æº
            self.load_sources()
            
            # 2. ä¸‹è½½å’Œè§£æè§„åˆ™
            self.download_and_parse_all()
            
            # 3. åº”ç”¨ç™½åå•
            self.apply_whitelist_fast()
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶
            self.generate_files()
            
            # 5. ç”ŸæˆREADME.md
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 50)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"âœ… ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("=" * 50)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # åœ¨GitHub Actionsä¸­è‡ªåŠ¨ä½¿ç”¨ä¼˜åŒ–ç‰ˆ
    print("\nâš¡ ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–ç‰ˆ...")
    generator = OptimizedAdBlockGenerator()
    
    # è¿è¡Œç”Ÿæˆå™¨
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
