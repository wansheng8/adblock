#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç²¾å‡†ä¿®å¤ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨
è§£å†³ä¸æ‹¦æˆªå’Œè¯¯æ‹¦æˆªé—®é¢˜ï¼Œå¢åŠ ç²¾ç¡®åŒ¹é…å’Œæ™ºèƒ½è¿‡æ»¤
"""

import os
import re
import json
import time
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, List, Optional, Tuple, Dict
import requests
from urllib.parse import urlparse
from collections import defaultdict

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 15,
    'TIMEOUT': 20,
    'RETRY_TIMES': 3,
    
    # æ–‡ä»¶è·¯å¾„
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    
    # è¾“å‡ºæ–‡ä»¶ï¼ˆå›ºå®šæ–‡ä»¶åï¼‰
    'AD_FILE': 'rules/outputs/ad.txt',
    'DNS_FILE': 'rules/outputs/dns.txt',
    'HOSTS_FILE': 'rules/outputs/hosts.txt',
    'BLACK_FILE': 'rules/outputs/black.txt',
    'WHITE_FILE': 'rules/outputs/white.txt',
    'INFO_FILE': 'rules/outputs/info.json',
    
    # æ–°å¢ï¼šæ™ºèƒ½è¿‡æ»¤é…ç½®
    'INTELLIGENT_FILTERING': {
        'enable_essential_domain_whitelist': True,  # å¯ç”¨å¿…è¦åŸŸåç™½åå•
        'enable_safe_domains_check': True,          # å¯ç”¨å®‰å…¨åŸŸåæ£€æŸ¥
        'enable_false_positive_filter': True,       # å¯ç”¨è¯¯æŠ¥è¿‡æ»¤
        'remove_suspicious_wildcards': True,        # ç§»é™¤å¯ç–‘é€šé…ç¬¦
        'keep_popular_domains': True,              # ä¿ç•™å¸¸ç”¨åŸŸå
        'enable_domain_validation': True           # å¯ç”¨åŸŸåéªŒè¯
    },
    
    # å¿…è¦åŸŸåç™½åå•ï¼ˆé˜²æ­¢è¯¯æ‹¦æˆªï¼‰
    'ESSENTIAL_DOMAINS': [
        # å¸¸ç”¨APPå’ŒæœåŠ¡åŸŸå
        'apple.com', 'google.com', 'microsoft.com', 'amazon.com',
        'github.com', 'gitlab.com', 'docker.com', 'cloudflare.com',
        'baidu.com', 'tencent.com', 'alibaba.com', 'taobao.com',
        'weixin.qq.com', 'qq.com', 'weibo.com', 'zhihu.com',
        'bilibili.com', 'douyin.com', 'kuaishou.com',
        
        # æ“ä½œç³»ç»Ÿå’Œæµè§ˆå™¨
        'windowsupdate.com', 'mozilla.org', 'chromium.org',
        'ubuntu.com', 'debian.org', 'redhat.com',
        
        # å®‰å…¨è¯ä¹¦å’ŒåŠ å¯†
        'letsencrypt.org', 'digicert.com', 'symantec.com',
        'verisign.com', 'globalsign.com',
        
        # å¼€å‘å·¥å…·
        'npmjs.com', 'yarnpkg.com', 'pypi.org', 'maven.org',
        'docker.io', 'kubernetes.io', 'terraform.io',
        
        # å¸¸è§CDNå’Œäº‘æœåŠ¡
        'akamai.net', 'fastly.net', 'aws.amazon.com',
        'azure.com', 'cloud.google.com', 'aliyun.com',
        'huaweicloud.com', 'tencentcloud.com',
        
        # é‚®ç®±æœåŠ¡
        'gmail.com', 'outlook.com', 'yahoo.com', '163.com',
        '126.com', 'foxmail.com', 'qq.com', 'sina.com',
        
        # ç¤¾äº¤åª’ä½“
        'facebook.com', 'twitter.com', 'instagram.com',
        'linkedin.com', 'pinterest.com', 'tiktok.com',
        
        # æ”¯ä»˜æœåŠ¡
        'paypal.com', 'stripe.com', 'alipay.com', 'wechat.com',
        'unionpay.com', 'visa.com', 'mastercard.com'
    ],
    
    # å®‰å…¨åŸŸåæ£€æŸ¥ï¼ˆä¸æ‹¦æˆªè¿™äº›åŸŸåï¼‰
    'SAFE_DOMAINS': [
        # ç³»ç»ŸåŸŸå
        'localhost', 'local', '127.0.0.1', '0.0.0.0', '::1',
        
        # å¸¸ç”¨å·¥å…·
        'stackoverflow.com', 'stackexchange.com', 'github.com',
        'gitlab.com', 'bitbucket.org', 'sourceforge.net',
        
        # æ–‡æ¡£å’Œå¸®åŠ©
        'wikipedia.org', 'wikimedia.org', 'archive.org',
        'creativecommons.org', 'gnu.org', 'apache.org',
        
        # æ”¿åºœå’Œéè¥åˆ©ç»„ç»‡
        'gov.cn', 'gov.uk', 'gov', 'org', 'edu', 'mil',
        
        # å¼€æºé¡¹ç›®
        'linuxfoundation.org', 'opensource.org', 'gnu.org',
        'apache.org', 'eclipse.org', 'mozilla.org'
    ],
    
    # å¯ç–‘è§„åˆ™æ¨¡å¼ï¼ˆå¯èƒ½è¯¯æ‹¦æˆªï¼‰
    'SUSPICIOUS_PATTERNS': [
        r'^\|\|([a-z]{1,2})\.com\^',          # çŸ­åŸŸå.com
        r'^\|\|([a-z]{1,3})\.(com|net|org)\^', # å¾ˆçŸ­çš„ä¸»åŸŸå
        r'^\|\|([a-z0-9]+-[a-z0-9]+)\.[a-z]+\^', # å¸¦æ¨ªçº¿çš„åŸŸå
        r'^\|\|([a-z]+)\d+[a-z]+\.[a-z]+\^',   # æ•°å­—åœ¨ä¸­é—´çš„åŸŸå
        r'^\|\|\*\.',                         # å…¨é€šé…ç¬¦
        r'^\|\|.*\$\$.*',                     # å¤æ‚å…ƒç´ è§„åˆ™
        r'^\|\|.*\$\$script.*',               # è„šæœ¬æ‹¦æˆªè§„åˆ™
        r'^\|\|.*\$\$image.*',                # å›¾ç‰‡æ‹¦æˆªè§„åˆ™
        r'^\|\|.*\$\$stylesheet.*',           # æ ·å¼è¡¨æ‹¦æˆªè§„åˆ™
    ],
    
    # ä¿ç•™çš„å…³é”®è§„åˆ™ï¼ˆç¡®ä¿æ‹¦æˆªï¼‰
    'CRITICAL_PATTERNS': [
        r'^.*doubleclick\.net.*',             # Googleå¹¿å‘Š
        r'^.*googlesyndication\.com.*',       # Googleè”ç›Ÿ
        r'^.*googleadservices\.com.*',        # Googleå¹¿å‘ŠæœåŠ¡
        r'^.*adsense\.com.*',                 # AdSense
        r'^.*amazon-adsystem\.com.*',         # äºšé©¬é€Šå¹¿å‘Š
        r'^.*facebook\.com\/ads.*',           # Facebookå¹¿å‘Š
        r'^.*\.ad\.',                         # å¹¿å‘Šå­åŸŸå
        r'^.*\.ads\.',                        # å¹¿å‘Šå­åŸŸå
        r'^.*\.tracking\.',                   # è¿½è¸ªå­åŸŸå
        r'^.*\.analytics\.',                  # åˆ†æå­åŸŸå
        r'^.*adserver.*',                     # å¹¿å‘ŠæœåŠ¡å™¨
        r'^.*tracking.*',                     # è¿½è¸ªç›¸å…³
        r'^.*analytics.*',                    # åˆ†æç›¸å…³
        r'^.*metrics.*',                      # æŒ‡æ ‡ç›¸å…³
        r'^.*beacon.*',                       # ä¿¡æ ‡
        r'^.*pixel.*',                        # åƒç´ 
        r'^.*tagmanager.*',                   # æ ‡ç­¾ç®¡ç†
    ]
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AccurateAdBlockGenerator:
    """ç²¾å‡†å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'domains_removed_by_whitelist': 0,
            'domains_removed_by_safe_check': 0,
            'domains_removed_by_suspicious': 0,
            'critical_domains_kept': 0,
            'essential_domains_whitelisted': 0,
            'total_domains_processed': 0
        }
        
        # åˆ›å»ºç›®å½•
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶
        self.create_example_sources()
    
    def create_example_sources(self):
        """åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶"""
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("""# é»‘åå•è§„åˆ™æº
# æ¨èä½¿ç”¨é«˜è´¨é‡çš„è§„åˆ™æºï¼Œé¿å…ä¸æ‹¦æˆªå’Œè¯¯æ‹¦æˆª

# é«˜è´¨é‡å¹¿å‘Šè§„åˆ™ï¼ˆæ¨èï¼‰
https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt
https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/tracking.txt

# å¯é€‰çš„é™„åŠ è§„åˆ™ï¼ˆæ ¹æ®éœ€è¦æ·»åŠ ï¼‰
# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/filters.txt
# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/other.txt
""")
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("""# ç™½åå•è§„åˆ™æº
# æ·»åŠ å¿…è¦çš„ç™½åå•ä»¥é˜²æ­¢è¯¯æ‹¦æˆª

# åŸºæœ¬ç™½åå•ï¼ˆæ¨èï¼‰
https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt

# é’ˆå¯¹å¸¸è§è¯¯æ‹¦æˆªçš„è¡¥å……ç™½åå•
# https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist_domains.txt
""")
    
    def load_sources(self) -> bool:
        """åŠ è½½è§„åˆ™æº"""
        print("ğŸ“‹ åŠ è½½è§„åˆ™æº...")
        
        # åŠ è½½é»‘åå•æº
        if os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
                self.black_urls = [line.strip() for line in f 
                                 if line.strip() and not line.startswith('#')]
        else:
            print(f"âŒ é»‘åå•æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['BLACK_SOURCE']}")
            return False
        
        # åŠ è½½ç™½åå•æº
        if os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
                self.white_urls = [line.strip() for line in f 
                                 if line.strip() and not line.startswith('#')]
        
        if not self.black_urls:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é»‘åå•æºURL")
            return False
        
        print(f"âœ… åŠ è½½å®Œæˆ: {len(self.black_urls)} é»‘åå•æº, {len(self.white_urls)} ç™½åå•æº")
        return True
    
    def download_url(self, url: str) -> Optional[str]:
        """ä¸‹è½½URLå†…å®¹"""
        for attempt in range(CONFIG['RETRY_TIMES']):
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0',
                    'Accept': 'text/plain,text/html'
                }
                
                response = requests.get(
                    url, 
                    headers=headers, 
                    timeout=CONFIG['TIMEOUT']
                )
                
                if response.status_code == 200:
                    return response.text
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: çŠ¶æ€ç  {response.status_code}")
                    
            except Exception as e:
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
        
        return None
    
    def download_all_urls(self) -> List[Tuple[str, str, str]]:
        """ä¸‹è½½æ‰€æœ‰URL"""
        print(f"ğŸ“¥ ä¸‹è½½è§„åˆ™æº...")
        
        all_urls = []
        for url in self.black_urls:
            all_urls.append((url, 'black'))
        for url in self.white_urls:
            all_urls.append((url, 'white'))
        
        results = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            future_to_url = {}
            for url, url_type in all_urls:
                future = executor.submit(self.download_url, url)
                future_to_url[future] = (url, url_type)
            
            for future in concurrent.futures.as_completed(future_to_url):
                url, url_type = future_to_url[future]
                content = future.result()
                if content:
                    results.append((url, url_type, content))
                    print(f"  âœ… ä¸‹è½½æˆåŠŸ: {url}")
                else:
                    print(f"  âŒ ä¸‹è½½å¤±è´¥: {url}")
        
        if not results:
            print("âŒ æ‰€æœ‰è§„åˆ™æºä¸‹è½½éƒ½å¤±è´¥äº†ï¼")
            return []
        
        return results
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæœ‰æ•ˆæ€§"""
        if not domain:
            return False
        
        domain = domain.strip().lower()
        
        # åŸºæœ¬æ£€æŸ¥
        if len(domain) < 4 or len(domain) > 253:
            return False
        
        if '.' not in domain:
            return False
        
        # æ’é™¤ç³»ç»ŸåŸŸå
        if domain in ['localhost', 'local', '127.0.0.1', '0.0.0.0', '::1']:
            return False
        
        # æ£€æŸ¥æ ¼å¼
        if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]*[a-z0-9])?)+$', domain):
            return False
        
        # ä¸èƒ½æœ‰ä¸¤ä¸ªè¿ç»­çš„ç‚¹æˆ–ç ´æŠ˜å·
        if '..' in domain or '--' in domain:
            return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        # é¡¶çº§åŸŸåè‡³å°‘2ä¸ªå­—ç¬¦
        if len(parts[-1]) < 2:
            return False
        
        for part in parts:
            if len(part) < 1 or len(part) > 63:
                return False
            
            if part.startswith('-') or part.endswith('-'):
                return False
        
        return True
    
    def extract_domains_from_content(self, content: str) -> Tuple[Set[str], Set[str]]:
        """ä»å†…å®¹ä¸­æå–åŸŸåï¼ˆé»‘ç™½åå•ï¼‰"""
        black_domains = set()
        white_domains = set()
        
        lines = content.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line or line.startswith('!') or line.startswith('#'):
                continue
            
            is_whitelist = line.startswith('@@')
            
            # æå–åŸŸå
            domain = None
            
            # å¸¸è§æ ¼å¼
            if line.startswith('||'):
                # ||domain.com^ æ ¼å¼
                if '^' in line:
                    domain = line[2:line.find('^')]
                else:
                    domain = line[2:]
            elif re.match(r'^\d+\.\d+\.\d+\.\d+\s+', line):
                # Hostsæ ¼å¼: 0.0.0.0 domain.com
                parts = line.split()
                if len(parts) >= 2:
                    domain = parts[1]
            elif line.startswith('@@||'):
                # @@||domain.com^ ç™½åå•æ ¼å¼
                if '^' in line:
                    domain = line[4:line.find('^')]
            elif '.' in line and ' ' not in line and '/' not in line:
                # ç®€å•åŸŸåæ ¼å¼
                domain = line.split('^')[0] if '^' in line else line
            
            # æ¸…ç†å’ŒéªŒè¯åŸŸå
            if domain:
                domain = domain.lower()
                domain = re.sub(r'^www\d*\.', '', domain)
                domain = re.sub(r'^\.+|\.+$', '', domain)
                
                if self.is_valid_domain(domain):
                    if is_whitelist:
                        white_domains.add(domain)
                    else:
                        black_domains.add(domain)
        
        return black_domains, white_domains
    
    def apply_essential_whitelist(self, domains: Set[str]) -> Set[str]:
        """åº”ç”¨å¿…è¦åŸŸåç™½åå•"""
        if not CONFIG['INTELLIGENT_FILTERING']['enable_essential_domain_whitelist']:
            return domains
        
        print("ğŸ”§ åº”ç”¨å¿…è¦åŸŸåç™½åå•...")
        
        essential_set = set(CONFIG['ESSENTIAL_DOMAINS'])
        filtered_domains = set()
        whitelisted_count = 0
        
        for domain in domains:
            is_essential = False
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å¿…è¦åŸŸååˆ—è¡¨ä¸­
            for essential_domain in essential_set:
                if domain == essential_domain or domain.endswith(f".{essential_domain}"):
                    is_essential = True
                    break
            
            if is_essential:
                whitelisted_count += 1
                self.white_domains.add(domain)  # æ·»åŠ åˆ°ç™½åå•
            else:
                filtered_domains.add(domain)
        
        self.stats['essential_domains_whitelisted'] = whitelisted_count
        print(f"  âœ… ç™½åå•ä¿æŠ¤äº† {whitelisted_count} ä¸ªå¿…è¦åŸŸå")
        
        return filtered_domains
    
    def check_safe_domains(self, domains: Set[str]) -> Set[str]:
        """æ£€æŸ¥å®‰å…¨åŸŸå"""
        if not CONFIG['INTELLIGENT_FILTERING']['enable_safe_domains_check']:
            return domains
        
        print("ğŸ” æ£€æŸ¥å®‰å…¨åŸŸå...")
        
        safe_set = set(CONFIG['SAFE_DOMAINS'])
        filtered_domains = set()
        removed_count = 0
        
        for domain in domains:
            is_safe = False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å®‰å…¨åŸŸå
            for safe_domain in safe_set:
                if domain == safe_domain or domain.endswith(f".{safe_domain}"):
                    is_safe = True
                    break
            
            if is_safe:
                removed_count += 1
                # å®‰å…¨åŸŸåä¸æ·»åŠ åˆ°é»‘åå•
            else:
                filtered_domains.add(domain)
        
        self.stats['domains_removed_by_safe_check'] = removed_count
        print(f"  âœ… ç§»é™¤äº† {removed_count} ä¸ªå®‰å…¨åŸŸå")
        
        return filtered_domains
    
    def filter_suspicious_domains(self, domains: Set[str]) -> Set[str]:
        """è¿‡æ»¤å¯ç–‘åŸŸå"""
        if not CONFIG['INTELLIGENT_FILTERING']['enable_false_positive_filter']:
            return domains
        
        print("ğŸ” è¿‡æ»¤å¯ç–‘åŸŸå...")
        
        filtered_domains = set()
        removed_count = 0
        
        for domain in domains:
            is_suspicious = False
            
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¯ç–‘æ¨¡å¼
            for pattern in CONFIG['SUSPICIOUS_PATTERNS']:
                if re.match(pattern, f"||{domain}^"):
                    is_suspicious = True
                    break
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­åŸŸåï¼ˆå¯èƒ½è¯¯æ‹¦æˆªï¼‰
            parts = domain.split('.')
            if len(parts) >= 2 and len(parts[-2]) <= 3 and len(domain) < 10:
                is_suspicious = True
            
            if is_suspicious:
                removed_count += 1
                # å¯ç–‘åŸŸåä¸æ·»åŠ åˆ°é»‘åå•
            else:
                filtered_domains.add(domain)
        
        self.stats['domains_removed_by_suspicious'] = removed_count
        print(f"  âœ… è¿‡æ»¤äº† {removed_count} ä¸ªå¯ç–‘åŸŸå")
        
        return filtered_domains
    
    def ensure_critical_domains(self, domains: Set[str]) -> Set[str]:
        """ç¡®ä¿å…³é”®å¹¿å‘ŠåŸŸåè¢«åŒ…å«"""
        print("ğŸ¯ ç¡®ä¿å…³é”®å¹¿å‘ŠåŸŸå...")
        
        final_domains = set(domains)
        added_count = 0
        
        # å…³é”®å¹¿å‘ŠåŸŸååˆ—è¡¨ï¼ˆç¡®ä¿è¿™äº›è¢«æ‹¦æˆªï¼‰
        critical_ad_domains = [
            # Googleå¹¿å‘Šç³»ç»Ÿ
            'doubleclick.net', 'googlesyndication.com', 'googleadservices.com',
            'adservice.google.com', 'adsense.com', 'google-analytics.com',
            
            # Facebookå¹¿å‘Š
            'facebook.com/ads', 'fbcdn.net',
            
            # äºšé©¬é€Šå¹¿å‘Š
            'amazon-adsystem.com',
            
            # å¸¸è§å¹¿å‘Šç½‘ç»œ
            'adnxs.com', 'rubiconproject.com', 'openx.net',
            'criteo.com', 'taboola.com', 'outbrain.com',
            
            # è¿½è¸ªå’Œç»Ÿè®¡
            'scorecardresearch.com', 'quantserve.com',
            'chartbeat.com', 'mixpanel.com',
            
            # ä¸­å›½å¹¿å‘Šç½‘ç»œ
            'tanx.com', 'alimama.com', 'tanx.com',
            'miaozhen.com', 'cnzz.com', '51.la',
        ]
        
        for critical_domain in critical_ad_domains:
            if critical_domain not in final_domains:
                # æ£€æŸ¥æ˜¯å¦ç™½åå•
                is_whitelisted = False
                for white_domain in self.white_domains:
                    if critical_domain == white_domain or critical_domain.endswith(f".{white_domain}"):
                        is_whitelisted = True
                        break
                
                if not is_whitelisted and self.is_valid_domain(critical_domain):
                    final_domains.add(critical_domain)
                    added_count += 1
        
        self.stats['critical_domains_kept'] = added_count
        print(f"  âœ… ç¡®ä¿äº† {added_count} ä¸ªå…³é”®å¹¿å‘ŠåŸŸå")
        
        return final_domains
    
    def apply_precise_whitelist(self, black_domains: Set[str], white_domains: Set[str]) -> Set[str]:
        """åº”ç”¨ç²¾ç¡®çš„ç™½åå•"""
        print("ğŸ¯ åº”ç”¨ç²¾ç¡®ç™½åå•...")
        
        filtered_domains = set(black_domains)
        removed_count = 0
        
        # æ„å»ºç™½åå•æ ‘ä»¥åŠ é€ŸåŒ¹é…
        white_tree = {}
        for domain in white_domains:
            parts = domain.split('.')
            parts.reverse()
            node = white_tree
            for part in parts:
                if part not in node:
                    node[part] = {}
                node = node[part]
            node['*'] = True
        
        # åº”ç”¨ç™½åå•
        for black_domain in black_domains:
            parts = black_domain.split('.')
            parts.reverse()
            node = white_tree
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•ä¸­
            is_whitelisted = False
            for part in parts:
                if '*' in node:
                    # å®Œå…¨åŒ¹é…ç™½åå•
                    is_whitelisted = True
                    break
                if part in node:
                    node = node[part]
                else:
                    break
            else:
                if '*' in node:
                    is_whitelisted = True
            
            if is_whitelisted:
                filtered_domains.remove(black_domain)
                removed_count += 1
        
        self.stats['domains_removed_by_whitelist'] = removed_count
        print(f"  âœ… ç™½åå•ç§»é™¤äº† {removed_count} ä¸ªåŸŸå")
        
        return filtered_domains
    
    def process_downloaded_content(self, results: List[Tuple[str, str, str]]):
        """å¤„ç†ä¸‹è½½çš„å†…å®¹ï¼ˆæ™ºèƒ½è¿‡æ»¤ç‰ˆï¼‰"""
        print("ğŸ”§ æ™ºèƒ½å¤„ç†è§„åˆ™å†…å®¹...")
        
        all_black_domains = set()
        all_white_domains = set()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰åŸŸå
        for url, url_type, content in results:
            black_domains, white_domains = self.extract_domains_from_content(content)
            
            if url_type == 'black':
                all_black_domains.update(black_domains)
                # é»‘åå•æºä¸­çš„ç™½åå•ä¹Ÿæ”¶é›†
                all_white_domains.update(white_domains)
            else:
                # ç™½åå•æºï¼šä¼˜å…ˆä½¿ç”¨
                all_white_domains.update(white_domains)
        
        self.stats['total_domains_processed'] = len(all_black_domains)
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(all_black_domains)} é»‘åå•åŸŸå, {len(all_white_domains)} ç™½åå•åŸŸå")
        
        # ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½è¿‡æ»¤å¤„ç†
        print("\nğŸ¯ å¼€å§‹æ™ºèƒ½è¿‡æ»¤...")
        
        # æ­¥éª¤1ï¼šåº”ç”¨å¿…è¦åŸŸåç™½åå•
        filtered_domains = self.apply_essential_whitelist(all_black_domains)
        
        # æ­¥éª¤2ï¼šæ£€æŸ¥å®‰å…¨åŸŸå
        filtered_domains = self.check_safe_domains(filtered_domains)
        
        # æ­¥éª¤3ï¼šè¿‡æ»¤å¯ç–‘åŸŸåï¼ˆå‡å°‘è¯¯æ‹¦æˆªï¼‰
        filtered_domains = self.filter_suspicious_domains(filtered_domains)
        
        # æ­¥éª¤4ï¼šåº”ç”¨ç²¾ç¡®ç™½åå•
        filtered_domains = self.apply_precise_whitelist(filtered_domains, all_white_domains)
        
        # æ­¥éª¤5ï¼šç¡®ä¿å…³é”®å¹¿å‘ŠåŸŸåï¼ˆé˜²æ­¢ä¸æ‹¦æˆªï¼‰
        final_domains = self.ensure_critical_domains(filtered_domains)
        
        # æœ€ç»ˆç»“æœ
        self.black_domains = final_domains
        self.white_domains = all_white_domains
        
        # ç”Ÿæˆè§„åˆ™
        for domain in self.black_domains:
            self.black_rules.add(f"||{domain}^")
        
        for domain in self.white_domains:
            self.white_rules.add(f"@@||{domain}^")
        
        print(f"\nâœ… å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(self.black_domains)} é»‘åå•åŸŸå, {len(self.white_domains)} ç™½åå•åŸŸå")
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # æ£€æŸ¥ç»“æœ
        if len(self.black_domains) == 0:
            print("âš ï¸  è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é»‘åå•åŸŸå")
        
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        version = datetime.now().strftime('%Y%m%d_%H%M')
        
        # 1. Adblockè§„åˆ™ (ad.txt)
        with open(CONFIG['AD_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"""! ç²¾å‡†å¹¿å‘Šè¿‡æ»¤è§„åˆ™
! ç”Ÿæˆæ—¶é—´: {timestamp}
! ç‰ˆæœ¬: {version}
! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª
! ç™½åå•åŸŸå: {len(self.white_domains):,} ä¸ª
! æ™ºèƒ½è¿‡æ»¤ç»Ÿè®¡:
!   - å¿…è¦åŸŸåä¿æŠ¤: {self.stats['essential_domains_whitelisted']} ä¸ª
!   - å®‰å…¨åŸŸåæ’é™¤: {self.stats['domains_removed_by_safe_check']} ä¸ª
!   - å¯ç–‘åŸŸåè¿‡æ»¤: {self.stats['domains_removed_by_suspicious']} ä¸ª
!   - ç™½åå•ç§»é™¤: {self.stats['domains_removed_by_whitelist']} ä¸ª
!   - å…³é”®å¹¿å‘ŠåŸŸå: {self.stats['critical_domains_kept']} ä¸ª
! é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}

! ========== ç™½åå•è§„åˆ™ï¼ˆé˜²æ­¢è¯¯æ‹¦æˆªï¼‰ ==========
""")
            for rule in sorted(self.white_rules):
                f.write(f"{rule}\n")
            
            f.write("""
! ========== é»‘åå•è§„åˆ™ï¼ˆç²¾å‡†å¹¿å‘Šè¿‡æ»¤ï¼‰ ==========
! å·²åº”ç”¨æ™ºèƒ½è¿‡æ»¤ï¼Œå‡å°‘è¯¯æ‹¦æˆªå’Œä¸æ‹¦æˆªé—®é¢˜
""")
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
        
        # 2. DNSè§„åˆ™ (dns.txt)
        with open(CONFIG['DNS_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"""# DNSè¿‡æ»¤è§„åˆ™
# ç”Ÿæˆæ—¶é—´: {timestamp}
# ç‰ˆæœ¬: {version}
# åŸŸåæ•°é‡: {len(self.black_domains):,}
# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}
# å·²åº”ç”¨æ™ºèƒ½è¿‡æ»¤ï¼Œå‡å°‘è¯¯æ‹¦æˆª

""")
            for domain in sorted(self.black_domains):
                f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        with open(CONFIG['HOSTS_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"""# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™
# ç”Ÿæˆæ—¶é—´: {timestamp}
# ç‰ˆæœ¬: {version}
# åŸŸåæ•°é‡: {len(self.black_domains):,}
# é¡¹ç›®åœ°å€: https://github.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}
# å·²åº”ç”¨æ™ºèƒ½è¿‡æ»¤ï¼Œå‡å°‘è¯¯æ‹¦æˆª

127.0.0.1 localhost
::1 localhost

# å¹¿å‘ŠåŸŸåå±è”½ï¼ˆæ™ºèƒ½è¿‡æ»¤ç‰ˆï¼‰
""")
            for domain in sorted(self.black_domains):
                f.write(f"0.0.0.0 {domain}\n")
        
        # 4. é»‘åå•è§„åˆ™ (black.txt)
        with open(CONFIG['BLACK_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"""! é»‘åå•è§„åˆ™
! ç”Ÿæˆæ—¶é—´: {timestamp}
! ç‰ˆæœ¬: {version}
! åŸŸåæ•°é‡: {len(self.black_domains):,}

""")
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        with open(CONFIG['WHITE_FILE'], 'w', encoding='utf-8') as f:
            f.write(f"""! ç™½åå•è§„åˆ™
! ç”Ÿæˆæ—¶é—´: {timestamp}
! ç‰ˆæœ¬: {version}
! åŸŸåæ•°é‡: {len(self.white_domains):,}

""")
            for domain in sorted(self.white_domains):
                f.write(f"@@||{domain}^\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': version,
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains)
            },
            'filtering_stats': self.stats,
            'config': {
                'intelligent_filtering': CONFIG['INTELLIGENT_FILTERING'],
                'essential_domains_count': len(CONFIG['ESSENTIAL_DOMAINS']),
                'safe_domains_count': len(CONFIG['SAFE_DOMAINS'])
            }
        }
        
        with open(CONFIG['INFO_FILE'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        print("âœ… è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.md"""
        print("ğŸ“– ç”ŸæˆREADME.md...")
        
        # è¯»å–è§„åˆ™ä¿¡æ¯
        try:
            with open(CONFIG['INFO_FILE'], 'r', encoding='utf-8') as f:
                info = json.load(f)
        except:
            info = {
                'version': datetime.now().strftime('%Y%m%d'),
                'updated_at': datetime.now().isoformat(),
                'rules': {'blacklist_domains': 0, 'whitelist_domains': 0}
            }
        
        # ç”Ÿæˆé“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        readme = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} æ›´æ–°å†…å®¹ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- æ™ºèƒ½è¿‡æ»¤ï¼šé˜²æ­¢è¯¯æ‹¦æˆªå’Œä¸æ‹¦æˆªé—®é¢˜
- å¿…è¦åŸŸåä¿æŠ¤ï¼š{info.get('filtering_stats', {}).get('essential_domains_whitelisted', 0)} ä¸ª

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme)
        
        print("âœ… README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 60)
        print("ğŸ¯ ç²¾å‡†å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("è§£å†³ä¸æ‹¦æˆªå’Œè¯¯æ‹¦æˆªé—®é¢˜")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # 1. åŠ è½½è§„åˆ™æº
            print("\næ­¥éª¤ 1/5: åŠ è½½è§„åˆ™æº")
            if not self.load_sources():
                return False
            
            # 2. ä¸‹è½½è§„åˆ™æº
            print(f"\næ­¥éª¤ 2/5: ä¸‹è½½è§„åˆ™æº")
            results = self.download_all_urls()
            if not results:
                return False
            
            # 3. æ™ºèƒ½å¤„ç†è§„åˆ™
            print(f"\næ­¥éª¤ 3/5: æ™ºèƒ½å¤„ç†è§„åˆ™")
            self.process_downloaded_content(results)
            
            # 4. ç”Ÿæˆè§„åˆ™æ–‡ä»¶
            print(f"\næ­¥éª¤ 4/5: ç”Ÿæˆè§„åˆ™æ–‡ä»¶")
            self.generate_files()
            
            # 5. ç”ŸæˆREADME
            print(f"\næ­¥éª¤ 5/5: ç”ŸæˆREADME.md")
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print("=" * 60)
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print("\nğŸ¯ æ™ºèƒ½è¿‡æ»¤ç»Ÿè®¡:")
            print(f"  â€¢ å¿…è¦åŸŸåä¿æŠ¤: {self.stats['essential_domains_whitelisted']}ä¸ª")
            print(f"  â€¢ å®‰å…¨åŸŸåæ’é™¤: {self.stats['domains_removed_by_safe_check']}ä¸ª")
            print(f"  â€¢ å¯ç–‘åŸŸåè¿‡æ»¤: {self.stats['domains_removed_by_suspicious']}ä¸ª")
            print(f"  â€¢ ç™½åå•ç§»é™¤: {self.stats['domains_removed_by_whitelist']}ä¸ª")
            print(f"  â€¢ å…³é”®å¹¿å‘ŠåŸŸå: {self.stats['critical_domains_kept']}ä¸ª")
            print("=" * 60)
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("ğŸ”— è®¢é˜…åœ°å€å·²åœ¨README.mdä¸­æ›´æ–°")
            print("=" * 60)
            
            # å»ºè®®
            if self.stats['domains_removed_by_suspicious'] > 100:
                print("\nğŸ’¡ å»ºè®®ï¼šæ£€æµ‹åˆ°å¤§é‡å¯ç–‘åŸŸåè¢«è¿‡æ»¤ï¼Œå¦‚æœå¹¿å‘Šæ‹¦æˆªæ•ˆæœä¸è¶³ï¼Œ")
                print("      å¯ä»¥åœ¨é…ç½®ä¸­å…³é—­ 'enable_false_positive_filter'")
            
            if self.stats['essential_domains_whitelisted'] > 50:
                print("\nğŸ’¡ å»ºè®®ï¼šå·²ä¿æŠ¤å¤§é‡å¿…è¦åŸŸåï¼Œå¯æœ‰æ•ˆå‡å°‘è¯¯æ‹¦æˆª")
            
            return True
            
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            return False
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help' or sys.argv[1] == '-h':
            print("ğŸ¯ ç²¾å‡†å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("  python run.py              # æ­£å¸¸è¿è¡Œ")
            print("  python run.py --strict     # ä¸¥æ ¼æ¨¡å¼ï¼ˆæ›´å¤šè¿‡æ»¤ï¼‰")
            print("  python run.py --loose      # å®½æ¾æ¨¡å¼ï¼ˆå‡å°‘è¿‡æ»¤ï¼‰")
            print("  python run.py --stats      # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡")
            return
        
        elif sys.argv[1] == '--strict':
            print("ğŸ”§ ä¸¥æ ¼æ¨¡å¼ï¼šæ›´å¤šè¿‡æ»¤ï¼Œå‡å°‘è¯¯æ‹¦æˆª")
            CONFIG['INTELLIGENT_FILTERING']['enable_false_positive_filter'] = True
            CONFIG['INTELLIGENT_FILTERING']['enable_safe_domains_check'] = True
        
        elif sys.argv[1] == '--loose':
            print("ğŸ”§ å®½æ¾æ¨¡å¼ï¼šå‡å°‘è¿‡æ»¤ï¼Œå¢åŠ æ‹¦æˆª")
            CONFIG['INTELLIGENT_FILTERING']['enable_false_positive_filter'] = False
            CONFIG['INTELLIGENT_FILTERING']['enable_safe_domains_check'] = False
        
        elif sys.argv[1] == '--stats':
            print("ğŸ“Š è¿‡æ»¤é…ç½®ç»Ÿè®¡:")
            print(f"  å¿…è¦åŸŸåæ•°é‡: {len(CONFIG['ESSENTIAL_DOMAINS'])}")
            print(f"  å®‰å…¨åŸŸåæ•°é‡: {len(CONFIG['SAFE_DOMAINS'])}")
            print(f"  å¯ç–‘æ¨¡å¼æ•°é‡: {len(CONFIG['SUSPICIOUS_PATTERNS'])}")
            print(f"  å…³é”®æ¨¡å¼æ•°é‡: {len(CONFIG['CRITICAL_PATTERNS'])}")
            
            print("\nğŸ”§ æ™ºèƒ½è¿‡æ»¤é…ç½®:")
            for key, value in CONFIG['INTELLIGENT_FILTERING'].items():
                status = "âœ… å¯ç”¨" if value else "âŒ ç¦ç”¨"
                print(f"  {key}: {status}")
            return
    
    # æ­£å¸¸è¿è¡Œ
    print("ğŸ¯ æ­£åœ¨å¯åŠ¨ç²¾å‡†å¹¿å‘Šè¿‡æ»¤ç”Ÿæˆå™¨...")
    print("ğŸ’¡ ç›®æ ‡ï¼šè§£å†³ä¸æ‹¦æˆªå’Œè¯¯æ‹¦æˆªé—®é¢˜")
    
    generator = AccurateAdBlockGenerator()
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
        print("\nğŸ’¡ å¦‚æœä»æœ‰ä¸æ‹¦æˆªæˆ–è¯¯æ‹¦æˆªé—®é¢˜ï¼Œå¯ä»¥ï¼š")
        print("   1. è°ƒæ•´ rules/sources/ ä¸­çš„è§„åˆ™æº")
        print("   2. ä½¿ç”¨ --strict æˆ– --loose æ¨¡å¼")
        print("   3. æŸ¥çœ‹ rules/outputs/info.json è·å–è¯¦ç»†ç»Ÿè®¡")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
