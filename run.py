#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - å®Œæ•´ç‰ˆ
æ‰€æœ‰åŠŸèƒ½éƒ½åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­
"""

import os
import re
import json
import time
import hashlib
import logging
import concurrent.futures
from datetime import datetime
from typing import Set, Dict, List, Optional, Tuple
import requests
import urllib.parse
from collections import defaultdict
import sys

# ========== é…ç½® ==========
CONFIG = {
    # GitHubä¿¡æ¯
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    
    # æ€§èƒ½è®¾ç½®
    'MAX_WORKERS': 5,
    'TIMEOUT': 30,
    'RETRY_TIMES': 3,
    
    # è§„åˆ™æºæ–‡ä»¶
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt',
    'CHINA_SOURCE': 'rules/sources/china.txt',
    'ENHANCED_SOURCE': 'rules/sources/enhanced.txt',
    
    # è¾“å‡ºæ–‡ä»¶
    'OUTPUT_FILES': {
        'ad': 'rules/outputs/ad.txt',
        'dns': 'rules/outputs/dns.txt',
        'hosts': 'rules/outputs/hosts.txt',
        'black': 'rules/outputs/black.txt',
        'white': 'rules/outputs/white.txt',
        'info': 'rules/outputs/info.json',
        'smart_ad': 'rules/outputs/smart_ad.txt',
        'mobile_ad': 'rules/outputs/mobile_ad.txt',
    },
    
    # æ’é™¤çš„åŸŸå
    'EXCLUDE_DOMAINS': [
        'localhost', 'local', 'broadcasthost',
        '127.0.0.1', '0.0.0.0', '::1'
    ],
    
    # å‘½ä¸­ç‡ä¼˜åŒ–é…ç½®
    'HIT_OPTIMIZATION': {
        'enable_smart_rules': True,
        'enable_china_focus': True,
        'enable_mobile_optimization': True,
        'min_hit_score': 0.3,
        'max_domains_per_source': 50000,
        'enable_wildcard_expansion': True,
        'enable_subdomain_generation': True,
    },
    
    # ä¸­æ–‡å¹¿å‘Šå…³é”®è¯
    'CHINESE_AD_KEYWORDS': [
        'å¹¿å‘Š', 'æ¨å¹¿', 'è¥é”€', 'æŠ•æ”¾', 'è”ç›Ÿ', 'æµé‡', 'å˜ç°',
        'å¼¹çª—', 'æ‚¬æµ®', 'æ¨ªå¹…', 'æ’å±', 'å¼€å±', 'è´´ç‰‡', 'å‰è´´',
        'ä¸­æ’', 'åè´´', 'è§’æ ‡', 'ä¿¡æ¯æµ', 'åŸç”Ÿ', 'æ¿€åŠ±è§†é¢‘',
        'admob', 'mopub', 'facebook', 'twitter', 'instagram',
        'googlead', 'doubleclick', 'adsystem', 'adservice',
        'tracking', 'analytics', 'statistics', 'monitor',
        'beacon', 'pixel', 'tag', 'cookie', 'fingerprint'
    ],
    
    # ç§»åŠ¨ç«¯å¹¿å‘Šå…³é”®è¯
    'MOBILE_AD_KEYWORDS': [
        'mobile', 'mob', 'android', 'ios', 'iphone', 'ipad',
        'app', 'sdk', 'inapp', 'interstitial', 'reward',
        'banner', 'native', 'video', 'fullscreen', 'push',
        'notification', 'advert', 'promo', 'offer', 'install'
    ]
}

# ========== æ—¥å¿—è®¾ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DomainOptimizer:
    """åŸŸåä¼˜åŒ–å™¨"""
    
    @staticmethod
    def expand_wildcard_domain(domain: str) -> List[str]:
        """æ‰©å±•é€šé…ç¬¦åŸŸå"""
        if '*' not in domain:
            return [domain]
        
        expansions = []
        if domain.startswith('*.') and domain.count('*') == 1:
            base = domain[2:]
            expansions.append(base)
            common_subs = ['www', 'm', 'mobile', 'app', 'api', 'static', 'cdn', 'img', 'image']
            for sub in common_subs:
                expansions.append(f"{sub}.{base}")
        
        return expansions
    
    @staticmethod
    def generate_subdomains(domain: str) -> List[str]:
        """ç”Ÿæˆå¸¸è§å­åŸŸå"""
        subdomains = []
        common_subs = [
            'ad', 'ads', 'adserver', 'advert', 'advertising',
            'track', 'tracking', 'analytics', 'stats', 'stat',
            'click', 'clk', 'affiliate', 'aff', 'promo',
            'banner', 'popup', 'float', 'sponsor', 'sponsored',
            'media', 'video', 'img', 'image', 'static', 'cdn',
            'js', 'script', 'pixel', 'beacon', 'tag'
        ]
        
        for sub in common_subs:
            subdomains.append(f"{sub}.{domain}")
        
        return subdomains
    
    @staticmethod
    def is_ad_domain(domain: str) -> bool:
        """åˆ¤æ–­åŸŸåæ˜¯å¦å¯èƒ½æ˜¯å¹¿å‘ŠåŸŸå"""
        ad_patterns = [
            r'ad[0-9]*[\._-]', r'ads[0-9]*[\._-]', r'advert',
            r'track', r'tracking', r'analytics', r'stats',
            r'doubleclick', r'googlead', r'googlesyndication',
            r'facebook\.com/(plugins|widgets)',
            r'amazon-adsystem', r'moatads', r'scorecardresearch',
            r'quantserve', r'outbrain', r'taboola',
            r'adsystem', r'adservice', r'adserver'
        ]
        
        domain_lower = domain.lower()
        for pattern in ad_patterns:
            if re.search(pattern, domain_lower):
                return True
        
        ad_suffixes = ['.ad.', '.ads.', '.adv.', '.advert.', '.advertising.']
        for suffix in ad_suffixes:
            if suffix in domain_lower:
                return True
        
        return False

class HitRateOptimizer:
    """å‘½ä¸­ç‡ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.domain_hits = defaultdict(int)
        self.pattern_hits = defaultdict(int)
        self.keyword_hits = defaultdict(int)
    
    def score_domain(self, domain: str) -> float:
        """ç»™åŸŸåè¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¯èƒ½æ˜¯å¹¿å‘Šï¼‰"""
        score = 0.0
        
        if DomainOptimizer.is_ad_domain(domain):
            score += 0.5
        
        for keyword in CONFIG['CHINESE_AD_KEYWORDS']:
            if keyword.lower() in domain.lower():
                score += 0.3
                break
        
        for keyword in CONFIG['MOBILE_AD_KEYWORDS']:
            if keyword.lower() in domain.lower():
                score += 0.2
                break
        
        parts = domain.split('.')
        if len(parts) >= 4:
            score += 0.2
        
        if re.search(r'\d{2,}', domain):
            score += 0.1
        
        return min(score, 1.0)
    
    def filter_low_score_domains(self, domains: Set[str], min_score: float = None) -> Set[str]:
        """è¿‡æ»¤ä½åˆ†åŸŸå"""
        if min_score is None:
            min_score = CONFIG['HIT_OPTIMIZATION']['min_hit_score']
        
        filtered = set()
        for domain in domains:
            score = self.score_domain(domain)
            if score >= min_score:
                filtered.add(domain)
        
        logger.info(f"åŸŸåè¿‡æ»¤: {len(domains)} -> {len(filtered)} (åˆ†æ•°é˜ˆå€¼: {min_score})")
        return filtered

class AdBlockGenerator:
    """åŸºç¡€ç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.black_urls = []
        self.white_urls = []
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.setup_directories()
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# é»‘åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n")
                f.write("https://easylist.to/easylist/easylist.txt\n")
                f.write("https://easylist.to/easylist/easyprivacy.txt\n")
            
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n")
    
    def load_sources(self):
        """åŠ è½½è§„åˆ™æºURL"""
        # é»‘åå•æº
        with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.black_urls.append(line)
        
        # ç™½åå•æº
        with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    self.white_urls.append(line)
        
        logger.info(f"åŠ è½½ {len(self.black_urls)} ä¸ªé»‘åå•æº")
        logger.info(f"åŠ è½½ {len(self.white_urls)} ä¸ªç™½åå•æº")
    
    def download_url(self, url: str) -> Optional[str]:
        """ä¸‹è½½URLå†…å®¹"""
        for attempt in range(CONFIG['RETRY_TIMES']):
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                response = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                response.raise_for_status()
                return response.text
            except Exception as e:
                if attempt < CONFIG['RETRY_TIMES'] - 1:
                    time.sleep(2)
                else:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                    return None
    
    def is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæœ‰æ•ˆæ€§"""
        if not domain or domain in CONFIG['EXCLUDE_DOMAINS']:
            return False
        
        if len(domain) < 3 or len(domain) > 253:
            return False
        
        if '.' not in domain:
            return False
        
        parts = domain.split('.')
        for part in parts:
            if len(part) < 1 or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def extract_domain(self, text: str) -> Optional[str]:
        """æå–åŸŸå"""
        if not text:
            return None
        
        text = text.strip()
        
        if '#' in text:
            text = text.split('#')[0].strip()
        
        patterns = [
            (r'^@@\|\|([^\^\$]+)\^', 1),
            (r'^\|\|([^\^\$]+)\^', 1),
            (r'^@@([^\|\^\$]+)$', 1),
            (r'^([a-zA-Z0-9.-]+)$', 1),
            (r'^\d+\.\d+\.\d+\.\d+\s+([a-zA-Z0-9.-]+)', 1),
            (r'^\*\.([a-zA-Z0-9.-]+)', 1),
        ]
        
        for pattern, group in patterns:
            match = re.match(pattern, text)
            if match:
                domain = match.group(group).lower()
                domain = re.sub(r'^www\.', '', domain)
                if self.is_valid_domain(domain):
                    return domain
        
        return None
    
    def parse_content(self, content: str) -> tuple:
        """è§£æè§„åˆ™å†…å®¹"""
        black_domains = set()
        black_rules = set()
        white_domains = set()
        white_rules = set()
        
        for line in content.split('\n'):
            line = line.strip()
            if not line or line.startswith('!') or line.startswith('#'):
                continue
            
            if line.startswith('@@'):
                domain = self.extract_domain(line)
                if domain:
                    white_domains.add(domain)
                    white_rules.add(f"@@||{domain}^")
                else:
                    white_rules.add(line)
            else:
                domain = self.extract_domain(line)
                if domain:
                    black_domains.add(domain)
                else:
                    if re.search(r'[a-zA-Z0-9]', line):
                        black_rules.add(line)
        
        return black_domains, black_rules, white_domains, white_rules
    
    def download_and_parse_all(self):
        """ä¸‹è½½å¹¶è§£ææ‰€æœ‰è§„åˆ™"""
        logger.info("å¼€å§‹ä¸‹è½½å’Œè§£æè§„åˆ™...")
        
        all_urls = [(url, 'black') for url in self.black_urls] + \
                   [(url, 'white') for url in self.white_urls]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = {executor.submit(self.download_url, url): (url, type_) for url, type_ in all_urls}
            
            for future in concurrent.futures.as_completed(futures):
                url, type_ = futures[future]
                try:
                    content = future.result()
                    if content:
                        black_domains, black_rules, white_domains, white_rules = self.parse_content(content)
                        
                        if type_ == 'black':
                            self.black_domains.update(black_domains)
                            self.black_rules.update(black_rules)
                            self.white_domains.update(white_domains)
                            self.white_rules.update(white_rules)
                        else:
                            self.white_domains.update(white_domains)
                            self.white_rules.update(white_rules)
                            self.black_domains.update(black_domains)
                            self.black_rules.update(black_rules)
                            
                        logger.debug(f"å¤„ç†å®Œæˆ: {url}")
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
        
        logger.info(f"è§£æå®Œæˆ: é»‘åå•åŸŸå {len(self.black_domains):,} ä¸ª")
        logger.info(f"ç™½åå•åŸŸå {len(self.white_domains):,} ä¸ª")
    
    def apply_whitelist(self):
        """åº”ç”¨ç™½åå•"""
        if not self.white_domains:
            logger.warning("æ²¡æœ‰ç™½åå•åŸŸå")
            return
        
        original = len(self.black_domains)
        self.black_domains -= self.white_domains
        
        if len(self.white_domains) < 5000:
            to_remove = set()
            for black_domain in self.black_domains:
                for white_domain in self.white_domains:
                    if black_domain.endswith(f".{white_domain}"):
                        to_remove.add(black_domain)
                        break
            
            self.black_domains -= to_remove
            removed = original - len(self.black_domains)
            logger.info(f"ç™½åå•åº”ç”¨å®Œæˆ: ç§»é™¤ {removed} ä¸ªåŸŸå")
        else:
            logger.info(f"ç™½åå•åŸŸåå¤ªå¤š({len(self.white_domains):,})ï¼Œè·³è¿‡å­åŸŸååŒ¹é…")
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        logger.info("ç”Ÿæˆè§„åˆ™æ–‡ä»¶...")
        
        # 1. Adblockè§„åˆ™ (ad.txt)
        with open(CONFIG['OUTPUT_FILES']['ad'], 'w', encoding='utf-8') as f:
            f.write(f"! å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"! é»‘åå•åŸŸå: {len(self.black_domains):,} ä¸ª\n")
            f.write(f"! ç™½åå•åŸŸå: {len(self.white_domains):,} ä¸ª\n\n")
            
            for rule in sorted(self.white_rules):
                f.write(f"{rule}\n")
            
            f.write("\n")
            
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
            
            f.write("\n")
            
            for rule in sorted(self.black_rules):
                f.write(f"{rule}\n")
        
        # 2. DNSè§„åˆ™ (dns.txt)
        with open(CONFIG['OUTPUT_FILES']['dns'], 'w', encoding='utf-8') as f:
            f.write(f"# DNSè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n\n")
            
            for domain in sorted(self.black_domains):
                f.write(f"{domain}\n")
        
        # 3. Hostsè§„åˆ™ (hosts.txt)
        with open(CONFIG['OUTPUT_FILES']['hosts'], 'w', encoding='utf-8') as f:
            f.write(f"# Hostsæ ¼å¼å¹¿å‘Šè¿‡æ»¤è§„åˆ™\n")
            f.write(f"# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# åŸŸåæ•°é‡: {len(self.black_domains):,}\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n\n")
            
            for domain in sorted(self.black_domains):
                f.write(f"0.0.0.0 {domain}\n")
        
        # 4. é»‘åå•è§„åˆ™ (black.txt)
        with open(CONFIG['OUTPUT_FILES']['black'], 'w', encoding='utf-8') as f:
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
        
        # 5. ç™½åå•è§„åˆ™ (white.txt)
        with open(CONFIG['OUTPUT_FILES']['white'], 'w', encoding='utf-8') as f:
            for rule in sorted(self.white_rules):
                f.write(f"{rule}\n")
        
        # 6. è§„åˆ™ä¿¡æ¯ (info.json)
        info = {
            'version': datetime.now().strftime('%Y%m%d'),
            'updated_at': datetime.now().isoformat(),
            'rules': {
                'blacklist_domains': len(self.black_domains),
                'whitelist_domains': len(self.white_domains),
                'blacklist_rules': len(self.black_rules),
                'whitelist_rules': len(self.white_rules)
            },
            'sources': {
                'blacklist': len(self.black_urls),
                'whitelist': len(self.white_urls)
            }
        }
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
        
        logger.info("è§„åˆ™æ–‡ä»¶ç”Ÿæˆå®Œæˆ")
    
    def generate_readme(self):
        """ç”ŸæˆREADME.mdæ–‡ä»¶"""
        logger.info("ç”ŸæˆREADME.md...")
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        version = info['version']
        
        readme_content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œé€‚ç”¨äºå„ç§å¹¿å‘Šæ‹¦æˆªå™¨å’ŒDNSè¿‡æ»¤å™¨ã€‚

## è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|----------|----------|----------|----------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {version} è§„åˆ™ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- å…¶ä»–è§„åˆ™ï¼šé»‘åå• {info['rules']['blacklist_rules']:,} æ¡ï¼Œç™½åå• {info['rules']['whitelist_rules']:,} æ¡

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œä¸»æµç¨‹"""
        print("=" * 50)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - åŸºç¡€ç‰ˆ")
        print("=" * 50)
        
        start_time = time.time()
        
        try:
            self.load_sources()
            self.download_and_parse_all()
            self.apply_whitelist()
            self.generate_files()
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 50)
            print("âœ… å¤„ç†å®Œæˆï¼")
            print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ“ è§„åˆ™æ–‡ä»¶: rules/outputs/")
            print("ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("=" * 50)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

class EnhancedAdBlockGenerator(AdBlockGenerator):
    """å¢å¼ºç‰ˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨"""
    
    def __init__(self):
        super().__init__()
        self.optimizer = HitRateOptimizer()
        self.china_domains = set()
        self.enhanced_domains = set()
        
        # åˆ›å»ºå¢å¼ºæºæ–‡ä»¶
        self.setup_enhanced_sources()
    
    def setup_enhanced_sources(self):
        """åˆ›å»ºå¢å¼ºæºæ–‡ä»¶"""
        # ä¸­æ–‡å¹¿å‘Šè§„åˆ™æº
        if not os.path.exists(CONFIG['CHINA_SOURCE']):
            os.makedirs(os.path.dirname(CONFIG['CHINA_SOURCE']), exist_ok=True)
            with open(CONFIG['CHINA_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ä¸­æ–‡å¹¿å‘Šè¿‡æ»¤è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/hoshsadiq/adblock-nocoin-list/master/nocoin.txt\n")
                f.write("https://easylist-downloads.adblockplus.org/easylistchina.txt\n")
                f.write("https://gitee.com/xinggsf/Adblock-Rule/raw/master/rule.txt\n")
                f.write("https://raw.githubusercontent.com/cjx82630/cjxlist/master/cjx-annoyance.txt\n")
        
        # å¢å¼ºè§„åˆ™æº
        if not os.path.exists(CONFIG['ENHANCED_SOURCE']):
            os.makedirs(os.path.dirname(CONFIG['ENHANCED_SOURCE']), exist_ok=True)
            with open(CONFIG['ENHANCED_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# å¢å¼ºå¹¿å‘Šè¿‡æ»¤è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/filters.txt\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/badware.txt\n")
                f.write("https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/privacy.txt\n")
                f.write("https://raw.githubusercontent.com/Spam404/lists/master/main-blacklist.txt\n")
    
    def load_sources(self):
        """åŠ è½½æ‰€æœ‰è§„åˆ™æº"""
        super().load_sources()
        
        # åŠ è½½ä¸­æ–‡è§„åˆ™æº
        try:
            with open(CONFIG['CHINA_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.black_urls.append(line)
        except FileNotFoundError:
            logger.warning(f"ä¸­æ–‡è§„åˆ™æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['CHINA_SOURCE']}")
        
        # åŠ è½½å¢å¼ºè§„åˆ™æº
        try:
            with open(CONFIG['ENHANCED_SOURCE'], 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        self.black_urls.append(line)
        except FileNotFoundError:
            logger.warning(f"å¢å¼ºè§„åˆ™æºæ–‡ä»¶ä¸å­˜åœ¨: {CONFIG['ENHANCED_SOURCE']}")
        
        logger.info(f"æ€»å…±åŠ è½½ {len(self.black_urls)} ä¸ªè§„åˆ™æº")
    
    def enhanced_extract_domain(self, line: str) -> Optional[str]:
        """å¢å¼ºç‰ˆåŸŸåæå–"""
        if not line:
            return None
        
        line = line.strip()
        
        if '!' in line:
            line = line.split('!')[0].strip()
        if '#' in line:
            line = line.split('#')[0].strip()
        
        if not line or line.startswith('!') or line.startswith('##'):
            return None
        
        patterns = [
            (r'^\|\|([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})\^', 1),
            (r'^\|\|([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+){2,})\^', 1),
            (r'^@@\|\|([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})\^', 1),
            (r'^(?:0\.0\.0\.0|127\.0\.0\.1)\s+([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})', 1),
            (r'^([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})$', 1),
            (r'^(?:\*\.)?([a-zA-Z0-9][a-zA-Z0-9-]*(\.[a-zA-Z0-9-]+)+\.[a-zA-Z]{2,})', 1),
        ]
        
        for pattern, group in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(group).lower()
                domain = re.sub(r'^www\d*\.', '', domain)
                domain = re.sub(r'^www\.', '', domain)
                domain = re.sub(r'^m\.', '', domain)
                domain = re.sub(r'^static\.', '', domain)
                domain = re.sub(r'^cdn\.', '', domain)
                
                if self.is_valid_domain(domain):
                    return domain
        
        return None
    
    def parse_enhanced_content(self, content: str) -> Tuple[Set[str], Set[str], Set[str], Set[str]]:
        """å¢å¼ºç‰ˆå†…å®¹è§£æ"""
        black_domains = set()
        black_rules = set()
        white_domains = set()
        white_rules = set()
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith('!') or line.startswith('#') or line.startswith('//'):
                continue
            
            # ç™½åå•è§„åˆ™
            if line.startswith('@@'):
                domain = self.enhanced_extract_domain(line)
                if domain:
                    white_domains.add(domain)
                    white_rules.add(f"@@||{domain}^")
                else:
                    if not re.match(r'^@@\|\|.*\^$', line):
                        white_rules.add(line)
            
            # é»‘åå•è§„åˆ™
            else:
                domain = self.enhanced_extract_domain(line)
                if domain:
                    black_domains.add(domain)
                    
                    # é€šé…ç¬¦æ‰©å±•
                    if CONFIG['HIT_OPTIMIZATION']['enable_wildcard_expansion'] and '*' in line:
                        expansions = DomainOptimizer.expand_wildcard_domain(domain)
                        for expanded in expansions:
                            if expanded != domain and self.is_valid_domain(expanded):
                                black_domains.add(expanded)
                    
                    # å­åŸŸåç”Ÿæˆ
                    if (CONFIG['HIT_OPTIMIZATION']['enable_subdomain_generation'] and 
                        DomainOptimizer.is_ad_domain(domain)):
                        subdomains = DomainOptimizer.generate_subdomains(domain)
                        for subdomain in subdomains:
                            if self.is_valid_domain(subdomain):
                                black_domains.add(subdomain)
                
                # ä¿ç•™å…¶ä»–è§„åˆ™
                else:
                    if self.is_valid_rule(line):
                        black_rules.add(line)
        
        return black_domains, black_rules, white_domains, white_rules
    
    def is_valid_rule(self, rule: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆè§„åˆ™"""
        if not rule or len(rule) < 3:
            return False
        
        invalid_patterns = [
            r'^\s*$',
            r'^##',
            r'^#\$#',
            r'^!\s+',
            r'^\[Adblock',
            r'^\/\*',
            r'^\*\/$'
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, rule):
                return False
        
        if not re.search(r'[a-zA-Z0-9]', rule):
            return False
        
        return True
    
    def download_and_parse_all(self):
        """å¢å¼ºç‰ˆä¸‹è½½å’Œè§£æ"""
        logger.info("å¼€å§‹ä¸‹è½½å’Œè§£æè§„åˆ™ï¼ˆå¢å¼ºç‰ˆï¼‰...")
        
        all_urls = []
        for url in self.black_urls:
            all_urls.append((url, 'black'))
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
            futures = {}
            for url, type_ in all_urls:
                future = executor.submit(self.download_url, url)
                futures[future] = (url, type_)
            
            processed = 0
            for future in concurrent.futures.as_completed(futures):
                processed += 1
                url, type_ = futures[future]
                
                try:
                    content = future.result()
                    if content:
                        # é™åˆ¶å†…å®¹å¤§å°
                        if len(content) > 10 * 1024 * 1024:
                            logger.warning(f"å†…å®¹è¿‡å¤§ï¼Œæˆªå–å‰10MB: {url}")
                            content = content[:10 * 1024 * 1024]
                        
                        black_domains, black_rules, white_domains, white_rules = self.parse_enhanced_content(content)
                        
                        self.black_domains.update(black_domains)
                        self.black_rules.update(black_rules)
                        self.white_domains.update(white_domains)
                        self.white_rules.update(white_rules)
                        
                        logger.debug(f"å¤„ç†å®Œæˆ: {url} ({len(black_domains)} ä¸ªåŸŸå)")
                        
                        if processed % 10 == 0:
                            logger.info(f"å¤„ç†è¿›åº¦: {processed}/{len(all_urls)}")
                
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
        
        logger.info(f"è§£æå®Œæˆ: é»‘åå•åŸŸå {len(self.black_domains):,} ä¸ª")
        logger.info(f"ç™½åå•åŸŸå {len(self.white_domains):,} ä¸ª")
    
    def apply_enhanced_whitelist(self):
        """å¢å¼ºç‰ˆç™½åå•åº”ç”¨"""
        if not self.white_domains:
            logger.warning("æ²¡æœ‰ç™½åå•åŸŸå")
            return
        
        original_black = len(self.black_domains)
        
        # 1. ç›´æ¥åŒ¹é…ç§»é™¤
        self.black_domains -= self.white_domains
        
        # 2. å­åŸŸååŒ¹é…
        if len(self.white_domains) < 10000:
            to_remove = set()
            
            for white_domain in self.white_domains:
                pattern = re.compile(rf'.*\.{re.escape(white_domain)}$')
                
                for black_domain in self.black_domains:
                    if pattern.match(black_domain):
                        to_remove.add(black_domain)
            
            self.black_domains -= to_remove
            
            total_removed = original_black - len(self.black_domains)
            logger.info(f"ç™½åå•åº”ç”¨å®Œæˆ: ç§»é™¤ {total_removed} ä¸ªåŸŸå")
    
    def optimize_for_hit_rate(self):
        """å‘½ä¸­ç‡ä¼˜åŒ–"""
        logger.info("å¼€å§‹å‘½ä¸­ç‡ä¼˜åŒ–...")
        
        all_domains = self.black_domains
        
        if CONFIG['HIT_OPTIMIZATION']['enable_smart_rules']:
            optimized_domains = self.optimizer.filter_low_score_domains(all_domains)
        else:
            optimized_domains = all_domains
        
        max_domains = CONFIG['HIT_OPTIMIZATION']['max_domains_per_source']
        if len(optimized_domains) > max_domains:
            scored_domains = []
            for domain in optimized_domains:
                score = self.optimizer.score_domain(domain)
                scored_domains.append((domain, score))
            
            scored_domains.sort(key=lambda x: x[1], reverse=True)
            optimized_domains = set([d[0] for d in scored_domains[:max_domains]])
            logger.info(f"é™åˆ¶åŸŸåæ•°é‡: {max_domains:,}")
        
        self.black_domains = optimized_domains
        logger.info(f"ä¼˜åŒ–ååŸŸåæ€»æ•°: {len(self.black_domains):,}")
    
    def generate_smart_rules(self):
        """ç”Ÿæˆæ™ºèƒ½è§„åˆ™"""
        logger.info("ç”Ÿæˆæ™ºèƒ½è§„åˆ™...")
        
        smart_rules = []
        
        smart_rules.extend([
            "! æ™ºèƒ½å¹¿å‘Šè¿‡æ»¤è§„åˆ™",
            "! ç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "! åŸŸåæ•°é‡: " + str(len(self.black_domains)),
            ""
        ])
        
        if self.white_rules:
            smart_rules.append("! ç™½åå•è§„åˆ™")
            for rule in sorted(self.white_rules):
                smart_rules.append(rule)
            smart_rules.append("")
        
        domain_groups = defaultdict(list)
        for domain in self.black_domains:
            score = self.optimizer.score_domain(domain)
            
            if score >= 0.7:
                domain_groups['high'].append(domain)
            elif score >= 0.4:
                domain_groups['medium'].append(domain)
            else:
                domain_groups['low'].append(domain)
        
        for group_name in ['high', 'medium', 'low']:
            if domain_groups[group_name]:
                smart_rules.append(f"! {group_name.capitalize()} ä¼˜å…ˆçº§åŸŸå")
                for domain in sorted(domain_groups[group_name]):
                    smart_rules.append(f"||{domain}^")
                smart_rules.append("")
        
        with open(CONFIG['OUTPUT_FILES']['smart_ad'], 'w', encoding='utf-8') as f:
            f.write('\n'.join(smart_rules))
        
        logger.info(f"æ™ºèƒ½è§„åˆ™ç”Ÿæˆå®Œæˆ: {len(smart_rules)} è¡Œ")
    
    def generate_mobile_rules(self):
        """ç”Ÿæˆç§»åŠ¨ç«¯ä¼˜åŒ–è§„åˆ™"""
        logger.info("ç”Ÿæˆç§»åŠ¨ç«¯ä¼˜åŒ–è§„åˆ™...")
        
        mobile_rules = [
            "! ç§»åŠ¨ç«¯å¹¿å‘Šè¿‡æ»¤è§„åˆ™",
            "! ç”Ÿæˆæ—¶é—´: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "! ä¸“ä¸ºç§»åŠ¨è®¾å¤‡ä¼˜åŒ–",
            ""
        ]
        
        mobile_domains = set()
        for domain in self.black_domains:
            is_mobile = False
            
            for keyword in CONFIG['MOBILE_AD_KEYWORDS']:
                if keyword.lower() in domain.lower():
                    is_mobile = True
                    break
            
            mobile_patterns = [
                r'^m\.', r'\.m\.', r'mobile', r'android', r'ios',
                r'app', r'sdk', r'inapp', r'interstitial'
            ]
            
            for pattern in mobile_patterns:
                if re.search(pattern, domain, re.IGNORECASE):
                    is_mobile = True
                    break
            
            if is_mobile:
                mobile_domains.add(domain)
        
        mobile_rules.append(f"! ç§»åŠ¨å¹¿å‘ŠåŸŸå: {len(mobile_domains)} ä¸ª")
        for domain in sorted(mobile_domains):
            mobile_rules.append(f"||{domain}^")
        
        mobile_rules.extend([
            "",
            "! ç§»åŠ¨ç«¯ç‰¹å®šè§„åˆ™",
            "||inmobi.com^",
            "||ironsrc.com^",
            "||applovin.com^",
            "||unity3d.com^$app=com.android.browser",
            "||vungle.com^",
            "||chartboost.com^",
            "||adjust.com^",
            "||appsflyer.com^",
            "||branch.io^",
            "||facebook.com/plugins/^$subdocument",
            "||google.com/ads/^$subdocument",
            ""
        ])
        
        with open(CONFIG['OUTPUT_FILES']['mobile_ad'], 'w', encoding='utf-8') as f:
            f.write('\n'.join(mobile_rules))
        
        logger.info(f"ç§»åŠ¨ç«¯è§„åˆ™ç”Ÿæˆå®Œæˆ: {len(mobile_domains)} ä¸ªåŸŸå")
    
    def generate_files(self):
        """å¢å¼ºç‰ˆæ–‡ä»¶ç”Ÿæˆ"""
        logger.info("ç”Ÿæˆè§„åˆ™æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰...")
        
        # å…ˆè¿›è¡Œå‘½ä¸­ç‡ä¼˜åŒ–
        self.optimize_for_hit_rate()
        
        # è°ƒç”¨çˆ¶ç±»ç”ŸæˆåŸºç¡€æ–‡ä»¶
        super().generate_files()
        
        # ç”Ÿæˆæ™ºèƒ½è§„åˆ™
        self.generate_smart_rules()
        
        # ç”Ÿæˆç§»åŠ¨ç«¯è§„åˆ™
        if CONFIG['HIT_OPTIMIZATION']['enable_mobile_optimization']:
            self.generate_mobile_rules()
        
        # æ›´æ–°info.json
        self.update_info_file()
    
    def update_info_file(self):
        """æ›´æ–°ä¿¡æ¯æ–‡ä»¶"""
        info_file = CONFIG['OUTPUT_FILES']['info']
        with open(info_file, 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        info['enhanced'] = {
            'smart_rules_generated': os.path.exists(CONFIG['OUTPUT_FILES']['smart_ad']),
            'mobile_rules_generated': os.path.exists(CONFIG['OUTPUT_FILES']['mobile_ad']),
            'optimization_applied': True,
            'hit_optimization_config': CONFIG['HIT_OPTIMIZATION']
        }
        
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
    
    def generate_readme(self):
        """å¢å¼ºç‰ˆREADMEç”Ÿæˆ"""
        logger.info("ç”ŸæˆREADME.mdï¼ˆå¢å¼ºç‰ˆï¼‰...")
        
        with open(CONFIG['OUTPUT_FILES']['info'], 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        version = info['version']
        
        readme_content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™ - å¢å¼ºç‰ˆ

ä¸€ä¸ªè‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™é›†åˆï¼Œç»è¿‡ä¼˜åŒ–æé«˜æ‹¦æˆªå‘½ä¸­ç‡ã€‚

## ğŸ“Š è§„åˆ™ç»Ÿè®¡

**ç‰ˆæœ¬ {version} è§„åˆ™ç»Ÿè®¡ï¼š**
- æ€»é»‘åå•åŸŸåï¼š{info['rules']['blacklist_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['rules']['whitelist_domains']:,} ä¸ª
- å…¶ä»–è§„åˆ™ï¼šé»‘åå• {info['rules']['blacklist_rules']:,} æ¡ï¼Œç™½åå• {info['rules']['whitelist_rules']:,} æ¡

## ğŸš€ è®¢é˜…åœ°å€

| è§„åˆ™åç§° | è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ | è¯´æ˜ |
|----------|----------|----------|----------|------|
| ç»¼åˆå¹¿å‘Šè¿‡æ»¤è§„åˆ™ | Adblock | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` | é€šç”¨è§„åˆ™ï¼Œé€‚åˆæ‰€æœ‰ç”¨æˆ· |
| æ™ºèƒ½å¹¿å‘Šè§„åˆ™ | Adblock | `{base_url}/smart_ad.txt` | `{cdn_url}/smart_ad.txt` | æ™ºèƒ½ä¼˜åŒ–ï¼Œé«˜å‘½ä¸­ç‡ |
| ç§»åŠ¨ç«¯è§„åˆ™ | Adblock | `{base_url}/mobile_ad.txt` | `{cdn_url}/mobile_ad.txt` | ç§»åŠ¨è®¾å¤‡ä¸“ç”¨ |
| DNSè¿‡æ»¤è§„åˆ™ | DNS | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` | Pi-holeç­‰DNSè¿‡æ»¤å™¨ |
| Hostsæ ¼å¼è§„åˆ™ | Hosts | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` | ç³»ç»ŸHostsæ–‡ä»¶ |
| é»‘åå•è§„åˆ™ | é»‘åå• | `{base_url}/black.txt` | `{cdn_url}/black.txt` | çº¯é»‘åå•åŸŸå |
| ç™½åå•è§„åˆ™ | ç™½åå• | `{base_url}/white.txt` | `{cdn_url}/white.txt` | æ’é™¤è¯¯æ€ |

## ğŸ¯ ä¼˜åŒ–ç‰¹æ€§

### 1. å‘½ä¸­ç‡ä¼˜åŒ–
- **æ™ºèƒ½è¯„åˆ†ç³»ç»Ÿ**ï¼šæ¯ä¸ªåŸŸåæ ¹æ®å¹¿å‘Šç‰¹å¾è¯„åˆ†
- **ä¼˜å…ˆçº§è¿‡æ»¤**ï¼šä¼˜å…ˆä¿ç•™é«˜å¹¿å‘Šå¯èƒ½æ€§åŸŸå
- **ä¸­æ–‡ä¼˜åŒ–**ï¼šä¸“é—¨é’ˆå¯¹ä¸­æ–‡ç½‘ç«™å¹¿å‘Šä¼˜åŒ–
- **ç§»åŠ¨ç«¯ä¼˜åŒ–**ï¼šä¼˜åŒ–ç§»åŠ¨è®¾å¤‡å¹¿å‘Šæ‹¦æˆª

### 2. è§„åˆ™è´¨é‡
- **è‡ªåŠ¨å»é‡**ï¼šç§»é™¤é‡å¤åŸŸå
- **æœ‰æ•ˆæ€§éªŒè¯**ï¼šéªŒè¯åŸŸåæ ¼å¼å’Œæœ‰æ•ˆæ€§
- **ç™½åå•ä¿æŠ¤**ï¼šé¿å…è¯¯æ€æ­£å¸¸ç½‘ç«™
- **å®šæœŸæ›´æ–°**ï¼šæ¯å¤©è‡ªåŠ¨æ›´æ–°è§„åˆ™

### 3. æ€§èƒ½ä¼˜åŒ–
- **å¤šçº¿ç¨‹ä¸‹è½½**ï¼šå¹¶è¡Œä¸‹è½½è§„åˆ™æº
- **æ™ºèƒ½ç¼“å­˜**ï¼šå‡å°‘é‡å¤ä¸‹è½½
- **å¢é‡æ›´æ–°**ï¼šåªæ›´æ–°å˜åŒ–éƒ¨åˆ†

## ğŸ“… æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated_at'].replace('T', ' ').replace('Z', '')}**

*è§„åˆ™æ¯å¤©è‡ªåŠ¨æ›´æ–°ï¼Œæ›´æ–°æ—¶é—´ï¼šåŒ—äº¬æ—¶é—´ 02:00*

## ğŸ”§ ä½¿ç”¨å»ºè®®

1. **AdGuard/uBlock Origin**ï¼šä½¿ç”¨ `smart_ad.txt` è·å¾—æœ€ä½³å¹³è¡¡
2. **Pi-hole/AdGuard Home**ï¼šä½¿ç”¨ `dns.txt` è¿›è¡ŒDNSå±‚é¢æ‹¦æˆª
3. **ç§»åŠ¨è®¾å¤‡**ï¼šä½¿ç”¨ `mobile_ad.txt` ä¸“é—¨é’ˆå¯¹ç§»åŠ¨å¹¿å‘Š
4. **å¦‚æœé‡åˆ°è¯¯æ€**ï¼šæ£€æŸ¥ `white.txt` æˆ–æäº¤Issue

## ğŸ“ˆ å‘½ä¸­ç‡æå‡æŠ€å·§

1. å®šæœŸæ›´æ–°è§„åˆ™ï¼ˆè‡³å°‘æ¯å‘¨ä¸€æ¬¡ï¼‰
2. ç»“åˆä½¿ç”¨æ™ºèƒ½è§„åˆ™å’ŒåŸºç¡€è§„åˆ™
3. é’ˆå¯¹ç‰¹å®šç½‘ç«™æ·»åŠ è‡ªå®šä¹‰è§„åˆ™
4. å…³æ³¨æ›´æ–°æ—¥å¿—ä¸­çš„ä¼˜åŒ–å†…å®¹

---

**æç¤º**ï¼šå¦‚æœå‘ç°è¯¯æ‹¦æˆªæˆ–æ¼æ‹¦æˆªï¼Œè¯·é€šè¿‡Issueåé¦ˆã€‚
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.mdç”Ÿæˆå®Œæˆ")
    
    def run(self):
        """è¿è¡Œå¢å¼ºç‰ˆæµç¨‹"""
        print("=" * 60)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - å¢å¼ºç‰ˆ")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            self.load_sources()
            self.download_and_parse_all()
            self.apply_enhanced_whitelist()
            self.generate_files()
            self.generate_readme()
            
            elapsed_time = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("âœ… å¢å¼ºç‰ˆå¤„ç†å®Œæˆï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"âœ… ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print(f"ğŸ¯ æ™ºèƒ½è§„åˆ™: rules/outputs/smart_ad.txt")
            print(f"ğŸ“± ç§»åŠ¨ç«¯è§„åˆ™: rules/outputs/mobile_ad.txt")
            print(f"ğŸ“ æ‰€æœ‰è§„åˆ™: rules/outputs/")
            print(f"ğŸ“– æ–‡æ¡£æ›´æ–°: README.md")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–ï¼šrequests")
        print("è¯·è¿è¡Œï¼špip install requests")
        return
    
    # è‡ªåŠ¨æ£€æµ‹è¿è¡Œç¯å¢ƒï¼Œéäº¤äº’å¼ç¯å¢ƒç›´æ¥ä½¿ç”¨å¢å¼ºæ¨¡å¼
    if sys.stdin and sys.stdin.isatty():
        # äº¤äº’å¼ç¯å¢ƒï¼Œå¯ä»¥è¯¢é—®ç”¨æˆ·
        print("è¯·é€‰æ‹©ç”Ÿæˆæ¨¡å¼ï¼š")
        print("1. åŸºç¡€æ¨¡å¼ï¼ˆå¿«é€Ÿï¼Œæ ‡å‡†è§„åˆ™ï¼‰")
        print("2. å¢å¼ºæ¨¡å¼ï¼ˆæ¨èï¼Œé«˜å‘½ä¸­ç‡ï¼‰")
        
        try:
            choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2, é»˜è®¤2): ").strip() or "2"
        except EOFError:
            # éäº¤äº’å¼ç¯å¢ƒä½†è¢«å½“ä½œäº¤äº’å¼ï¼Œé»˜è®¤ä½¿ç”¨å¢å¼ºæ¨¡å¼
            print("æ£€æµ‹åˆ°éäº¤äº’å¼ç¯å¢ƒï¼Œè‡ªåŠ¨é€‰æ‹©å¢å¼ºæ¨¡å¼")
            choice = "2"
    else:
        # éäº¤äº’å¼ç¯å¢ƒï¼Œç›´æ¥ä½¿ç”¨å¢å¼ºæ¨¡å¼
        print("éäº¤äº’å¼ç¯å¢ƒï¼Œè‡ªåŠ¨é€‰æ‹©å¢å¼ºæ¨¡å¼")
        choice = "2"
    
    if choice == "1":
        print("\nğŸš€ ä½¿ç”¨åŸºç¡€æ¨¡å¼...")
        generator = AdBlockGenerator()
    else:
        print("\nâš¡ ä½¿ç”¨å¢å¼ºæ¨¡å¼...")
        generator = EnhancedAdBlockGenerator()
    
    # è¿è¡Œç”Ÿæˆå™¨
    success = generator.run()
    
    if success:
        print("\nğŸ‰ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹README.mdè·å–è®¢é˜…é“¾æ¥")
        print("ğŸ“Š å‘½ä¸­ç‡å»ºè®®ï¼šä½¿ç”¨ smart_ad.txt è·å¾—æœ€ä½³æ•ˆæœ")
        print("ğŸš€ GitHub Actionsä¼šè‡ªåŠ¨æäº¤æ›´æ–°")
    else:
        print("\nğŸ’¥ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")

if __name__ == "__main__":
    main()
