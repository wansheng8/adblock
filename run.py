#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨ - æç®€ç‰ˆ
"""

import os
import re
import json
import time
import concurrent.futures
from datetime import datetime
from typing import Set
import requests

# é…ç½®
CONFIG = {
    'GITHUB_USER': 'wansheng8',
    'GITHUB_REPO': 'adblock',
    'GITHUB_BRANCH': 'main',
    'MAX_WORKERS': 10,
    'TIMEOUT': 15,
    'RETRY': 2,
    'BLACK_SOURCE': 'rules/sources/black.txt',
    'WHITE_SOURCE': 'rules/sources/white.txt'
}

class RuleGenerator:
    def __init__(self):
        self.black_domains = set()
        self.white_domains = set()
        self.black_rules = set()
        self.white_rules = set()
        
        # åˆ›å»ºç›®å½•
        os.makedirs('rules/sources', exist_ok=True)
        os.makedirs('rules/outputs', exist_ok=True)
        
        # åˆ›å»ºç¤ºä¾‹æºæ–‡ä»¶
        if not os.path.exists(CONFIG['BLACK_SOURCE']):
            with open(CONFIG['BLACK_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# å¹¿å‘Šè§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/adservers.txt\n")
        
        if not os.path.exists(CONFIG['WHITE_SOURCE']):
            with open(CONFIG['WHITE_SOURCE'], 'w', encoding='utf-8') as f:
                f.write("# ç™½åå•è§„åˆ™æº\n")
                f.write("https://raw.githubusercontent.com/AdguardTeam/AdguardFilters/master/BaseFilter/sections/whitelist.txt\n")
    
    def download(self, url):
        """ä¸‹è½½è§„åˆ™"""
        for i in range(CONFIG['RETRY']):
            try:
                headers = {'User-Agent': 'Mozilla/5.0'}
                r = requests.get(url, headers=headers, timeout=CONFIG['TIMEOUT'])
                r.raise_for_status()
                return r.text
            except:
                if i < CONFIG['RETRY'] - 1:
                    time.sleep(1)
        return None
    
    def parse_domain(self, line):
        """æå–åŸŸå"""
        line = line.strip()
        if not line or line.startswith('!'):
            return None
        
        # ç§»é™¤æ³¨é‡Š
        if '#' in line:
            line = line.split('#')[0].strip()
        
        # åŒ¹é…åŸŸåæ ¼å¼
        patterns = [
            r'^\|\|([a-zA-Z0-9.-]+)\^',
            r'^@@\|\|([a-zA-Z0-9.-]+)\^',
            r'^([a-zA-Z0-9.-]+)$',
            r'^\d+\.\d+\.\d+\.\d+\s+([a-zA-Z0-9.-]+)',
            r'^\*\.([a-zA-Z0-9.-]+)'
        ]
        
        for pattern in patterns:
            match = re.match(pattern, line)
            if match:
                domain = match.group(1).lower()
                domain = re.sub(r'^www\.', '', domain)
                
                # éªŒè¯åŸŸå
                if self.is_domain(domain):
                    return domain
        
        return None
    
    def is_domain(self, domain):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå"""
        if not domain or len(domain) > 253:
            return False
        
        # æ’é™¤æœ¬åœ°åŸŸå
        bad_domains = ['localhost', 'local', 'broadcasthost', '0.0.0.0']
        if domain in bad_domains:
            return False
        
        parts = domain.split('.')
        if len(parts) < 2:
            return False
        
        for part in parts:
            if not part or len(part) > 63:
                return False
            if not re.match(r'^[a-z0-9]([a-z0-9\-]*[a-z0-9])?$', part):
                return False
        
        return True
    
    def process_url(self, url):
        """å¤„ç†å•ä¸ªURL"""
        content = self.download(url)
        if not content:
            return set(), set(), set(), set()
        
        black_domains = set()
        white_domains = set()
        black_lines = set()
        white_lines = set()
        
        for line in content.split('\n'):
            line = line.strip()
            if not line:
                continue
            
            # ç™½åå•
            if line.startswith('@@'):
                domain = self.parse_domain(line)
                if domain:
                    white_domains.add(domain)
                    white_lines.add(f"@@||{domain}^")
                else:
                    if len(line) > 5:
                        white_lines.add(line)
            
            # é»‘åå•
            else:
                domain = self.parse_domain(line)
                if domain:
                    black_domains.add(domain)
                else:
                    if len(line) > 3 and re.search(r'[a-zA-Z0-9]', line):
                        black_lines.add(line)
        
        return black_domains, white_domains, black_lines, white_lines
    
    def load_sources(self):
        """åŠ è½½è§„åˆ™æº"""
        urls = []
        
        # é»‘åå•æº
        with open(CONFIG['BLACK_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    urls.append(('black', line))
        
        # ç™½åå•æº
        with open(CONFIG['WHITE_SOURCE'], 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    urls.append(('white', line))
        
        print(f"ğŸ“¥ åŠ è½½ {len(urls)} ä¸ªè§„åˆ™æº")
        return urls
    
    def run(self):
        """ä¸»æµç¨‹"""
        print("=" * 50)
        print("å¹¿å‘Šè¿‡æ»¤è§„åˆ™ç”Ÿæˆå™¨")
        print("=" * 50)
        
        start = time.time()
        
        try:
            # åŠ è½½æº
            urls = self.load_sources()
            
            # å¹¶è¡Œå¤„ç†
            with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG['MAX_WORKERS']) as executor:
                futures = []
                for type_, url in urls:
                    future = executor.submit(self.process_url, url)
                    futures.append((type_, future))
                
                for type_, future in futures:
                    try:
                        bd, wd, bl, wl = future.result(timeout=20)
                        self.black_domains.update(bd)
                        self.white_domains.update(wd)
                        self.black_rules.update(bl)
                        self.white_rules.update(wl)
                    except Exception as e:
                        print(f"âš ï¸  å¤„ç†å¤±è´¥: {e}")
            
            # åº”ç”¨ç™½åå•
            if self.white_domains:
                self.black_domains -= self.white_domains
                # å­åŸŸååŒ¹é…
                to_remove = set()
                for black in self.black_domains:
                    for white in self.white_domains:
                        if black.endswith(f".{white}"):
                            to_remove.add(black)
                            break
                self.black_domains -= to_remove
            
            # ç”Ÿæˆæ–‡ä»¶
            self.generate_files()
            
            # ç”ŸæˆREADME
            self.generate_readme()
            
            # ç»Ÿè®¡
            end = time.time()
            print(f"\nâœ… å®Œæˆ! è€—æ—¶: {end-start:.1f}ç§’")
            print(f"ğŸ“Š é»‘åå•åŸŸå: {len(self.black_domains):,}ä¸ª")
            print(f"ğŸ“Š ç™½åå•åŸŸå: {len(self.white_domains):,}ä¸ª")
            print("ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ° rules/outputs/")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_files(self):
        """ç”Ÿæˆè§„åˆ™æ–‡ä»¶"""
        version = datetime.now().strftime('%Y%m%d')
        time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # 1. ad.txt - Adblockæ ¼å¼
        with open('rules/outputs/ad.txt', 'w', encoding='utf-8') as f:
            f.write(f"! Adblock Rules v{version}\n")
            f.write(f"! Updated: {time_str}\n")
            f.write(f"! Domains: {len(self.black_domains):,}\n")
            f.write("!\n")
            # ç™½åå•
            for rule in sorted(self.white_rules):
                if 'domain=' not in rule:
                    f.write(f"{rule}\n")
            # é»‘åå•
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
        
        # 2. dns.txt - DNSæ ¼å¼
        with open('rules/outputs/dns.txt', 'w', encoding='utf-8') as f:
            f.write(f"# DNS Block List v{version}\n")
            f.write(f"# Updated: {time_str}\n")
            f.write(f"# Domains: {len(self.black_domains):,}\n\n")
            for domain in sorted(self.black_domains):
                f.write(f"{domain}\n")
        
        # 3. hosts.txt - Hostsæ ¼å¼
        with open('rules/outputs/hosts.txt', 'w', encoding='utf-8') as f:
            f.write(f"# Hosts Block List v{version}\n")
            f.write(f"# Updated: {time_str}\n")
            f.write(f"# Domains: {len(self.black_domains):,}\n\n")
            f.write("127.0.0.1 localhost\n")
            f.write("::1 localhost\n\n")
            for domain in sorted(self.black_domains):
                f.write(f"0.0.0.0 {domain}\n")
        
        # 4. black.txt - é»‘åå•
        with open('rules/outputs/black.txt', 'w', encoding='utf-8') as f:
            for domain in sorted(self.black_domains):
                f.write(f"||{domain}^\n")
        
        # 5. white.txt - ç™½åå•
        with open('rules/outputs/white.txt', 'w', encoding='utf-8') as f:
            for rule in sorted(self.white_rules):
                f.write(f"{rule}\n")
        
        # 6. info.json - ä¿¡æ¯æ–‡ä»¶
        info = {
            'version': version,
            'updated': time_str,
            'stats': {
                'black_domains': len(self.black_domains),
                'white_domains': len(self.white_domains),
                'black_rules': len(self.black_rules),
                'white_rules': len(self.white_rules)
            }
        }
        
        with open('rules/outputs/info.json', 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)
    
    def generate_readme(self):
        """ç”ŸæˆREADME.md"""
        # è¯»å–ä¿¡æ¯
        with open('rules/outputs/info.json', 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # ç”Ÿæˆé“¾æ¥
        base_url = f"https://raw.githubusercontent.com/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}/{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        cdn_url = f"https://cdn.jsdelivr.net/gh/{CONFIG['GITHUB_USER']}/{CONFIG['GITHUB_REPO']}@{CONFIG['GITHUB_BRANCH']}/rules/outputs"
        
        # ç”ŸæˆREADME
        content = f"""# å¹¿å‘Šè¿‡æ»¤è§„åˆ™

è‡ªåŠ¨æ›´æ–°çš„å¹¿å‘Šè¿‡æ»¤è§„åˆ™ï¼Œé€‚ç”¨äºAdGuardã€AdBlock Plusã€uBlock Originã€AdGuard Homeã€Pi-holeç­‰ã€‚

---

## è®¢é˜…åœ°å€

| è§„åˆ™ç±»å‹ | åŸå§‹é“¾æ¥ | åŠ é€Ÿé“¾æ¥ |
|:----------|:----------|:----------|
| **AdBlockè§„åˆ™** | `{base_url}/ad.txt` | `{cdn_url}/ad.txt` |
| **DNSè¿‡æ»¤è§„åˆ™** | `{base_url}/dns.txt` | `{cdn_url}/dns.txt` |
| **Hostsè§„åˆ™** | `{base_url}/hosts.txt` | `{cdn_url}/hosts.txt` |
| **é»‘åå•è§„åˆ™** | `{base_url}/black.txt` | `{cdn_url}/black.txt` |
| **ç™½åå•è§„åˆ™** | `{base_url}/white.txt` | `{cdn_url}/white.txt` |

**ç‰ˆæœ¬ {info['version']} ç»Ÿè®¡ï¼š**
- é»‘åå•åŸŸåï¼š{info['stats']['black_domains']:,} ä¸ª
- ç™½åå•åŸŸåï¼š{info['stats']['white_domains']:,} ä¸ª

---

## æœ€æ–°æ›´æ–°æ—¶é—´

**{info['updated']}**

*æ¯æ—¥è‡ªåŠ¨æ›´æ–°ï¼ŒåŒ—äº¬æ—¶é—´ 02:00*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(content)

if __name__ == '__main__':
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
    except ImportError:
        print("è¯·å®‰è£…ä¾èµ–: pip install requests")
        exit(1)
    
    # è¿è¡Œç”Ÿæˆå™¨
    gen = RuleGenerator()
    success = gen.run()
    
    if success:
        print("\nâœ¨ è§„åˆ™ç”ŸæˆæˆåŠŸï¼")
        print("ğŸ“„ æŸ¥çœ‹ README.md è·å–è®¢é˜…é“¾æ¥")
    else:
        print("\nâŒ è§„åˆ™ç”Ÿæˆå¤±è´¥ï¼")
